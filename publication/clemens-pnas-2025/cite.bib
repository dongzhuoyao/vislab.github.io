@article{
doi:10.1073/pnas.2414005122,
author = {Clemens G. Bartnik  and Christina Sartzetaki  and Abel Puigseslloses Sanchez  and Elijah Molenkamp  and Steven Bommer  and Nikolina Vukšić  and Iris I. A. Groen },
title = {Representation of locomotive action affordances in human behavior, brains, and deep neural networks},
journal = {Proceedings of the National Academy of Sciences},
volume = {122},
number = {24},
pages = {e2414005122},
year = {2025},
doi = {10.1073/pnas.2414005122},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2414005122},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2414005122},
abstract = {To navigate the world around us, we can use different actions, such as walking, swimming, or climbing. How does our brain compute and represent such locomotive action affordances? Here, we show that activation patterns in visual regions in the human brain represent information about affordances independent of other visual elements such as surface materials and objects, and do so in an automatic manner. We also demonstrate that commonly used models of visual processing in human brains, namely object- and scene-classification trained deep neural networks, do not strongly represent this information. Our results suggest that locomotive action affordance perception in scenes relies on specialized neural representations different from those used for other visual understanding tasks. To decide how to move around the world, we must determine which locomotive actions (e.g., walking, swimming, or climbing) are afforded by the immediate visual environment. The neural basis of our ability to recognize locomotive affordances is unknown. Here, we compare human behavioral annotations, functional MRI (fMRI) measurements, and deep neural network (DNN) activations to both indoor and outdoor real-world images to demonstrate that the human visual cortex represents locomotive action affordances in complex visual scenes. Hierarchical clustering of behavioral annotations of six possible locomotive actions show that humans group environments into distinct affordance clusters using at least three separate dimensions. Representational similarity analysis of multivoxel fMRI responses in the scene-selective visual cortex shows that perceived locomotive affordances are represented independently from other scene properties such as objects, surface materials, scene category, or global properties and independent of the task performed in the scanner. Visual feature activations from DNNs trained on object or scene classification as well as a range of other visual understanding tasks correlate comparatively lower with behavioral and neural representations of locomotive affordances than with object representations. Training DNNs directly on affordance labels or using affordance-centered language embeddings increases alignment with human behavior, but none of the tested models fully captures locomotive action affordance perception. These results uncover a type of representation in the human brain that reflects locomotive action affordances.}}