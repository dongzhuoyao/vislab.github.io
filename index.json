[{"authors":["admin"],"categories":null,"content":"I am currently a Postdoctoral Researcher at the University of Amsterdam. I completed my Ph.D. in Artificial Intelligence through a joint program between Politecnico di Torino and the University of Padova. I am particularly interested in topics such as real2sim2real transfer, few-shot learning, 3D vision, and human perception, with the overarching goal of developing learning systems that allow robots to acquire new skills efficiently. My ultimate goal is to enable robots to better understand and assist humans in real-world environments.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d509f3b276925970cdd2b6e9ef5e5dc5","permalink":"https://ivi.fnwi.uva.nl/vislab/author/leonardo-barcellona/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/leonardo-barcellona/","section":"authors","summary":"I am currently a Postdoctoral Researcher at the University of Amsterdam. I completed my Ph.D. in Artificial Intelligence through a joint program between Politecnico di Torino and the University of Padova.","tags":null,"title":"Leonardo Barcellona","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"916cd6bb558551c13fe71d84c511c939","permalink":"https://ivi.fnwi.uva.nl/vislab/author/mehmet-altinkaya/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/mehmet-altinkaya/","section":"authors","summary":"  ","tags":null,"title":"Mehmet Altinkaya","type":"authors"},{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a new Ph.D. candidate at both VIS and INDE labs. I did a master\u0026rsquo;s in AI at the University of Amsterdam. I\u0026rsquo;m originally from Iran where I did a bachelor in Information Technology at the University of Tehran. When I\u0026rsquo;m not behind my computer, I like to swim and do gardening.\nMy research is an intersection of Knowledge Graphs and Video Understanding; for instance, building a spatio-temporal knowledge graph from videos or vice versa. I will be working with Paul Groth from the INDE lab and Pascal Mettes from the VIS lab during my Ph.D.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"75ead19974353b45c7736ea6e019911b","permalink":"https://ivi.fnwi.uva.nl/vislab/author/melika-ayoughi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/melika-ayoughi/","section":"authors","summary":"I\u0026rsquo;m a new Ph.D. candidate at both VIS and INDE labs. I did a master\u0026rsquo;s in AI at the University of Amsterdam. I\u0026rsquo;m originally from Iran where I did a bachelor in Information Technology at the University of Tehran.","tags":null,"title":"Melika Ayoughi","type":"authors"},{"authors":["admin"],"categories":null,"content":".\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"621bf200817265fce1470ab7e53724d2","permalink":"https://ivi.fnwi.uva.nl/vislab/author/carlo-bretti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/carlo-bretti/","section":"authors","summary":".","tags":null,"title":"Carlo Bretti","type":"authors"},{"authors":["admin"],"categories":null,"content":"I completed my BSc and MSc in Electrical and Computer Engineering at the National Technical University of Athens in 2021. During my studies, I worked on different projects and studied various subjects such as Deep Learning, Pattern Recognition, Natural Language Processing, Computer Vision, and Information Theory. For my master\u0026rsquo;s thesis, I studied Structured Pruning for Deep Learning Language Models. After my master\u0026rsquo;s graduation, I worked on a European Space Agency project on the automatic detection and classification of lunar impact flashes. From 2022 I am a Ph.D. candidate in the POP-AART lab, a collaboration between the Netherlands Cancer Institute, the University of Amsterdam, and Elekta. This collaboration focuses on the use of artificial intelligence for precision radiotherapy. The lab is supervised by Jan-Jakob Sonke and Efstratios Gavves. In my Ph.D. I will focus on using Reinforcement Learning for Adaptive Radiotherapy Treatment.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4ba9a47700aa3b4f30d92436234aa51c","permalink":"https://ivi.fnwi.uva.nl/vislab/author/stefanos-stamatis-achlatis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/stefanos-stamatis-achlatis/","section":"authors","summary":"I completed my BSc and MSc in Electrical and Computer Engineering at the National Technical University of Athens in 2021. During my studies, I worked on different projects and studied various subjects such as Deep Learning, Pattern Recognition, Natural Language Processing, Computer Vision, and Information Theory.","tags":null,"title":"Stefanos Stamatis Achlatis","type":"authors"},{"authors":["admin"],"categories":null,"content":"I am a postdoc at the Brain and Cognition group at the University of Amsterdam, working with Steven Scholte and Iris Groen. Before that, I did my PhD in Cognitive Neuroscience at the Donders Institute in Nijmegen.\nMy research focuses on investigating how the brain can visually parse complex everyday scenes. I plan to do so by comparing human neuroimaging data with computational models that explicitly represent relations between objects.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"918c08b14143fce49824b0852ec5b049","permalink":"https://ivi.fnwi.uva.nl/vislab/author/giacomo-aldegheri/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/giacomo-aldegheri/","section":"authors","summary":"I am a postdoc at the Brain and Cognition group at the University of Amsterdam, working with Steven Scholte and Iris Groen. Before that, I did my PhD in Cognitive Neuroscience at the Donders Institute in Nijmegen.","tags":null,"title":"Giacomo Aldegheri","type":"authors"},{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"51a4b186e92436a6376bf920e5155df4","permalink":"https://ivi.fnwi.uva.nl/vislab/author/ilze-amanda-auzina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/ilze-amanda-auzina/","section":"authors","summary":"","tags":null,"title":"Ilze Amanda Auzina","type":"authors"},{"authors":["admin"],"categories":null,"content":"Dr. Yuki M. Asano is an assistant professor for computer vision and machine learning at the QUVA lab at the University of Amsterdam. His work focusses on self-supervised and multi-modal learning, and most recently ethics \u0026amp; privacy in computer vision and has been publishes at various top-tier conferences. His PhD was at the Visual Geometry Group (VGG) at the University of Oxford where he was supervised by Professor Andrea Vedaldi. Prior to this he studied physics at the University of Munich, Economics in Hagen as well as a MSc in Mathematical Modelling and Scientific Computing at the Mathematical Institute in Oxford.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"30a712377268707fa0d6c9595bc9ea45","permalink":"https://ivi.fnwi.uva.nl/vislab/author/yuki-m.-asano/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/yuki-m.-asano/","section":"authors","summary":"Dr. Yuki M. Asano is an assistant professor for computer vision and machine learning at the QUVA lab at the University of Amsterdam. His work focusses on self-supervised and multi-modal learning, and most recently ethics \u0026amp; privacy in computer vision and has been publishes at various top-tier conferences.","tags":null,"title":"Yuki M. Asano","type":"authors"},{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b813feb9fbaf2fbe5044558375edd1f0","permalink":"https://ivi.fnwi.uva.nl/vislab/author/amber-brands/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/amber-brands/","section":"authors","summary":"","tags":null,"title":"Amber Brands","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2c6bb9e4545271f6d6c06400df5391c1","permalink":"https://ivi.fnwi.uva.nl/vislab/author/aritra-bhowmik/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/aritra-bhowmik/","section":"authors","summary":"  ","tags":null,"title":"Aritra Bhowmik","type":"authors"},{"authors":["admin"],"categories":null,"content":"I am a PhD candidate at the VIS-lab (Video and Object Sensing - Lab) of the University of Amsterdam, under the supervision of Iris Groen and Cees Snoek.\nPreviously, I received a B.Sc. and an M.Sc. in Pschology from the Otto Friedrich University Bamberg. I completed my Master\u0026rsquo;s thesis under the supervision of Ute Schmid Cognitive Systems Group at the University of Bamberg on a quantitative comparison of visual saliency maps of convolutional neural networks and those of human beings.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b208f3a5ecb98c80ea25934d7bae0461","permalink":"https://ivi.fnwi.uva.nl/vislab/author/clemens-georg-bartnik/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/clemens-georg-bartnik/","section":"authors","summary":"I am a PhD candidate at the VIS-lab (Video and Object Sensing - Lab) of the University of Amsterdam, under the supervision of Iris Groen and Cees Snoek.\nPreviously, I received a B.","tags":null,"title":"Clemens Georg Bartnik","type":"authors"},{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"29b67bfeaa1b00a55fa672bc773fe2b9","permalink":"https://ivi.fnwi.uva.nl/vislab/author/leonard-bereska/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/leonard-bereska/","section":"authors","summary":"","tags":null,"title":"Leonard Bereska","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0b61ec40049131ae60594ec1212179b8","permalink":"https://ivi.fnwi.uva.nl/vislab/author/rein-van-den-boomgaard/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/rein-van-den-boomgaard/","section":"authors","summary":"  ","tags":null,"title":"Rein van den Boomgaard","type":"authors"},{"authors":["admin"],"categories":null,"content":"Shuo is a third-year PhD candidate at University of Amsterdam under the supervision of Prof. Cees Snoek and Dr. Pascal Mettes.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f27d25a611c7804f3dd194ebfc5c2182","permalink":"https://ivi.fnwi.uva.nl/vislab/author/shuo-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/shuo-chen/","section":"authors","summary":"Shuo is a third-year PhD candidate at University of Amsterdam under the supervision of Prof. Cees Snoek and Dr. Pascal Mettes.","tags":null,"title":"Shuo Chen","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"72628b6491bf8df3ee7b6e10904ad840","permalink":"https://ivi.fnwi.uva.nl/vislab/author/yunlu-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/yunlu-chen/","section":"authors","summary":"  ","tags":null,"title":"Yunlu Chen","type":"authors"},{"authors":["admin"],"categories":null,"content":"I got my master’s degree in Data Science from MIPT \u0026amp; Skoltech in 2022. Then, I worked as a junior researcher at Artificial Intelligence Research Institute (AIRI) for a year. My previous research was focused on Topological Data Analysis and Natural Language Processing and published a number of papers. Now, I am into Embodied AI \u0026amp; Causality, but my interests in AI are still broad.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9d9dabd8f0448f9ee35b6ed8bab40d63","permalink":"https://ivi.fnwi.uva.nl/vislab/author/daniil-cherniavskii/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/daniil-cherniavskii/","section":"authors","summary":"I got my master’s degree in Data Science from MIPT \u0026amp; Skoltech in 2022. Then, I worked as a junior researcher at Artificial Intelligence Research Institute (AIRI) for a year. My previous research was focused on Topological Data Analysis and Natural Language Processing and published a number of papers.","tags":null,"title":"Daniil Cherniavskii","type":"authors"},{"authors":["admin"],"categories":null,"content":"I am a Postdoctoral researcher at the University of Amsterdam, working with with Prof. Cees Snoek. I completed my PhD at the University of Bristol, advised by Dr. Dima Damen and Prof. Walterio Mayol-Cuevas. My area of interest is Video Understanding, with my PhD thesis focussing on Skill Determination.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9a5cef4435e598b927a2170aa71dcd0b","permalink":"https://ivi.fnwi.uva.nl/vislab/author/hazel-doughty/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/hazel-doughty/","section":"authors","summary":"I am a Postdoctoral researcher at the University of Amsterdam, working with with Prof. Cees Snoek. I completed my PhD at the University of Bristol, advised by Dr. Dima Damen and Prof.","tags":null,"title":"Hazel Doughty","type":"authors"},{"authors":["admin"],"categories":null,"content":"I am a PhD student in the QUVA lab supervised by Yuki Asano and Cees Snoek. My research interests include video understanding, self-supervised and multi-modal learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6bf15fba9f44e441ae097a233bd7c428","permalink":"https://ivi.fnwi.uva.nl/vislab/author/michael-dorkenwald/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/michael-dorkenwald/","section":"authors","summary":"I am a PhD student in the QUVA lab supervised by Yuki Asano and Cees Snoek. My research interests include video understanding, self-supervised and multi-modal learning.","tags":null,"title":"Michael Dorkenwald","type":"authors"},{"authors":["admin"],"categories":null,"content":"My name is Mohammad Mahdi (call me Mohammad). I am a first-year PhD student at UvA working under the supervision of Prof. Cees Snoek and Dr. Xiantong Zhen. I am working in the domain of continual learning/life-long learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"68ff9ddf10c95b9f79d2247cf7bb16a3","permalink":"https://ivi.fnwi.uva.nl/vislab/author/mohammad-mahdi-derakhshani/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/mohammad-mahdi-derakhshani/","section":"authors","summary":"My name is Mohammad Mahdi (call me Mohammad). I am a first-year PhD student at UvA working under the supervision of Prof. Cees Snoek and Dr. Xiantong Zhen. I am working in the domain of continual learning/life-long learning.","tags":null,"title":"Mohammad Mahdi Derakhshani","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5a6ad75b4889ca16c7395b21531e716f","permalink":"https://ivi.fnwi.uva.nl/vislab/author/yingjun-du/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/yingjun-du/","section":"authors","summary":"  ","tags":null,"title":"Yingjun Du","type":"authors"},{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a postdoctoral researcher in VISLab at the University of Amsterdam working with Stratis Gavves. Previously, I did my PhD at the Max Planck Institute for Intelligent Systems and University of Tübingen jointly supervised by Georg Martius and Martin Butz, followed by a postdoc at the TU Dresden working with Katharina von Kriegstein.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b9dfa4d7663340397a05e43edf9bf3d4","permalink":"https://ivi.fnwi.uva.nl/vislab/author/christian-gumbsch/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/christian-gumbsch/","section":"authors","summary":"I\u0026rsquo;m a postdoctoral researcher in VISLab at the University of Amsterdam working with Stratis Gavves. Previously, I did my PhD at the Max Planck Institute for Intelligent Systems and University of Tübingen jointly supervised by Georg Martius and Martin Butz, followed by a postdoc at the TU Dresden working with Katharina von Kriegstein.","tags":null,"title":"Christian Gumbsch","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ab415906ec76f20118c9406f47e7ee45","permalink":"https://ivi.fnwi.uva.nl/vislab/author/alex-gabel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/alex-gabel/","section":"authors","summary":"  ","tags":null,"title":"Alex Gabel","type":"authors"},{"authors":["admin"],"categories":null,"content":"I’m a PhD student at the QUVA lab at the University of Amsterdam supervised by Yuki Asano and Cees Snoek. My research primarily focuses on foundation models, and how they can be efficiently adapted to incorporate desirable behaviors and properties.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bc80c4ace41a8ba61fe89a2499096203","permalink":"https://ivi.fnwi.uva.nl/vislab/author/danilo-de-goede/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/danilo-de-goede/","section":"authors","summary":"I’m a PhD student at the QUVA lab at the University of Amsterdam supervised by Yuki Asano and Cees Snoek. My research primarily focuses on foundation models, and how they can be efficiently adapted to incorporate desirable behaviors and properties.","tags":null,"title":"Danilo de Goede","type":"authors"},{"authors":["admin"],"categories":null,"content":"Dr. Efstratios Gavves is an Associate Professor with the University of Amsterdam in the Netherlands and Scientific Director of the QUVA Deep Vision Lab. He is a recipient of the prestigious ERC Career Starting Grant 2020 to research on the Computational Learning of Temporality for spatiotemporal sequences. Also, he is a co-founder of Ellogon.AI, a University spinoff and in collaboration with the Dutch Cancer Institute (NKI), with the mission of using AI for pathology and genomics. He is currently supervising more than 12 Ph.D. and postdoctoral students in projects with the University of Amsterdam, the Dutch Cancer Institute, Ellogon.AI, and BMW. Efstratios has authored several papers in the top Computer Vision and Machine Learning conferences and journals and he is also the author of several patents. Further, Efstratios teaches Deep Learning in the MSc in Artificial Intelligence at the University of Amsterdam. All material is available on the project website, uvadlc.github.io. His research focus is on Temporal Machine Learning and Dynamics, Efficient Computer Vision, and Machine Learning for Oncology.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"016c844e69157a923b708b441904ac50","permalink":"https://ivi.fnwi.uva.nl/vislab/author/efstratios-gavves/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/efstratios-gavves/","section":"authors","summary":"Dr. Efstratios Gavves is an Associate Professor with the University of Amsterdam in the Netherlands and Scientific Director of the QUVA Deep Vision Lab. He is a recipient of the prestigious ERC Career Starting Grant 2020 to research on the Computational Learning of Temporality for spatiotemporal sequences.","tags":null,"title":"Efstratios Gavves","type":"authors"},{"authors":["admin"],"categories":null,"content":"Iris Groen is an assistant-professor at the VIS lab. Her research focuses on how the human brain understands real-world scenes and videos. She studies visual perception in the human brain by combining human brain imaging techniques such as M/EEG, fMRI, and ECoG with computational models, ranging from neurophysiologically-informed visual encoding models to deep neural networks developed in AI. Before returning to UvA as an assistant-professor, she completed two post-docs in the USA, at the National Institutes of Health (with Chris Baker) and at New York University (with Jonathan Winawer). Her research is funded by a MacGillavry Fellowship and by an NWO VENI grant.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"06fb5faadbbb0a6cef721ea3ffd890ae","permalink":"https://ivi.fnwi.uva.nl/vislab/author/iris-groen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/iris-groen/","section":"authors","summary":"Iris Groen is an assistant-professor at the VIS lab. Her research focuses on how the human brain understands real-world scenes and videos. She studies visual perception in the human brain by combining human brain imaging techniques such as M/EEG, fMRI, and ECoG with computational models, ranging from neurophysiologically-informed visual encoding models to deep neural networks developed in AI.","tags":null,"title":"Iris Groen","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f07a651fa6620d658d6736614f5c3223","permalink":"https://ivi.fnwi.uva.nl/vislab/author/mina-ghadimiatigh/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/mina-ghadimiatigh/","section":"authors","summary":"  ","tags":null,"title":"Mina Ghadimiatigh","type":"authors"},{"authors":["admin"],"categories":null,"content":"I am a PhD student in the Video \u0026amp; Image Sense Lab (VIS), and Bosch Delta Lab under the supervision of Prof. Dr. Arnold Smeulders. Before my PhD I completed my masters in Electrical engineering from KAIST, South Korea under the supervision of Prof. Dr. Jong-Hwan Kim in Robotics intelligence technology lab.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"526f643ddfd1fdaec0ba27df06007243","permalink":"https://ivi.fnwi.uva.nl/vislab/author/sadaf-gulshad/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/sadaf-gulshad/","section":"authors","summary":"I am a PhD student in the Video \u0026amp; Image Sense Lab (VIS), and Bosch Delta Lab under the supervision of Prof. Dr. Arnold Smeulders. Before my PhD I completed my masters in Electrical engineering from KAIST, South Korea under the supervision of Prof.","tags":null,"title":"Sadaf Gulshad","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8b53ccecc35d7122c4e014cd0dee4538","permalink":"https://ivi.fnwi.uva.nl/vislab/author/vincenttao-hu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/vincenttao-hu/","section":"authors","summary":"  ","tags":null,"title":"(Vincent)Tao Hu","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"556bed4424f870251e04cbf3cf8ffb38","permalink":"https://ivi.fnwi.uva.nl/vislab/author/tessa-van-de-heiden/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/tessa-van-de-heiden/","section":"authors","summary":"  ","tags":null,"title":"Tessa van de Heiden","type":"authors"},{"authors":["admin"],"categories":null,"content":"David Knigge did his bachelor\u0026rsquo;s and master\u0026rsquo;s in Artificial Intelligence at the UvA, and over this period became very interested in model generalisability and improvement through the incorporation of geometric biases in model design. He works under the supervision of Efstratios Gavves at the POP-AART lab, where they attempt to improve radiotherapy methods through the use of geometric deep learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"cd6bce4bebf2effa3819d648ba928599","permalink":"https://ivi.fnwi.uva.nl/vislab/author/david-knigge/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/david-knigge/","section":"authors","summary":"David Knigge did his bachelor\u0026rsquo;s and master\u0026rsquo;s in Artificial Intelligence at the UvA, and over this period became very interested in model generalisability and improvement through the incorporation of geometric biases in model design.","tags":null,"title":"David Knigge","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b26823320bf5d80d07324fdc5d61ff47","permalink":"https://ivi.fnwi.uva.nl/vislab/author/dennis-koelma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/dennis-koelma/","section":"authors","summary":"  ","tags":null,"title":"Dennis Koelma","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d01f47abd688b1563dd47b0833a6d5df","permalink":"https://ivi.fnwi.uva.nl/vislab/author/mert-kilickaya/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/mert-kilickaya/","section":"authors","summary":"  ","tags":null,"title":"Mert Kilickaya","type":"authors"},{"authors":["admin"],"categories":null,"content":"My name is Miltiadis Kofinas and I am a PhD student in the Video \u0026amp; Image Sense Lab at the University of Amsterdam, supervised by Efstratios Gavves. My research focuses on future spatio-temporal forecasting, with applications on forecasting for autonomous vehicles. My research interests include graph neural networks, temporal dynamics, geometric deep learning and equivariant representations.\nPrior to my PhD, I received a Diploma in Electrical and Computer Engineering from the Aristotle University of Thessaloniki. For my Diploma thesis, I researched the topic of Scene Graph Generation using Graph Neural Networks. During my studies, I was a computer vision \u0026amp; machine learning engineer at P.A.N.D.O.R.A. Robotics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9e6ad7b28db126299d4aca44a532010d","permalink":"https://ivi.fnwi.uva.nl/vislab/author/miltiadis-kofinas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/miltiadis-kofinas/","section":"authors","summary":"My name is Miltiadis Kofinas and I am a PhD student in the Video \u0026amp; Image Sense Lab at the University of Amsterdam, supervised by Efstratios Gavves. My research focuses on future spatio-temporal forecasting, with applications on forecasting for autonomous vehicles.","tags":null,"title":"Miltiadis Kofinas","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e26c7f270dca918bffb8fa974f261d2c","permalink":"https://ivi.fnwi.uva.nl/vislab/author/nguyen-duy-kien/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/nguyen-duy-kien/","section":"authors","summary":"  ","tags":null,"title":"Nguyen Duy Kien","type":"authors"},{"authors":["admin"],"categories":null,"content":"I am a first year PhD student at VIS Lab and ELLIS Unit Amsterdam under the supervision of Pascal Mettes and Rita Cucchiara.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"462f6e9f0112ed721d632c5a561fab51","permalink":"https://ivi.fnwi.uva.nl/vislab/author/tejaswi-kasarla/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/tejaswi-kasarla/","section":"authors","summary":"I am a first year PhD student at VIS Lab and ELLIS Unit Amsterdam under the supervision of Pascal Mettes and Rita Cucchiara.","tags":null,"title":"Tejaswi Kasarla","type":"authors"},{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"31199e4d7d11941a8052c258a6de2ccb","permalink":"https://ivi.fnwi.uva.nl/vislab/author/jie-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/jie-liu/","section":"authors","summary":"","tags":null,"title":"Jie Liu","type":"authors"},{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a PhD student in the QUVA lab under the supervision of Efstratios Gavves and Taco Cohen. My research interests include causality, generative modelling and graph neural networks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7c3d852e3b93f91e5c0516b934fba92f","permalink":"https://ivi.fnwi.uva.nl/vislab/author/phillip-lippe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/phillip-lippe/","section":"authors","summary":"I\u0026rsquo;m a PhD student in the QUVA lab under the supervision of Efstratios Gavves and Taco Cohen. My research interests include causality, generative modelling and graph neural networks.","tags":null,"title":"Phillip Lippe","type":"authors"},{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"79a3e48ea852dc49a285437e29b3b55a","permalink":"https://ivi.fnwi.uva.nl/vislab/author/yongtuo-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/yongtuo-liu/","section":"authors","summary":"","tags":null,"title":"Yongtuo Liu","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f1ef6061435d65146341cd50fdacc9a9","permalink":"https://ivi.fnwi.uva.nl/vislab/author/artem-moskalev/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/artem-moskalev/","section":"authors","summary":"  ","tags":null,"title":"Artem Moskalev","type":"authors"},{"authors":["admin"],"categories":null,"content":"I am a PhD candidate at the Brain and Cognition Group of the University of Amsterdam under supervision of Steven Scholte and Iris Groen.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7837135ea932c25034bdeff77a75c35b","permalink":"https://ivi.fnwi.uva.nl/vislab/author/niklas-muller/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/niklas-muller/","section":"authors","summary":"I am a PhD candidate at the Brain and Cognition Group of the University of Amsterdam under supervision of Steven Scholte and Iris Groen.\n  ","tags":null,"title":"Niklas Müller","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"be34e0fae754a57247b514740f35be49","permalink":"https://ivi.fnwi.uva.nl/vislab/author/pascal-mettes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/pascal-mettes/","section":"authors","summary":"  ","tags":null,"title":"Pascal Mettes","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9ac106ba4fd51658fb3e4d6ec44402f2","permalink":"https://ivi.fnwi.uva.nl/vislab/author/adeel-pervez/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/adeel-pervez/","section":"authors","summary":"  ","tags":null,"title":"Adeel Pervez","type":"authors"},{"authors":["admin"],"categories":null,"content":"Experienced Doctoral Researcher with a demonstrated history of working in the research industry. Skilled in biomedical research using artificial intelligence, machine learning and extensive leadership background. Strong research professional with a Master of Science - in Artificial Intelligence from University of Amsterdam.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"defd54fbc53d85867e0709bdf9c00e67","permalink":"https://ivi.fnwi.uva.nl/vislab/author/andreas-panteli/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/andreas-panteli/","section":"authors","summary":"Experienced Doctoral Researcher with a demonstrated history of working in the research industry. Skilled in biomedical research using artificial intelligence, machine learning and extensive leadership background. Strong research professional with a Master of Science - in Artificial Intelligence from University of Amsterdam.","tags":null,"title":"Andreas Panteli","type":"authors"},{"authors":["admin"],"categories":null,"content":"I have a background in information engineering, computer engineering, and artificial intelligence. In 2021 I completed both an MSc (cum laude) in Computer Engineering at the University of Padova and an MSc in Human-Centered Artificial Intelligence at the Technical University of Denmark. During my studies, I focused on fundamental research in the field of Deep Learning, specifically on how to obtain useful representations of images to enable the automation of higher-level cognitive tasks.\nI am now a PhD candidate under the POP-AART Lab (2021-2024), a collaboration between Elekta, the University of Amsterdam, and the Netherlands Cancer Institute. The aim of the collaboration is personalized online radiotherapy using artificial intelligence methods. The lab is supervised by Jan-Jakob Sonke and Efstratios Gavves. I will focus on using deep generative models to improve the quality of Cone Beam Computed Tomography (CBCT) while enforcing geometric and pathological integrity.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d45f8e784356e8b09223d62394147358","permalink":"https://ivi.fnwi.uva.nl/vislab/author/samuele-papa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/samuele-papa/","section":"authors","summary":"I have a background in information engineering, computer engineering, and artificial intelligence. In 2021 I completed both an MSc (cum laude) in Computer Engineering at the University of Padova and an MSc in Human-Centered Artificial Intelligence at the Technical University of Denmark.","tags":null,"title":"Samuele Papa","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"78f24b86b78364137c9f55abbfb8c8d2","permalink":"https://ivi.fnwi.uva.nl/vislab/author/sarah-rastegar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/sarah-rastegar/","section":"authors","summary":"  ","tags":null,"title":"Sarah Rastegar","type":"authors"},{"authors":["admin"],"categories":null,"content":"After Delft and Leyden, first medical informatics at VU and Erasmus, then professor of biology and since 1995 professor of computer science at the UvA ever since. Advised 60 PhD\u0026rsquo;s to graduation, chaired IPN for 7 years, member of the Faculty Board for finance for 4 years, member of the Academia Europeana, recipient of the De Vries medallion, recipient of the ACM SIGMM Life time award, member of the KHMW, member of the ERC and other selection committees, Euvision and Kepler startups.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9ee87853a17913d8d41764802d451865","permalink":"https://ivi.fnwi.uva.nl/vislab/author/arnold-smeulders/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/arnold-smeulders/","section":"authors","summary":"After Delft and Leyden, first medical informatics at VU and Erasmus, then professor of biology and since 1995 professor of computer science at the UvA ever since. Advised 60 PhD\u0026rsquo;s to graduation, chaired IPN for 7 years, member of the Faculty Board for finance for 4 years, member of the Academia Europeana, recipient of the De Vries medallion, recipient of the ACM SIGMM Life time award, member of the KHMW, member of the ERC and other selection committees, Euvision and Kepler startups.","tags":null,"title":"Arnold Smeulders","type":"authors"},{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ec4b57a7cd01144d63958b73d8821074","permalink":"https://ivi.fnwi.uva.nl/vislab/author/cees-g.m-snoek/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/cees-g.m-snoek/","section":"authors","summary":"","tags":null,"title":"Cees G.M Snoek","type":"authors"},{"authors":["admin"],"categories":null,"content":"I am a PhD candidate supervised by Iris Groen \u0026amp; Cees Snoek and funded by the European AI Laboratory ELLIS. My research focuses on the representational alignment between deep video-AI and the human brain and behaviour. Through assessing the extent of alignment in current state-of-the-art models and researching methods to explicitly increase alignment, I aim to work towards the goal of enhancing the models’ robustness and generalisability.\nPrior to my PhD I worked as a Machine Learning Engineer in DeepLab, a Greek tech company, where I was mainly involved with research projects on Brain Computer Interfaces, as well as scRNA-seq data. Before that, I worked on my Master’s thesis project on Social Video Question Answering.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0c564ef688caded44c349371f7d9de89","permalink":"https://ivi.fnwi.uva.nl/vislab/author/christina-sartzetaki/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/christina-sartzetaki/","section":"authors","summary":"I am a PhD candidate supervised by Iris Groen \u0026amp; Cees Snoek and funded by the European AI Laboratory ELLIS. My research focuses on the representational alignment between deep video-AI and the human brain and behaviour.","tags":null,"title":"Christina Sartzetaki","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7fa583c595003ca40a668d8a8d5c6cef","permalink":"https://ivi.fnwi.uva.nl/vislab/author/ivan-sosnovik/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/ivan-sosnovik/","section":"authors","summary":"  ","tags":null,"title":"Ivan Sosnovik","type":"authors"},{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"99bdce110a58ae495561c5adf8c929b8","permalink":"https://ivi.fnwi.uva.nl/vislab/author/max-van-spengler/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/max-van-spengler/","section":"authors","summary":"","tags":null,"title":"Max van Spengler","type":"authors"},{"authors":["admin"],"categories":null,"content":"My name is Mohammadreza, a first-year Ph.D. student at QUVA lab working under the supervision of Cees Snoek, Stratis Gavves, and Yuki M. Asano. I am currently working on SSL methods for efficient video representation learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c0087025756f5381ebb021c2f62f8f39","permalink":"https://ivi.fnwi.uva.nl/vislab/author/mohammadreza-salehi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/mohammadreza-salehi/","section":"authors","summary":"My name is Mohammadreza, a first-year Ph.D. student at QUVA lab working under the supervision of Cees Snoek, Stratis Gavves, and Yuki M. Asano. I am currently working on SSL methods for efficient video representation learning.","tags":null,"title":"Mohammadreza Salehi","type":"authors"},{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5b6bd2912b3809d95b5829b5378786d7","permalink":"https://ivi.fnwi.uva.nl/vislab/author/yoni-schirris/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/yoni-schirris/","section":"authors","summary":"","tags":null,"title":"Yoni Schirris","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"42106c3bdf45f65b13693c34f5ebb4ba","permalink":"https://ivi.fnwi.uva.nl/vislab/author/zenglin-shi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/zenglin-shi/","section":"authors","summary":"  ","tags":null,"title":"Zenglin Shi","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1746a836f059c8a219ec8b070845df37","permalink":"https://ivi.fnwi.uva.nl/vislab/author/fida-thoker/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/fida-thoker/","section":"authors","summary":"  ","tags":null,"title":"Fida Thoker","type":"authors"},{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2dad3fa51cd01b9d8b5baa0440bc4f2f","permalink":"https://ivi.fnwi.uva.nl/vislab/author/wenfang-sun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/wenfang-sun/","section":"authors","summary":"","tags":null,"title":"Wenfang Sun","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"89e5dc6f8d87caf4fac1fe5fc82ef430","permalink":"https://ivi.fnwi.uva.nl/vislab/author/william-thong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/william-thong/","section":"authors","summary":"  ","tags":null,"title":"William Thong","type":"authors"},{"authors":["admin"],"categories":null,"content":"Hi! I\u0026rsquo;m a PhD student under the joint supervision of Cees Snoek and Gertjan Brughouts, working together with TNO, Delft University of Technology and the Royal Marechaussee. Our project focuses on improving robotics in an open world settings through the use of foundational models.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c10cc8a33a0b2b440201dbaf433a6b81","permalink":"https://ivi.fnwi.uva.nl/vislab/author/thomas-wiggers/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/thomas-wiggers/","section":"authors","summary":"Hi! I\u0026rsquo;m a PhD student under the joint supervision of Cees Snoek and Gertjan Brughouts, working together with TNO, Delft University of Technology and the Royal Marechaussee. Our project focuses on improving robotics in an open world settings through the use of foundational models.","tags":null,"title":"Thomas Wiggers","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ce34cc8adbfb1a577a0908396afbbab1","permalink":"https://ivi.fnwi.uva.nl/vislab/author/petra-venema/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/petra-venema/","section":"authors","summary":"  ","tags":null,"title":"Petra Venema","type":"authors"},{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"11dedac81c50502684a8220f10e294f2","permalink":"https://ivi.fnwi.uva.nl/vislab/author/riccardo-valperga/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/riccardo-valperga/","section":"authors","summary":"","tags":null,"title":"Riccardo Valperga","type":"authors"},{"authors":["admin"],"categories":null,"content":"I’m a PhD candidate at the VIS Lab, supervised by Cees Snoek, working on multimodal foundation models as part of the Horizon Europe ELLIOT project. My research focuses on designing unified architectures that combine multiple modalities into a shared space, aiming for models that generalize, adapt efficiently, and assist across diverse tasks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2fb8230b19a7be8312c1fde81c430136","permalink":"https://ivi.fnwi.uva.nl/vislab/author/dheeraj-varghese/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/dheeraj-varghese/","section":"authors","summary":"I’m a PhD candidate at the VIS Lab, supervised by Cees Snoek, working on multimodal foundation models as part of the Horizon Europe ELLIOT project. My research focuses on designing unified architectures that combine multiple modalities into a shared space, aiming for models that generalize, adapt efficiently, and assist across diverse tasks.","tags":null,"title":"Dheeraj Varghese","type":"authors"},{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5d38846ef14deeaf3f8204fd39d5b60f","permalink":"https://ivi.fnwi.uva.nl/vislab/author/haochen-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/haochen-wang/","section":"authors","summary":"","tags":null,"title":"Haochen Wang","type":"authors"},{"authors":["admin"],"categories":null,"content":"I obtained my Bachelor and Master degree from Beihang University in Beijing, China. During my master\u0026rsquo;s degree, my major research topic was crowd counting and density estimation. Now I\u0026rsquo;m a PhD candidate of AIM Lab in UvA and mainly focus on the early diagnosis of Alzhemer disease by deep neural networks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c3357e3eb2bca46a8f94f2d3b0dcb8ae","permalink":"https://ivi.fnwi.uva.nl/vislab/author/zehao-xiao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/zehao-xiao/","section":"authors","summary":"I obtained my Bachelor and Master degree from Beihang University in Beijing, China. During my master\u0026rsquo;s degree, my major research topic was crowd counting and density estimation. Now I\u0026rsquo;m a PhD candidate of AIM Lab in UvA and mainly focus on the early diagnosis of Alzhemer disease by deep neural networks.","tags":null,"title":"Zehao Xiao","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0c7a3ef1ccc4276c383c289649cb1b2e","permalink":"https://ivi.fnwi.uva.nl/vislab/author/pengwan-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/pengwan-yang/","section":"authors","summary":"  ","tags":null,"title":"Pengwan Yang","type":"authors"},{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0487891d03d96baa2c749870279d49b2","permalink":"https://ivi.fnwi.uva.nl/vislab/author/wenzhe-yin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/wenzhe-yin/","section":"authors","summary":"","tags":null,"title":"Wenzhe Yin","type":"authors"},{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a Postdoctoral Researcher in VISLab, University of Amsterdam, working with Efstratios Gavves. I\u0026rsquo;ve obtained PhD at Max Planck ETH Center for Learning Systems, Switzerland jointly supervised by Georg Martius and Fanny Yang.\nAdditionally, I\u0026rsquo;ve done several internships in Amazon AWS Lablets team supervised by Thomas Brox and Francesco Locallello.\nI have completed my Master studies in The University of Tübingen, Germany studying Computational Neuroscience and Machine Learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f0252b09d81efc86b7ac25aeac2a778f","permalink":"https://ivi.fnwi.uva.nl/vislab/author/andrii-zadaianchuk/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/andrii-zadaianchuk/","section":"authors","summary":"I\u0026rsquo;m a Postdoctoral Researcher in VISLab, University of Amsterdam, working with Efstratios Gavves. I\u0026rsquo;ve obtained PhD at Max Planck ETH Center for Learning Systems, Switzerland jointly supervised by Georg Martius and Fanny Yang.","tags":null,"title":"Andrii Zadaianchuk","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0427c828a1a55c45bf1391e564d7b50f","permalink":"https://ivi.fnwi.uva.nl/vislab/author/david-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/david-zhang/","section":"authors","summary":"  ","tags":null,"title":"David Zhang","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f4ce0d6413e7ff3c4796c8a2a5f30f9d","permalink":"https://ivi.fnwi.uva.nl/vislab/author/jiaojiao-zhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/jiaojiao-zhao/","section":"authors","summary":"  ","tags":null,"title":"Jiaojiao Zhao","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e20559cfff0f2ca5a7393768c31efae2","permalink":"https://ivi.fnwi.uva.nl/vislab/author/riaan-zoetmulder/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/riaan-zoetmulder/","section":"authors","summary":"  ","tags":null,"title":"Riaan Zoetmulder","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"445f34721ba94ededeecb9a61b3e9404","permalink":"https://ivi.fnwi.uva.nl/vislab/author/xiantong-zhen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/xiantong-zhen/","section":"authors","summary":"  ","tags":null,"title":"Xiantong Zhen","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ab672aaec854c166be615851dbcb76ea","permalink":"https://ivi.fnwi.uva.nl/vislab/author/yunhua-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/yunhua-zhang/","section":"authors","summary":"  ","tags":null,"title":"Yunhua Zhang","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"35088d471bf922d771a80c2118e40df2","permalink":"https://ivi.fnwi.uva.nl/vislab/author/kirill-gavrilyuk/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/kirill-gavrilyuk/","section":"authors","summary":"  ","tags":null,"title":"Kirill Gavrilyuk","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3859c3dbf8c42a46638d4a1ab00698aa","permalink":"https://ivi.fnwi.uva.nl/vislab/author/deepak-gupta/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/deepak-gupta/","section":"authors","summary":"  ","tags":null,"title":"Deepak Gupta","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"25b77c782bd5257f573e8fed309bbeef","permalink":"https://ivi.fnwi.uva.nl/vislab/author/noureldien-hussein/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/noureldien-hussein/","section":"authors","summary":"  ","tags":null,"title":"Noureldien Hussein","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6321626f014d8b7ff2a439b687f94888","permalink":"https://ivi.fnwi.uva.nl/vislab/author/berkay-kicanaoglu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/berkay-kicanaoglu/","section":"authors","summary":"  ","tags":null,"title":"Berkay Kicanaoglu","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e7a1f98ac8dbd22bc4f901365b13c1ac","permalink":"https://ivi.fnwi.uva.nl/vislab/author/shuai-liao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/shuai-liao/","section":"authors","summary":"  ","tags":null,"title":"Shuai Liao","type":"authors"},{"authors":["admin"],"categories":null,"content":"  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"84fb0ab911ea888cba7b88d7fd0fc8eb","permalink":"https://ivi.fnwi.uva.nl/vislab/author/tom-runia/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/author/tom-runia/","section":"authors","summary":"  ","tags":null,"title":"Tom Runia","type":"authors"},{"authors":["Jie Liu","Jiayi Shen","Pan Zhou","Jan-Jakob Sonke","Efstratios Gavves"],"categories":null,"content":"","date":1759276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1759276800,"objectID":"c78fdc80435758321e12c74924577c42","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/jie-iccv-2025/","publishdate":"2025-10-01T00:00:00Z","relpermalink":"/vislab/publication/jie-iccv-2025/","section":"publication","summary":"Generalized Few-Shot Semantic Segmentation (GFSS) aims to extend a segmentation model to novel classes with only a few annotated examples while maintaining performance on base classes. Recently, pretrained vision-language models (VLMs) such as CLIP have been leveraged in GFSS to improve generalization on novel classes through multi-modal prototypes learning. However, existing prototype-based methods are inherently deterministic, limiting the adaptability of learned prototypes to diverse samples, particularly for novel classes with scarce annotations. To address this, we propose FewCLIP, a probabilistic prototype calibration framework over multi-modal prototypes from the pretrained CLIP, thus providing more adaptive prototype learning for GFSS. Specifically, FewCLIP first introduces a prototype calibration mechanism, which refines frozen textual prototypes with learnable visual calibration prototypes, leading to a more discriminative and adaptive representation. Furthermore, unlike deterministic prototype learning techniques, FewCLIP introduces distribution regularization over these calibration prototypes. This probabilistic formulation ensures structured and uncertainty-aware prototype learning, effectively mitigating overfitting to limited novel class data while enhancing generalization. Extensive experimental results on PASCAL-5i and COCO-20i datasets demonstrate that our proposed FewCLIP significantly outperforms state-of-the-art approaches across both GFSS and class-incremental setting.","tags":["Transfer","low-shot","semi- and un-supervised learning"],"title":"Probabilistic Prototype Calibration of Vision-Language Models for Generalized Few-shot Semantic Segmentation","type":"publication"},{"authors":["Aritra Bhowmik","Mohammad Mahdi Derakhshani","Dennis Koelma","Yuki M. Asano","Martin R. Oswald","Cees G. M. Snoek"],"categories":null,"content":"","date":1759276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1759276800,"objectID":"4af0f0447a6ee52e6468d5fd19f3a52f","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/aritra-iccv-2025/","publishdate":"2025-10-01T00:00:00Z","relpermalink":"/vislab/publication/aritra-iccv-2025/","section":"publication","summary":"Spatial awareness is key to enable embodied multimodal AI systems. Yet, without vast amounts of spatial supervision, current Multimodal Large Language Models (MLLMs) struggle at this task. In this paper, we introduce TWIST \u0026 SCOUT, a framework that equips pre-trained MLLMs with visual grounding ability without forgetting their existing image and language understanding skills. To this end, we propose TWIST, a twin-expert stepwise tuning module that modifies the decoder of the language model using one frozen module pre-trained on image understanding tasks and another learnable one for visual grounding tasks. This allows the MLLM to retain previously learned knowledge and skills, while acquiring what is missing. To fine-tune the model effectively, we generate a high-quality synthetic dataset we call SCOUT, which mimics human reasoning in visual grounding. This dataset provides rich supervision signals, describing a step-by-step multimodal reasoning process, thereby simplifying the task of visual grounding. We evaluate our approach on several standard benchmark datasets, encompassing grounded image captioning, zero-shot localization, and visual grounding tasks. Our method consistently delivers strong performance across all tasks, while retaining the pre-trained image understanding capabilities.","tags":["Multi-modal learning"],"title":"TWIST \u0026 SCOUT: Grounding Multimodal LLM-Experts by Forget-Free Tuning","type":"publication"},{"authors":["Jie Liu","Pan Zhou","Zehao Xiao","Jiayi Shen","Wenzhe Yin","Jan-Jakob Sonke","Efstratios Gavves"],"categories":null,"content":"","date":1751328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751328000,"objectID":"1020692b3009e885013b5bd14114ab6c","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/jie-icml-2025/","publishdate":"2025-07-01T00:00:00Z","relpermalink":"/vislab/publication/jie-icml-2025/","section":"publication","summary":"Interactive 3D segmentation has emerged as a promising solution for generating accurate object masks in complex 3D scenes by incorporating user-provided clicks. However, two critical challenges remain underexplored: (1) effectively generalizing from sparse user clicks to produce accurate segmentation, and (2) quantifying predictive uncertainty to help users identify unreliable regions. In this work, we propose NPISeg3D, a novel probabilistic framework that builds upon Neural Processes (NPs) to address these challenges. Specifically, NPISeg3D introduces a hierarchical latent variable structure with scene-specific and object-specific latent variables to enhance few-shot generalization by capturing both global context and object-specific characteristics. Additionally, we design a probabilistic prototype modulator that adaptively modulates click prototypes with object-specific latent variables, improving the model's ability to capture object-aware context and quantify predictive uncertainty. Experiments on four 3D point cloud datasets demonstrate that NPISeg3D achieves superior segmentation performance with fewer clicks while providing reliable uncertainty estimations.","tags":["3D computer vision"],"title":"Probabilistic Interactive 3D Segmentation with Hierarchical Neural Processes","type":"publication"},{"authors":["Cansu Sancaktar","Christian Gumbsch","Andrii Zadaianchuk","Pavel Kolev","Georg Martius"],"categories":null,"content":"","date":1751328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751328000,"objectID":"6160107ad573b39d3cb746f78c2bda22","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/andrii-icml-2025/","publishdate":"2025-07-01T00:00:00Z","relpermalink":"/vislab/publication/andrii-icml-2025/","section":"publication","summary":"Exploration is a cornerstone of reinforcement learning (RL). Intrinsic motivation attempts to decouple exploration from external, task-based rewards. However, established approaches to intrinsic motivation that follow general principles such as information gain, often only uncover low-level interactions. In contrast, children's play suggests that they engage in meaningful high-level behavior by imitating or interacting with their caregivers. Recent work has focused on using foundation models to inject these semantic biases into exploration. However, these methods often rely on unrealistic assumptions, such as language-embedded environments or access to high-level actions. We propose SEmaNtically Sensible ExploratIon (SENSEI), a framework to equip model-based RL agents with an intrinsic motivation for semantically meaningful behavior. SENSEI distills a reward signal of interestingness from Vision Language Model (VLM) annotations, enabling an agent to predict these rewards through a world model. Using model-based RL, SENSEI trains an exploration policy that jointly maximizes semantic rewards and uncertainty. We show that in both robotic and video game-like simulations SENSEI discovers a variety of meaningful behaviors from image observations and low-level actions. SENSEI provides a general tool for learning from foundation model feedback, a crucial research direction, as VLMs become more powerful.","tags":["World models"],"title":"SENSEI: Semantic Exploration Guided by Foundation Models to Learn Versatile World Models","type":"publication"},{"authors":["Huabin Liu","Filip Ilievski","Cees G. M. Snoek"],"categories":null,"content":"","date":1748736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748736000,"objectID":"3727b0ce5b4f79578bff3f947691b709","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/cees-cvpr-2025/","publishdate":"2025-06-01T00:00:00Z","relpermalink":"/vislab/publication/cees-cvpr-2025/","section":"publication","summary":"This paper proposes the first video-grounded entailment tree reasoning method for commonsense video question answering (VQA). Despite the remarkable progress of large visual-language models (VLMs), there are growing concerns that they learn spurious correlations between videos and likely answers, reinforced by their black-box nature and remaining benchmarking biases. Our method explicitly grounds VQA tasks to video fragments in four steps: entailment tree construction, video-language entailment verification, tree reasoning, and dynamic tree expansion. A vital benefit of the method is its generalizability to current video and image-based VLMs across reasoning types. To support fair evaluation, we devise a de-biasing procedure based on large-language models that rewrites VQA benchmark answer sets to enforce model reasoning. Systematic experiments on existing and de-biased benchmarks highlight the impact of our method components across benchmarks, VLMs, and reasoning types.","tags":["Visual reasoning and logical representation"],"title":"Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning","type":"publication"},{"authors":["Aniket Didolkar","Andrii Zadaianchuk","Rabiul Awal","Maximilian Seitzer","Efstratios Gavves","Aishwarya Agrawal"],"categories":null,"content":"","date":1748736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748736000,"objectID":"4eb39dbb4388117024b6fe5fce61a8b0","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/andrii-cvpr-2025-2/","publishdate":"2025-06-01T00:00:00Z","relpermalink":"/vislab/publication/andrii-cvpr-2025-2/","section":"publication","summary":"Object-centric representation learning aims to decompose visual scenes into fixed-size vectors called \"slots\" or \"object files\", where each slot captures a distinct object. Current state-of-the-art object-centric models have shown remarkable success in object discovery in diverse domains, including complex real-world scenes. However, these models suffer from a key limitation: they lack controllability. Specifically, current object-centric models learn representations based on their preconceived understanding of objects, without allowing user input to guide which objects are represented. Introducing controllability into object-centric models could unlock a range of useful capabilities, such as the ability to extract instance-specific representations from a scene. In this work, we propose a novel approach for user-directed control over slot representations by conditioning slots on language descriptions. The proposed ConTRoLlable Object-centric representation learning approach, which we term CTRL-O, achieves targeted object-language binding in complex real-world scenes without requiring mask supervision. Next, we apply these controllable slot representations on two downstream vision language tasks: text-to-image generation and visual question answering. The proposed approach enables instance-specific text-to-image generation and also achieves strong performance on visual question answering.","tags":["Multi-modal learning"],"title":"CTRL-O: Language-Controllable Object-Centric Visual Representation Learning","type":"publication"},{"authors":["Tobia Poppi*","Tejaswi Kasarla*","Pascal Mettes","Lorenzo Baraldi","Rita Cucchiara"],"categories":null,"content":"","date":1748736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748736000,"objectID":"2dec46c88e40d4cbe7197ff03530d85e","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/tejaswi-cvpr-2025/","publishdate":"2025-06-01T00:00:00Z","relpermalink":"/vislab/publication/tejaswi-cvpr-2025/","section":"publication","summary":"Addressing the retrieval of unsafe content from vision-language models such as CLIP is an important step towards real-world integration. Current efforts have relied on unlearning techniques that try to erase the model's knowledge of unsafe concepts. While effective in reducing unwanted outputs, unlearning limits the model's capacity to discern between safe and unsafe content. In this work, we introduce a novel approach that shifts from unlearning to an awareness paradigm by leveraging the inherent hierarchical properties of the hyperbolic space. We propose to encode safe and unsafe content as an entailment hierarchy, where both are placed in different regions of hyperbolic space. Our HySAC, Hyperbolic Safety-Aware CLIP, employs entailment loss functions to model the hierarchical and asymmetrical relations between safe and unsafe image-text pairs. This modelling, ineffective in standard vision-language models due to their reliance on Euclidean embeddings, endows the model with awareness of unsafe content, enabling it to serve as both a multimodal unsafe classifier and a flexible content retriever, with the option to dynamically redirect unsafe queries toward safer alternatives or retain the original output. Extensive experiments show that our approach not only enhances safety recognition but also establishes a more adaptable and interpretable framework for content moderation in vision-language models.","tags":["Multi-modal learning"],"title":"Hyperbolic Safety-Aware Vision-Language Models","type":"publication"},{"authors":["Anna Manasyan","Maximilian Seitzer","Filip Radovic","Georg Martius","Andrii Zadaianchuk"],"categories":null,"content":"","date":1748736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748736000,"objectID":"3bab77dae41e0213ea4670f91e8d6161","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/andrii-cvpr-2025/","publishdate":"2025-06-01T00:00:00Z","relpermalink":"/vislab/publication/andrii-cvpr-2025/","section":"publication","summary":"Unsupervised object-centric learning from videos is a promising approach to extract structured representations from large, unlabeled collections of videos. To support downstream tasks like autonomous control, these representations must be both compositional and temporally consistent. Existing approaches based on recurrent processing often lack long-term stability across frames because their training objective does not enforce temporal consistency. In this work, we introduce a novel object-level temporal contrastive loss for video object-centric models that explicitly promotes temporal consistency. Our method significantly improves the temporal consistency of the learned object-centric representations, yielding more reliable video decompositions that facilitate challenging downstream tasks such as unsupervised object dynamics prediction. Furthermore, the inductive bias added by our loss strongly improves object discovery, leading to state-of-the-art results on both synthetic and real-world datasets, outperforming even weakly-supervised methods that leverage motion masks as additional cues.","tags":["Motion and tracking"],"title":"Temporally Consistent Object-Centric Learning by Contrasting Slots","type":"publication"},{"authors":["Duy-Kien Nguyen","Mahmoud Assran","Unnat Jain","Martin R. Oswald","Cees G. M. Snoek","Xinlei Chen"],"categories":null,"content":"","date":1743465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743465600,"objectID":"2784d33cc9edfaae2ad6cb905ef992d0","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/duy-kien-iclr-2025/","publishdate":"2025-04-01T00:00:00Z","relpermalink":"/vislab/publication/duy-kien-iclr-2025/","section":"publication","summary":"This work does not introduce a new method. Instead, we present an interesting finding that questions the necessity of the inductive bias of locality in modern computer vision architectures. Concretely, we find that vanilla Transformers can operate by directly treating each individual pixel as a token and achieve highly performant results. This is substantially different from the popular design in Vision Transformer, which maintains the inductive bias from ConvNets towards local neighborhoods (e.g. by treating each 16x16 patch as a token). We showcase the effectiveness of pixels-as-tokens across three well-studied computer vision tasks: supervised learning for classification and regression, self-supervised learning via masked autoencoding, and image generation with diffusion models. Although it's computationally less practical to directly operate on individual pixels, we believe the community must be made aware of this surprising piece of knowledge when devising the next generation of neural network architectures for computer vision.","tags":["Vision application"],"title":"An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels","type":"publication"},{"authors":["Diego Garcia Cerdas","Christina Sartzetaki","Magnus Petersen","Gemma Roig","Pascal Mettes","Iris I.A. Groen"],"categories":null,"content":"","date":1743465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743465600,"objectID":"cc61e5f5d6c3952a0926eb3418d16e09","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/christina-iclr-2025-2/","publishdate":"2025-04-01T00:00:00Z","relpermalink":"/vislab/publication/christina-iclr-2025-2/","section":"publication","summary":"The human brain efficiently represents visual inputs through specialized neural populations that selectively respond to specific categories. Advancements in generative modeling have enabled data-driven discovery of neural selectivity using brain-optimized image synthesis. However, current methods independently generate one sample at a time, without enforcing structural constraints on the generations; thus, these individual images have no explicit point of comparison, making it hard to discern which image features drive neural response selectivity. To address this issue, we introduce Brain Activation Control Through Image Variation (BrainACTIV), a method for manipulating a reference image to enhance or decrease activity in a target cortical region using pretrained diffusion models. Starting from a reference image allows for fine-grained and reliable offline identification of optimal visuo-semantic properties, as well as producing controlled stimuli for novel neuroimaging studies. We show that our manipulations effectively modulate predicted fMRI responses and agree with hypothesized preferred categories in established regions of interest, while remaining structurally close to the reference image. Moreover, we demonstrate how our method accentuates differences between brain regions that are selective to the same category, and how it could be used to explore neural representation of brain regions with unknown selectivities. Hence, BrainACTIV holds the potential to formulate robust hypotheses about brain representation and to facilitate the production of naturalistic stimuli for neuroscientific experiments.","tags":["Generative model","Neuroscience","Image manipulation"],"title":"BrainACTIV: Identifying visuo-semantic properties driving cortical selectivity using diffusion-based image manipulation","type":"publication"},{"authors":["Jie Liu","Pan Zhou","Yinjun Du","Ah-Hwee Tan","Cees G.M. Snoek","Jan-Jakob Sonke","Efstratios Gavves"],"categories":null,"content":"","date":1743465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743465600,"objectID":"68e12567a38450092912250517884f17","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/jie-iclr-2025/","publishdate":"2025-04-01T00:00:00Z","relpermalink":"/vislab/publication/jie-iclr-2025/","section":"publication","summary":"In this work, we address the cooperation problem among large language model (LLM) based embodied agents, where agents must cooperate to achieve a common goal. Previous methods often execute actions extemporaneously and incoherently, without long-term strategic and cooperative planning, leading to redundant steps, failures, and even serious repercussions in complex tasks like search-and-rescue missions where discussion and cooperative plan are crucial. To solve this issue, we propose Cooperative Plan Optimization (CaPo) to enhance the cooperation effi- ciency of LLM-based embodied agents. Inspired by human cooperation schemes, CaPo improves cooperation efficiency with two phases: 1) meta-plan generation, and 2) progress-adaptive meta-plan and execution. In the first phase, all agents analyze the task, discuss, and cooperatively create a meta-plan that decomposes the task into subtasks with detailed steps, ensuring a long-term strategic and co- herent plan for efficient coordination. In the second phase, agents execute tasks according to the meta-plan and dynamically adjust it based on their latest progress (e.g., discovering a target object) through multi-turn discussions. This progress- based adaptation eliminates redundant actions, improving the overall cooperation efficiency of agents. Experimental results on the ThreeDworld Multi-Agent Trans- port and Communicative Watch-And-Help tasks demonstrate CaPo’s much higher task completion rate and efficiency compared with state-of-the-arts. The code is released at https://github.com/jliu4ai/CaPo.","tags":["Embodied Agent","Multi-Agent Cooperation"],"title":"CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent Cooperation","type":"publication"},{"authors":["Avik Pal","Max van Spengler","Guido Maria D'Amely di Melendugno","Alessandro Flaborea","Fabio Galasso","Pascal Mettes"],"categories":null,"content":"","date":1743465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743465600,"objectID":"8437aadec18f51bf76fe33b58f067a95","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/max-iclr-2025/","publishdate":"2025-04-01T00:00:00Z","relpermalink":"/vislab/publication/max-iclr-2025/","section":"publication","summary":"Image-text representation learning forms a cornerstone in vision-language models, where pairs of images and textual descriptions are contrastively aligned in a shared embedding space. Since visual and textual concepts are naturally hierarchical, recent work has shown that hyperbolic space can serve as a high-potential manifold to learn vision-language representation with strong downstream performance. In this work, for the first time we show how to fully leverage the innate hierarchical nature of hyperbolic embeddings by looking beyond individual image-text pairs. We propose Compositional Entailment Learning for hyperbolic vision-language models. The idea is that an image is not only described by a sentence but is itself a composition of multiple object boxes, each with their own textual description. Such information can be obtained freely by extracting nouns from sentences and using openly available localized grounding models. We show how to hierarchically organize images, image boxes, and their textual descriptions through contrastive and entailment-based objectives. Empirical evaluation on a hyperbolic vision-language model trained with millions of image-text pairs shows that the proposed compositional learning approach outperforms conventional Euclidean CLIP learning, as well as recent hyperbolic alternatives, with better zero-shot and retrieval generalization and clearly stronger hierarchical performance. Code available at https://github.com/PalAvik/hycoclip.","tags":["Multi-modal learning"],"title":"Compositional entailment learning for hyperbolic vision-language models","type":"publication"},{"authors":["Leonardo Barcellona","Andrii Zadaianchuk","Davide Allegro","Samuele Papa","Stefano Ghidoni","Efstratios Gavves"],"categories":null,"content":"","date":1743465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743465600,"objectID":"f2ed7f9ebb5b5f4de3d06cf944364641","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/leonardo-iclr-2025/","publishdate":"2025-04-01T00:00:00Z","relpermalink":"/vislab/publication/leonardo-iclr-2025/","section":"publication","summary":"A world model provides an agent with a representation of its environment, enabling it to predict the causal consequences of its actions. Current world models typically cannot directly and explicitly imitate the actual environment in front of a robot, often resulting in unrealistic behaviors and hallucinations that make them unsuitable for real-world robotics applications. To overcome those challenges, we propose to rethink robot world models as learnable digital twins. We introduce DreMa, a new approach for constructing digital twins automatically using learned explicit representations of the real world and its dynamics, bridging the gap between traditional digital twins and world models. DreMa replicates the observed world and its structure by integrating Gaussian Splatting and physics simulators, allowing robots to imagine novel configurations of objects and to predict the future consequences of robot actions thanks to its compositionality. We leverage this capability to generate new data for imitation learning by applying equivariant transformations to a small set of demonstrations. Our evaluations across various settings demonstrate significant improvements in accuracy and robustness by incrementing actions and object distributions, reducing the data needed to learn a policy and improving the generalization of the agents. As a highlight, we show that a real Franka Emika Panda robot, powered by DreMa’s imagination, can successfully learn novel physical tasks from just a single example per task variation (one-shot policy learning). Our project page can be found in: https://dreamtomanipulate.github.io/.","tags":["Robot learning"],"title":"Dream to Manipulate: Compositional World Models Empowering Robot Imitation Learning with Imagination","type":"publication"},{"authors":["Zehao Xiao","Shilin Yan","Jack Hong","Jiayin Cai","Xiaolong Jiang","Yao Hu","Jiayi Shen","Qi (Cheems) Wang","Cees Snoek"],"categories":null,"content":"","date":1743465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743465600,"objectID":"aaf043d6747c79f34493980d9feed066","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/zehao-iclr-2025/","publishdate":"2025-04-01T00:00:00Z","relpermalink":"/vislab/publication/zehao-iclr-2025/","section":"publication","summary":"Test-time prompt tuning enhances zero-shot generalization of vision-language models but tends to ignore the relatedness among test samples during inference. Online test-time prompt tuning provides a simple way to leverage the information in previous test samples, albeit with the risk of prompt collapse due to error accumulation. To enhance test-time prompt tuning, we propose DynaPrompt, short for dynamic test-time prompt tuning, exploiting relevant data distribution information while reducing error accumulation. Built on an online prompt buffer, DynaPrompt adaptively selects and optimizes the relevant prompts for each test sample during tuning. Specifically, we introduce a dynamic prompt selection strategy based on two metrics: prediction entropy and probability difference. For unseen test data information, we develop dynamic prompt appending, which allows the buffer to append new prompts and delete the inactive ones. By doing so, the prompts are optimized to exploit beneficial information on specific test data, while alleviating error accumulation. Experiments on fourteen datasets demonstrate the effectiveness of dynamic test-time prompt tuning.","tags":["Transfer","low-shot","semi- and un-supervised learning"],"title":"DynaPrompt: Dynamic Test-Time Prompt Tuning","type":"publication"},{"authors":["John Gkountouras","Matthias Lindemann","Phillip Lippe","Efstratios Gavves","Ivan Titov"],"categories":null,"content":"","date":1743465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743465600,"objectID":"e92b0d9241c9e5f8256f0d68490ff674","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/phillip-iclr-2025/","publishdate":"2025-04-01T00:00:00Z","relpermalink":"/vislab/publication/phillip-iclr-2025/","section":"publication","summary":"Large Language Models (LLMs) have recently shown great promise in planning and reasoning applications. These tasks demand robust systems, which arguably require a causal understanding of the environment. While LLMs can acquire and reflect common sense causal knowledge from their pretraining data, this information is often incomplete, incorrect, or inapplicable to a specific environment. In contrast, causal representation learning (CRL) focuses on identifying the underlying causal structure within a given environment. We propose a framework that integrates CRLs with LLMs to enable causally-aware reasoning and planning. This framework learns a causal world model, with causal variables linked to natural language expressions. This mapping provides LLMs with a flexible interface to process and generate descriptions of actions and states in text form. Effectively, the causal world model acts as a simulator that the LLM can query and interact with. We evaluate the framework on causal inference and planning tasks across temporal scales and environmental complexities. Our experiments demonstrate the effectiveness of the approach, with the causally-aware method outperforming LLM-based reasoners, especially for longer planning horizons.","tags":["Generative model"],"title":"Language Agents Meet Causality -- Bridging LLMs and Causal World Models","type":"publication"},{"authors":["Valentinos Pariza","Mohammadreza Salehi","Gertjan J. Burghouts","Francesco Locatello","Yuki M. Asano"],"categories":null,"content":"","date":1743465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743465600,"objectID":"93420759cc7bb27bac78bca961142f85","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mohammadreza-iclr-2025/","publishdate":"2025-04-01T00:00:00Z","relpermalink":"/vislab/publication/mohammadreza-iclr-2025/","section":"publication","summary":"We introduce NeCo: Patch Neighbor Consistency, a novel self-supervised training loss that enforces patch-level nearest neighbor consistency across a student and teacher model. Compared to contrastive approaches that only yield binary learning signals, i.e. 'attract' and 'repel', this approach benefits from the more fine-grained learning signal of sorting spatially dense features relative to reference patches. Our method leverages differentiable sorting applied on top of pretrained representations, such as DINOv2-registers to bootstrap the learning signal and further improve upon them. This dense post-pretraining leads to superior performance across various models and datasets, despite requiring only 19 hours on a single GPU. This method generates high-quality dense feature encoders and establishes several new state-of-the-art results such as +2.3 % and +4.2% for non-parametric in-context semantic segmentation on ADE20k and Pascal VOC, +1.6% and +4.8% for linear segmentation evaluations on COCO-Things and -Stuff and improvements in the 3D understanding of multi-view consistency on SPair-71k, by more than 1.5%.","tags":["Self-supervised learning"],"title":"Near, far: Patch-ordering enhances vision foundation models' scene understanding","type":"publication"},{"authors":["Aniket Rajiv Didolkar","Andrii Zadaianchuk","Anirudh Goyal","Michael Curtis Mozer","Yoshua Bengio","Georg Martius","Maximilian Seitzer"],"categories":null,"content":"","date":1743465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743465600,"objectID":"a85f35ef6f027a5f297c3728b7cd4c79","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/andrii-iclr-2025/","publishdate":"2025-04-01T00:00:00Z","relpermalink":"/vislab/publication/andrii-iclr-2025/","section":"publication","summary":"The goal of object-centric representation learning is to decompose visual scenes into a structured representation that isolates the entities into individual vectors. Recent successes have shown that object-centric representation learning can be scaled to real-world scenes by utilizing features from pre-trained foundation models like DINO. However, so far, these object-centric methods have mostly been applied in-distribution, with models trained and evaluated on the same dataset. This is in contrast to the underlying foundation models, which have been shown to be applicable to a wide range of data and tasks. Thus, in this work, we answer the question of whether current real-world capable object-centric methods exhibit similar levels of transferability by introducing a benchmark comprising seven different synthetic and real-world datasets. We analyze the factors influencing performance under transfer and find that training on diverse real-world images improves generalization to unseen scenarios. Furthermore, inspired by the success of task-specific fine-tuning in foundation models, we introduce a novel fine-tuning strategy to adapt pre-trained vision encoders for the task of object discovery. We find that the proposed approach results in state-of-the-art performance for unsupervised object discovery, exhibiting strong zero-shot transfer to unseen datasets.","tags":["Transfer","low-shot","semi- and un-supervised learning"],"title":"On the Transfer of Object-Centric Representation Learning","type":"publication"},{"authors":["Christina Sartzetaki","Gemma Roig","Cees G. M. Snoek","Iris I.A. Groen"],"categories":null,"content":"","date":1743465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743465600,"objectID":"11317001c3a28462e502f13fe4f56523","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/christina-iclr-2025/","publishdate":"2025-04-01T00:00:00Z","relpermalink":"/vislab/publication/christina-iclr-2025/","section":"publication","summary":"What can we learn from comparing video models to human brains, arguably the most efficient and effective video processing systems in existence? Our work takes a step towards answering this question by performing the first large-scale benchmarking of deep video models on representational alignment to the human brain, using publicly available models and a recently released video brain imaging (fMRI) dataset. We disentangle four factors of variation in the models (temporal modeling, classification task, architecture, and training dataset) that affect alignment to the brain, which we measure by conducting Representational Similarity Analysis across multiple brain regions and model layers. We show that temporal modeling is key for alignment to brain regions involved in early visual processing, while a relevant classification task is key for alignment to higher-level regions. Moreover, we identify clear differences between the brain scoring patterns across layers of CNNs and Transformers, and reveal how training dataset biases transfer to alignment with functionally selective brain areas. Additionally, we uncover a negative correlation of computational complexity to brain alignment. Measuring a total of 99 neural networks and 10 human brains watching videos, we aim to forge a path that widens our understanding of temporal and semantic video representations in brains and machines, ideally leading towards more efficient video models and more mechanistic explanations of processing in the human brain.","tags":["Video understanding","Neuroscience","Representational alignment"],"title":"One Hundred Neural Networks and Brains Watching Videos: Lessons from Alignment","type":"publication"},{"authors":["Ivona Najdenkoska*","Mohammad Mahdi Derakhshani*","Yuki M. Asano","Nanne van Noord","Marcel Worring","Cees G. M. Snoek"],"categories":null,"content":"","date":1743465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743465600,"objectID":"4601de3842b21add311bed61b17c518a","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mohammad-iclr-2025/","publishdate":"2025-04-01T00:00:00Z","relpermalink":"/vislab/publication/mohammad-iclr-2025/","section":"publication","summary":"We address the challenge of representing long captions in vision-language models, such as CLIP. By design these models are limited by fixed, absolute positional encodings, restricting inputs to a maximum of 77 tokens and hindering performance on tasks requiring longer descriptions. Although recent work has attempted to overcome this limit, their proposed approaches struggle to model token relationships over longer distances and simply extend to a fixed new token length. Instead, we propose a generalizable method, named TULIP, able to upgrade the token length to any length for CLIP-like models. We do so by improving the architecture with relative position encodings, followed by a training procedure that (i) distills the original CLIP text encoder into an encoder with relative position encodings and (ii) enhances the model for aligning longer captions with images. By effectively encoding captions longer than the default 77 tokens, our model outperforms baselines on cross-modal tasks such as retrieval and text-to-image generation.","tags":["Multi-modal learning"],"title":"TULIP: Token-length Upgraded CLIP","type":"publication"},{"authors":["Aritra Bhowmik","Pascal Mettes","Martin R. Oswald","Cees G. M. Snoek"],"categories":null,"content":"","date":1743465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743465600,"objectID":"22e72cb371882457ae719e6e67ebce80","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/aritra-iclr-2025/","publishdate":"2025-04-01T00:00:00Z","relpermalink":"/vislab/publication/aritra-iclr-2025/","section":"publication","summary":"This paper revisits the problem of predicting box locations in object detection architectures. Typically, each box proposal or box query aims to directly maximize the intersection-over-union score with the ground truth, followed by a winner-takes-all non-maximum suppression where only the highest scoring box in each region is retained. We observe that both steps are sub-optimal: the first involves regressing proposals to the entire ground truth, which is a difficult task even with large receptive fields, and the second neglects valuable information from boxes other than the top candidate. Instead of regressing proposals to the whole ground truth, we propose a simpler approach: regress only to the area of intersection between the proposal and the ground truth. This avoids the need for proposals to extrapolate beyond their visual scope, improving localization accuracy. Rather than adopting a winner-takes-all strategy, we take the union over the regressed intersections of all boxes in a region to generate the final box outputs. Our plug-and-play method integrates seamlessly into proposal-based, grid-based, and query-based detection architectures with minimal modifications, consistently improving object localization and instance segmentation. We demonstrate its broad applicability and versatility across various detection and segmentation tasks.","tags":["Object detection","Object categorization"],"title":"Union-over-Intersections: Object Detection beyond Winner-Takes-All","type":"publication"},{"authors":["Sameer Ambekar","Zehao Xiao","Xiantong Zhen","Cees G. M. Snoek"],"categories":[],"content":"","date":1740528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740528000,"objectID":"0ce396f56546398e20a07dda893b19ee","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/sameer-wacv-2025/","publishdate":"2025-02-26T00:00:00Z","relpermalink":"/vislab/publication/sameer-wacv-2025/","section":"publication","summary":"We consider the problem of test-time domain generalization, where a model is trained on several source domains and adjusted on target domains never seen during training. Different from the common methods that fine-tune the model or adjust the classifier parameters online, we propose to generate multiple layer parameters on the fly during inference by a lightweight meta-learned transformer, which we call GeneralizeFormer. The layer-wise parameters are generated per target batch without fine-tuning or online adjustment. By doing so, our method is more effective in dynamic scenarios with multiple target distributions and also avoids forgetting valuable source distribution characteristics. Moreover, by considering layer-wise gradients, the proposed method adapts itself to various distribution shifts. To reduce the computational and time cost, we fix the convolutional parameters while only generating parameters of the Batch Normalization layers and the linear classifier. Experiments on six widely used domain generalization datasets demonstrate the benefits and abilities of the proposed method to efficiently handle various distribution shifts, generalize in dynamic scenarios, and avoid forgetting.","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"GeneralizeFormer: Layer-Adaptive Model Generation across Test-Time Distribution Shifts","type":"publication"},{"authors":["Yingjun Du","Wenfang Sun","Cees G. M. Snoek"],"categories":null,"content":"","date":1735084800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735084800,"objectID":"b6e9a6ccfe107f842d058232c19beee1","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yingjun-neurips-2024/","publishdate":"2024-12-25T00:00:00Z","relpermalink":"/vislab/publication/yingjun-neurips-2024/","section":"publication","summary":"Pre-trained vision-language models like CLIP have remarkably adapted to various downstream tasks. Nonetheless, their performance heavily depends on the specificity of the input text prompts, which requires skillful prompt template engineering. Instead, current approaches to prompt optimization learn the prompts through gradient descent, where the prompts are treated as adjustable parameters. However, these methods tend to lead to overfitting of the base classes seen during training and produce prompts that are no longer understandable by humans. This paper introduces a simple but interpretable prompt optimizer (IPO), that utilizes large language models (LLMs) to generate textual prompts dynamically. We introduce a Prompt Optimization Prompt that not only guides LLMs in creating effective prompts but also stores past prompts with their performance metrics, providing rich in-context information. Additionally, we incorporate a large multimodal model (LMM) to condition on visual content by generating image descriptions, which enhance the interaction between textual and visual modalities. This allows for the creation of dataset-specific prompts that improve generalization performance, while maintaining human comprehension. Extensive testing across 11 datasets reveals that IPO not only improves the accuracy of existing gradient-descent-based prompt learning methods but also considerably enhances the interpretability of the generated prompts. By leveraging the strengths of LLMs, our approach ensures that the prompts remain human-understandable, thereby facilitating better transparency and oversight for vision-language models.","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"IPO: Interpretable Prompt Optimization for Vision-Language Models","type":"publication"},{"authors":["Walter Simoncini","Spyros Gidaris","Andrei Bursuc","Yuki M. Asano"],"categories":null,"content":"","date":1735084800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735084800,"objectID":"03553dbf4589a9f6ac159b9789aa108c","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yuki-neurips-2024/","publishdate":"2024-12-25T00:00:00Z","relpermalink":"/vislab/publication/yuki-neurips-2024/","section":"publication","summary":"This paper introduces FUNGI, Features from UNsupervised GradIents, a method to enhance the features of transformer encoders by leveraging self-supervised gradients. Our method is simple: given any pretrained model, we first compute gradients from various self-supervised objectives for each input. These gradients are projected to a lower dimension and then concatenated with the model's output embedding. The resulting features are evaluated on k-nearest neighbor classification over 11 datasets from vision, 5 from natural language processing, and 2 from audio. Across backbones spanning various sizes and pretraining strategies, FUNGI features provide consistent performance improvements over the embeddings. We also show that using FUNGI features can benefit linear classification, clustering and image retrieval, and that they significantly improve the retrieval-based in-context scene understanding abilities of pretrained models, for example improving upon DINO by +17% for semantic segmentation - without any training. ","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen Representations","type":"publication"},{"authors":["David Knigge","David Wessels","Riccardo Valperga","Samuele Papa","Jan-Jakob Sonke","Efstratios Gavves","Erik J. Bekkers"],"categories":null,"content":"","date":1735084800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735084800,"objectID":"d7bc4b522ebf23672a6d6d208ca62e6d","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/d.m.knigge-neurips-2024/","publishdate":"2024-12-25T00:00:00Z","relpermalink":"/vislab/publication/d.m.knigge-neurips-2024/","section":"publication","summary":"Building on recent work using neural fields as representation for PDE solving, we investigate how to incorporate symmetries that often occur in physical data into a framework for continuous PDE solving by using Equivariant Neural Fields. We obtain impressive performance increases, especially over complicated geometries, which we attribute to the marked reduction in modelling complexity when respecting a PDE's symmetries.","tags":["Continuous methods"],"title":"Space-time continuous pde forecasting using equivariant neural fields","type":"publication"},{"authors":["Ricardo E. Gonzalez Valenzuela","Pascal Mettes","Bruno G. Loos","Henk Marquering","Erwin Berkhout"],"categories":[],"content":"","date":1730419200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730419200,"objectID":"42e526dbd4417500b52e69e0a9e3415e","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pascal-bmc-2024/","publishdate":"2024-11-01T00:00:00Z","relpermalink":"/vislab/publication/pascal-bmc-2024/","section":"publication","summary":"Proximal caries datasets for training artificial intelligence (AI) algorithms commonly include clinician-annotated radiographs. These conventional annotations are susceptible to observer variability, and early caries may be missed. Micro-computed tomography (CT), while not feasible in clinical applications, offers a more accurate imaging modality to support the creation of a reference-standard dataset for caries annotations. Herein, we present the Academic Center for Dentistry Amsterdam—Diagnostic Insights for Radiographic Early-caries with micro-CT (ACTA-DIRECT) dataset, which is the first dataset pairing dental radiographs and micro-CT scans to enable higher-quality annotations. ","tags":["Recognition (object detection, categorization)"],"title":"Enhancement of early proximal caries annotations in radiographs: introducing the Diagnostic Insights for Radiographic Early-caries with micro-CT (ACTA-DIRECT) dataset","type":"publication"},{"authors":["Guido M. D’Amely di Melendugno","Alessandro Flaborea","Pascal Mettes","Fabio Galasso"],"categories":[],"content":"","date":1727740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1727740800,"objectID":"4b8f442c7d08729cee612d5f432fe0b6","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pascal-iros-2024/","publishdate":"2024-10-01T00:00:00Z","relpermalink":"/vislab/publication/pascal-iros-2024/","section":"publication","summary":"Autonomous robots are increasingly becoming a strong fixture in social environments. Effective crowd navigation requires not only safe yet fast planning, but should also enable interpretability and computational efficiency for working in real-time on embedded devices. In this work, we advocate for hyperbolic learning to enable crowd navigation and we introduce Hyp 2 Nav. Different from conventional reinforcement learning-based crowd navigation methods, Hyp 2 Nav leverages the intrinsic properties of hyperbolic geometry to better encode the hierarchical nature of decision-making processes in navigation tasks. We propose a hyperbolic policy model and a hyperbolic curiosity module that results in effective social navigation, best success rates, and returns across multiple simulation settings, using up to 6 times fewer parameters than competitor state-of-the-art models. With our approach, it becomes even possible to obtain policies that work in 2-dimensional embedding spaces, opening up new possibilities for low-resource crowd navigation and model interpretability. Insightfully, the internal hyperbolic representation of Hyp 2 Nav correlates with how much attention the robot pays to the surrounding crowds, e.g. due to multiple people occluding its pathway or to a few of them showing colliding plans, rather than to its own planned route. The code is available at https://github.com/GDam90/hyp2nav.","tags":["Reinforcement learning"],"title":"Hyp 2 Nav: Hyperbolic Planning and Curiosity for Crowd Navigation","type":"publication"},{"authors":["Ryo Nakamura","Ryu Tadokoro","Ryosuke Yamada","Yuki M. Asano","Iro Laina","Christian Rupprecht","Nakamasa Inoue","Rio Yokota","Hirokatsu Kataoka"],"categories":[],"content":"","date":1725926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725926400,"objectID":"1f15164286280458f0ccedf43fe440d1","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yuki-eccv-2024/","publishdate":"2024-09-10T00:00:00Z","relpermalink":"/vislab/publication/yuki-eccv-2024/","section":"publication","summary":"Pre-training and transfer learning are an important building block of current computer vision systems. While pre-training is usually performed on large real-world image datasets, in this paper we ask whether this is truly necessary. To this end, we search for a minimal, purely synthetic pre-training dataset that allows us to achieve performance similar to the 1 million images of ImageNet-1k. We construct such a dataset from a single fractal with perturbations. With this, we contribute three main findings. (i) We show that pre-training is effective even with minimal synthetic images, with performance on par with large-scale pre-training datasets like ImageNet-1k for full fine-tuning. (ii) We investigate the single parameter with which we construct artificial categories for our dataset. We find that while the shape differences can be indistinguishable to humans, they are crucial for obtaining strong performances. (iii) Finally, we investigate the minimal requirements for successful pre-training. Surprisingly, we find that a substantial reduction of synthetic images from 1k to 1 can even lead to an increase in pre-training performance, a motivation to further investigate ''scaling backwards''. Finally, we extend our method from synthetic images to real images to see if a single real image can show similar pre-training effect through shape augmentation. We find that the use of grayscale images and affine transformations allows even real images to ''scale backwards.","tags":["Self-supervised learning"],"title":" Scaling Backwards: Minimal Synthetic Pretraining?","type":"publication"},{"authors":["Luc P.J. Sträter","Mohammadreza Salehi","Efstratios Gavves","Cees G. M. Snoek","Yuki M. Asano"],"categories":[],"content":"","date":1725926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725926400,"objectID":"06efdc8a9ce914a2f2c1d197049b9d13","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mohammadreza-eccv-2024-2/","publishdate":"2024-09-10T00:00:00Z","relpermalink":"/vislab/publication/mohammadreza-eccv-2024-2/","section":"publication","summary":"In the domain of anomaly detection, methods often excel in either high-level semantic or low-level industrial benchmarks, rarely achieving cross-domain proficiency. Semantic anomalies are novelties that differ in meaning from the training set, like unseen objects in self-driving cars. In contrast, industrial anomalies are subtle defects that preserve semantic meaning, such as cracks in airplane components. In this paper, we present GeneralAD, an anomaly detection framework designed to operate in semantic, near-distribution, and industrial settings with minimal per-task adjustments. In our approach, we capitalize on the inherent design of Vision Transformers, which are trained on image patches, thereby ensuring that the last hidden states retain a patch-based structure. We propose a novel self-supervised anomaly generation module that employs straightforward operations like noise addition and shuffling to patch features to construct pseudo-abnormal samples. These features are fed to an attention-based discriminator, which is trained to score every patch in the image. With this, our method can both accurately identify anomalies at the image level and also generate interpretable anomaly maps. We extensively evaluated our approach on ten datasets, achieving state-of-the-art results in six and on-par performance in the remaining for both localization and detection tasks.","tags":["Recognition (object detection, categorization)"],"title":"GeneralAD: Anomaly Detection Across Domains by Attending to Distorted Features","type":"publication"},{"authors":["Kumara Kahatapitiya","Adil Karjauv","Davide Abati","Fatih Porikli","Yuki M. Asano","Amirhossein Habibian"],"categories":[],"content":"","date":1725926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725926400,"objectID":"6a75c6d9425ece5eab4a5efb54602216","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yuki-eccv-2024-2/","publishdate":"2024-09-10T00:00:00Z","relpermalink":"/vislab/publication/yuki-eccv-2024-2/","section":"publication","summary":"Diffusion-based video editing have reached impressive quality and can transform either the global style, local structure, and attributes of given video inputs, following textual edit prompts. However, such solutions typically incur heavy memory and computational costs to generate temporally-coherent frames, either in the form of diffusion inversion and/or cross-frame attention. In this paper, we conduct an analysis of such inefficiencies, and suggest simple yet effective modifications that allow significant speed-ups whilst maintaining quality. Moreover, we introduce Object-Centric Diffusion, to fix generation artifacts and further reduce latency by allocating more computations towards foreground edited regions, arguably more important for perceptual quality. We achieve this by two novel proposals: i) Object-Centric Sampling, decoupling the diffusion steps spent on salient or background regions and spending most on the former, and ii) Object-Centric Token Merging, which reduces cost of cross-frame attention by fusing redundant tokens in unimportant background regions. Both techniques are readily applicable to a given video editing model without retraining, and can drastically reduce its memory and computational cost. We evaluate our proposals on inversion-based and control-signal-based editing pipelines, and show a latency reduction up to 10× for a comparable synthesis quality.","tags":["Self-supervised learning"],"title":"Object-Centric Diffusion for Efficient Video Editing","type":"publication"},{"authors":["Sarah Rastegar","Mohammadreza Salehi","Yuki M Asano","Hazel Doughty","Cees G M Snoek"],"categories":[],"content":"","date":1725926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725926400,"objectID":"d79d00223eddad863118189773791282","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/sarah-eccv-2024/","publishdate":"2024-09-10T00:00:00Z","relpermalink":"/vislab/publication/sarah-eccv-2024/","section":"publication","summary":"In this paper, we address Generalized Category Discovery, aiming to simultaneously uncover novel categories and accurately classify known ones. Traditional methods, which lean heavily on self-supervision and contrastive learning, often fall short when distinguishing between fine-grained categories. To address this, we introduce a novel concept called self-expertise, which enhances the model's ability to recognize subtle differences and uncover unknown categories. Our approach combines unsupervised and supervised self-expertise strategies to refine the model's discernment and generalization. Initially, hierarchical pseudo-labeling is used to provide soft supervision, improving the effectiveness of self-expertise. Our supervised technique differs from traditional methods by utilizing more abstract positive and negative samples, aiding in the formation of clusters that can generalize to novel categories. Meanwhile, our unsupervised strategy encourages the model to sharpen its category distinctions by considering within-category examples as `hard' negatives. Supported by theoretical insights, our empirical results showcase that our method outperforms existing state-of-the-art techniques in Generalized Category Discovery across several fine-grained datasets. Our code is available at: https://github.com/SarahRastegar/SelEx.","tags":["Self-supervised learning"],"title":"SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery","type":"publication"},{"authors":["Mohammadreza Salehi","Michael Dorkenwald","Fida Mohammad Thoker","Efstratios Gavves","Cees G. M. Snoek","Yuki M. Asano"],"categories":[],"content":"","date":1725926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725926400,"objectID":"8dedcef29906a56f0a156a10626cd02d","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mohammadreza-eccv-2024/","publishdate":"2024-09-10T00:00:00Z","relpermalink":"/vislab/publication/mohammadreza-eccv-2024/","section":"publication","summary":"Video-based pretraining offers immense potential for learning strong visual representations on an unprecedented scale. Recently, masked video modeling methods have shown promising scalability, yet fall short in capturing higher-level semantics due to reconstructing predefined low-level targets such as pixels. To tackle this, we present Sinkhorn-guided Masked Video Modelling (SIGMA), a novel video pretraining method that jointly learns the video model in addition to a target feature space using a projection network. However, this simple modification means that the regular L2 reconstruction loss will lead to trivial solutions as both networks are jointly optimized. As a solution, we distribute features of space-time tubes evenly across a limited number of learnable clusters. By posing this as an optimal transport problem, we enforce high entropy in the generated features across the batch, infusing semantic and temporal meaning into the feature space. The resulting cluster assignments are used as targets for a symmetric prediction task where the video model predicts cluster assignment of the projection network and vice versa. Experimental results on ten datasets across three benchmarks validate the effectiveness of SIGMA in learning more performant, temporally-aware, and robust video representations improving upon state-of-the-art methods.","tags":["Self-supervised learning"],"title":"SIGMA: Sinkhorn-Guided Masked Video Modeling","type":"publication"},{"authors":["Sameer Ambekar","Zehao Xiao","Jiayi Shen","Xiantong Zhen","Cees G. M. Snoek"],"categories":[],"content":"","date":1722211200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1722211200,"objectID":"29ba006e2984216c88f71359b8865e2e","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/zehao-collas-2024/","publishdate":"2024-07-29T00:00:00Z","relpermalink":"/vislab/publication/zehao-collas-2024/","section":"publication","summary":"This paper strives for domain generalization, where models are trained exclusively on source domains before being deployed on unseen target domains. We follow the strict separation of source training and target testing, but exploit the value of the unlabeled target data itself during inference. We make three contributions. First, we propose probabilistic pseudo-labeling of target samples to generalize the source-trained model to the target domain at test time. We formulate the generalization at test time as a variational inference problem, by modeling pseudo labels as distributions, to consider the uncertainty during generalization and alleviate the misleading signal of inaccurate pseudo labels. Second, we learn variational neighbor labels that incorporate the information of neighboring target samples to generate more robust pseudo labels. Third, to learn the ability to incorporate more representative target information and generate more precise and robust variational neighbor labels, we introduce a meta-generalization stage during training to simulate the generalization procedure. Experiments on seven widely-used datasets demonstrate the benefits, abilities, and effectiveness of our proposal.","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"Probabilistic Test-Time Generalization by Variational Neighbor-Labeling","type":"publication"},{"authors":["E. Egorov","R. Valperga","E. Gavves"],"categories":null,"content":"","date":1721520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721520000,"objectID":"7e7264e7b7d979dbd0740b1204addcff","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/riccardo-icml-2024/","publishdate":"2024-07-21T00:00:00Z","relpermalink":"/vislab/publication/riccardo-icml-2024/","section":"publication","summary":"Markov chain Monte Carlo methods have become popular in statistics as versatile techniques to sample from complicated probability distributions. In this work, we propose a method to parameterize and train transition kernels of Markov chains to achieve efficient sampling and good mixing. This training procedure minimizes the total variation distance between the stationary distribution of the chain and the empirical distribution of the data. Our approach leverages involutive Metropolis-Hastings kernels constructed from reversible neural networks that ensure detailed balance by construction. We find that reversibility also implies C2-equivariance of the discriminator function which can be used to restrict its function space.","tags":["Sampling"],"title":"Ai-Sampler: Adversarial Learning of Markov Kernels with Involjtive Maps","type":"publication"},{"authors":["Yongtuo Liu","Sara Magliacane","Miltiadis Kofinas","Efstratios Gavves"],"categories":null,"content":"","date":1721520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721520000,"objectID":"7eb29dbb5d7061c2a306cdf3fb4bbf6b","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yongtuo-icml-2024/","publishdate":"2024-07-21T00:00:00Z","relpermalink":"/vislab/publication/yongtuo-icml-2024/","section":"publication","summary":"Hybrid dynamical systems are prevalent in science and engineering to express complex systems with continuous and discrete states. To learn the laws of systems, all previous methods for equation discovery in hybrid systems follow a two-stage paradigm, i.e. they first group time series into small cluster fragments and then discover equations in each fragment separately through methods in non-hybrid systems. Although effective, these methods do not fully take advantage of the commonalities in the shared dynamics of multiple fragments that are driven by the same equations. Besides, the two-stage paradigm breaks the interdependence between categorizing and representing dynamics that jointly form hybrid systems. In this paper, we reformulate the problem and propose an end-to-end learning framework, i.e. Amortized Equation Discovery (AMORE), to jointly categorize modes and discover equations characterizing the dynamics of each mode by all segments of the mode. Experiments on four hybrid and six non-hybrid systems show that our method outperforms previous methods on equation discovery, segmentation, and forecasting.","tags":["Self-supervised learning"],"title":"Amortized Equation Discovery in Hybrid Dynamical Systems","type":"publication"},{"authors":["Emiel C.A. Roefs","Wouter Schellekens","Mario G. Báez-Yáñez","Alex A. Bhogal","Iris I.A. Groen","Matthias J.P. van Osch","Jeroen C.W. Siero","Natalia Petridou"],"categories":null,"content":"","date":1719532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719532800,"objectID":"188119e3ff82c77b5cb9735c510f42e3","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/iris-neuroscience-2024/","publishdate":"2024-06-28T00:00:00Z","relpermalink":"/vislab/publication/iris-neuroscience-2024/","section":"publication","summary":"Assessment of neuronal activity using blood oxygenation level-dependent (BOLD) is confounded by how the cerebrovascular architecture modulates hemodynamic responses. To understand brain function at the laminar level, it is crucial to distinguish neuronal signal contributions from those determined by the cortical vascular organization. Therefore, our aim was to investigate the purely vascular contribution in the BOLD signal by using vasoactive stimuli and compare that with neuronal-induced BOLD responses from a visual task. To do so, we estimated the hemodynamic response function (HRF) across cortical depth following brief visual stimulations under different conditions using ultrahigh-field (7 Tesla) functional (f)MRI. We acquired gradient-echo (GE)-echo-planar-imaging (EPI) BOLD, containing contributions from all vessel sizes, and spin-echo (SE)-EPI BOLD for which signal changes predominately originate from microvessels, to distinguish signal weighting from different vascular compartments. Non-neuronal hemodynamic changes were induced by hypercapnia and hyperoxia to estimate cerebrovascular reactivity and venous cerebral blood volume (WJ). Results show that increases in GE HRF amplitude from deeper to superficial layers coincided with increased macrovascular WJ. -normalized GE-HRF amplitudes yielded similar cortical depth profiles as SE, thereby possibly improving specificity to neuronal activation. For GE BOLD, faster onset time and shorter time-to-peak were observed toward the deeper layers. Hypercapnia reduced the amplitude of visual stimulus-induced signal responses as denoted by lower GE-HRF amplitudes and longer time-to-peak. In contrast, the SE-HRF amplitude was unaffected by hypercapnia, suggesting that these responses reflect predominantly neurovascular processes that are less contaminated by macrovascular signal contributions.","tags":["MRI, brain activity measurements"],"title":"The contribution of the vascular architecture and cerebrovascular reactivity to the BOLD signal formation across cortical depth","type":"publication"},{"authors":["Lars Doorenbos","Pablo Márquez Neila","Raphael Sznitman","Pascal Mettes"],"categories":[],"content":"","date":1717632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717632000,"objectID":"d1ff03a9d7637bcf5a379964131650de","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pascal-tmlr-2024/","publishdate":"2024-06-06T00:00:00Z","relpermalink":"/vislab/publication/pascal-tmlr-2024/","section":"publication","summary":"Hyperbolic space is becoming a popular choice for representing data due to the hierarchical structure - whether implicit or explicit - of many real-world datasets. Along with it comes a need for algorithms capable of solving fundamental tasks, such as classification, in hyperbolic space. Recently, multiple papers have investigated hyperbolic alternatives to hyperplane-based classifiers, such as logistic regression and SVMs. While effective, these approaches struggle with more complex hierarchical data. We, therefore, propose to generalize the well-known random forests to hyperbolic space. We do this by redefining the notion of a split using horospheres. Since finding the globally optimal split is computationally intractable, we find candidate horospheres through a large-margin classifier. To make hyperbolic random forests work on multi-class data and imbalanced experiments, we furthermore outline new methods for combining classes based on the lowest common ancestor and class-balanced large-margin losses. Experiments on standard and new benchmarks show that our approach outperforms both conventional random forest algorithms and recent hyperbolic classifiers.","tags":["Recognition (object detection, categorization)"],"title":"Hyperbolic Random Forests","type":"publication"},{"authors":["Sarah Ibrahimi","Mina Ghadimi Atigh","Nanne Van Noord","Pascal Mettes","Marcel Worring"],"categories":[],"content":"","date":1717632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717632000,"objectID":"276526f9ee57052fd42bbc80fcd5a7a8","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pascal-tmlr-2-2024/","publishdate":"2024-06-06T00:00:00Z","relpermalink":"/vislab/publication/pascal-tmlr-2-2024/","section":"publication","summary":"Vision-language models have in short time been established as powerful networks, demonstrating strong performance on a wide range of downstream tasks. A key factor behind their success is the learning of a joint embedding space where pairs of images and textual descriptions are contrastively aligned. Recent work has explored the geometry of the joint embedding space, finding that hyperbolic embeddings provide a compelling alternative to the commonly used Euclidean embeddings. Specifically, hyperbolic embeddings yield improved zero-shot generalization, better visual recognition, and more consistent semantic interpretations. In this paper, we conduct a deeper study into the hyperbolic embeddings and find that they open new doors for vision-language models. In particular, we find that hyperbolic vision-language models provide spatial awareness that Euclidean vision-language models lack, are better capable of dealing with ambiguity, and effectively discriminate between distributions. Our findings shed light on the greater potential of hyperbolic embeddings in large-scale settings, reaching beyond conventional downstream tasks. Our code is available at https://github.com/saibr/hypvl","tags":["Visual reasoning and logical representation"],"title":"Intriguing Properties of Hyperbolic Embeddings in Vision-Language Models","type":"publication"},{"authors":["Zehao Xiao","Jiayi Shen","Mohammad Mahdi Derakhshani","Shengcai Liao","Cees G. M. Snoek"],"categories":[],"content":"","date":1717200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717200000,"objectID":"4feebea13d247c8e7a53281a8c7ce41c","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/zehao-cvpr-2024/","publishdate":"2024-06-01T00:00:00Z","relpermalink":"/vislab/publication/zehao-cvpr-2024/","section":"publication","summary":"Image-language models with prompt learning have shown remarkable advances in numerous downstream vision tasks. Nevertheless, conventional prompt learning methods overfit their training distribution and lose the generalization ability on test distributions. To improve generalization across various distribution shifts, we propose any-shift prompting: a general probabilistic inference framework that considers the relationship between training and test distributions during prompt learning. We explicitly connect training and test distributions in the latent space by constructing training and test prompts in a hierarchical architecture. Within this framework, the test prompt exploits the distribution relationships to guide the generalization of the CLIP image-language model from training to any test distribution. To effectively encode the distribution information and their relationships, we further introduce a transformer inference network with a pseudo-shift training mechanism. The network generates the tailored test prompt with both training and test information in a feedforward pass, avoiding extra training costs at test time. Extensive experiments on twenty-three datasets demonstrate the effectiveness of any-shift prompting on the generalization over various distribution shifts.","tags":["Multi-modal learning"],"title":"Any-Shift Prompting for Generalization over Distributions","type":"publication"},{"authors":["Samuele Papa","Riccardo Valperga","David Knigge","Miltiadis Kofinas","Phillip Lippe","Jan-Jakob Sonke","Efstratios Gavves"],"categories":[],"content":"","date":1717200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717200000,"objectID":"c256864e17dabf05a10fdda6bfac17dd","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/samuele-cvpr-2024/","publishdate":"2024-06-01T00:00:00Z","relpermalink":"/vislab/publication/samuele-cvpr-2024/","section":"publication","summary":"Neural fields (NeFs) have recently emerged as a versatile method for modeling signals of various modalities, including images, shapes, and scenes. Subsequently, a number of works have explored the use of NeFs as representations for downstream tasks, e.g. classifying an image based on the parameters of a NeF that has been fit to it. However, the impact of the NeF hyperparameters on their quality as downstream representation is scarcely understood and remains largely unexplored. This is in part caused by the large amount of time required to fit datasets of neural fields. In this work, we propose a JAX-based library that leverages parallelization to enable fast optimization of large-scale NeF datasets, resulting in a significant speed-up. With this library, we perform a comprehensive study that investigates the effects of different hyperparameters on fitting NeFs for downstream tasks. In particular, we explore the use of a shared initialization, the effects of overtraining, and the expressiveness of the network architectures used. Our study provides valuable insights on how to train NeFs and offers guidance for optimizing their effectiveness in downstream applications. Finally, based on the proposed library and our analysis, we propose Neural Field Arena, a benchmark consisting of neural field variants of popular vision datasets, including MNIST, CIFAR, variants of ImageNet, and ShapeNetv2. Our library and the Neural Field Arena will be open-sourced to introduce standardized benchmarking and promote further research on neural fields.","tags":["3D computer vision"],"title":"How to Train Neural Field Representations: A Comprehensive Study and Benchmark","type":"publication"},{"authors":["Lukas Knobel","Tengda Han","Yuki M. Asano"],"categories":[],"content":"","date":1717200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717200000,"objectID":"e68af7fcd15e6da4adfcdd6c86854d37","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yuki-cvpr-2024/","publishdate":"2024-06-01T00:00:00Z","relpermalink":"/vislab/publication/yuki-cvpr-2024/","section":"publication","summary":"While recent supervised methods for reference-based object counting continue to improve the performance on benchmark datasets, they have to rely on small datasets due to the cost associated with manually annotating dozens of objects in images. We propose UnCounTR, a model that can learn this task without requiring any manual annotations. To this end, we construct 'Self-Collages', images with various pasted objects as training samples, that provide a rich learning signal covering arbitrary object types and counts. Our method builds on existing unsupervised representations and segmentation techniques to successfully demonstrate for the first time the ability of reference-based counting without manual supervision. Our experiments show that our method not only outperforms simple baselines and generic models such as FasterRCNN and DETR, but also matches the performance of supervised counting models in some domains.","tags":["Self-supervised learning"],"title":"Learning to Count without Annotations","type":"publication"},{"authors":["Yunhua Zhang","Hazel Doughty","Cees Snoek"],"categories":[],"content":"","date":1717200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717200000,"objectID":"e43b311523ca5d15cbbaaa7383c5c75d","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yunhua-cvpr-2024/","publishdate":"2024-06-01T00:00:00Z","relpermalink":"/vislab/publication/yunhua-cvpr-2024/","section":"publication","summary":"Low-resource settings are well-established in natural language processing, where many languages lack sufficient data for deep learning at scale. However, low-resource problems are under-explored in computer vision. In this paper, we address this gap and explore the challenges of low-resource image tasks with vision foundation models. We first collect a benchmark of genuinely low-resource image data, covering historic maps, circuit diagrams, and mechanical drawings.These low-resource settings all share three challenges: data scarcity, fine-grained differences, and the distribution shift from natural images to the specialized domain of interest.While existing foundation models have shown impressive generalizability, we find they cannot transfer well to our low-resource tasks. To begin to tackle the challenges of low-resource vision, we introduce one simple baseline per challenge. Specifically, we i) enlarge the data space by generative models, ii) adopt the best sub-kernels to encode local regions for fine-grained difference discovery and iii) learn attention for specialized domains. Experiments on our three low-resource tasks demonstrate our proposals already provide a better baseline than transfer learning, data augmentation, and fine-grained methods. This highlights the unique characteristics and challenges of low-resource vision for foundation models that warrant further investigation","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"Low-Resource Vision Challenges for Foundation Models","type":"publication"},{"authors":["Michael Dorkenwald","Nimrod Barazani","Cees G. M. Snoek","Yuki M Asano"],"categories":[],"content":"","date":1717200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717200000,"objectID":"85e7b147c5919d638aff68ca9f893753","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/michael-cvpr-2024/","publishdate":"2024-06-01T00:00:00Z","relpermalink":"/vislab/publication/michael-cvpr-2024/","section":"publication","summary":"Vision-Language Models (VLMs), such as Flamingo and GPT-4V, have shown immense potential by integrating large language models with vision systems. Nevertheless, these models face challenges in the fundamental computer vision task of object localisation, due to their training on multimodal data containing mostly captions without explicit spatial grounding. While it is possible to construct custom, supervised training pipelines with bounding box annotations that integrate with VLMs, these result in specialized and hard-to-scale models. In this paper, we aim to explore the limits of caption-based VLMs and instead propose to tackle the challenge in a simpler manner by i) keeping the weights of a caption-based VLM frozen and ii) not using any supervised detection data. To this end, we introduce an input-agnostic Positional Insert (PIN), a learnable spatial prompt, containing a minimal set of parameters that are slid inside the frozen VLM, unlocking object localisation capabilities. Our PIN module is trained with a simple next-token prediction task on synthetic data without requiring the introduction of new output heads. Our experiments demonstrate strong zero-shot localisation performances on a variety of images, including Pascal VOC, COCO, LVIS, and diverse images like paintings or cartoons.","tags":["Multi-modal learning"],"title":"PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs","type":"publication"},{"authors":["Sarah Rastegar","Hazel Doughty","Cees G.M. Snoek"],"categories":[],"content":"","date":1714521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714521600,"objectID":"c92dc4c0dfe8a9e36d632fae79469dc9","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/sarah-cviu-2024/","publishdate":"2024-05-01T00:00:00Z","relpermalink":"/vislab/publication/sarah-cviu-2024/","section":"publication","summary":"We aim to recognize actions under an appearance distribution shift between a source training domain and a target test domain. To enable such video domain generalization, our key idea is to intervene on the action to remove the confounding effect of the domain background on the class label using causal inference. Towards this, we propose to learn a causally debiased model on a source domain that intervenes on the action through three possible Do-operators, which separate the action and background. To better align the source and target distributions, we also introduce a test-time action intervention. Experiments on two challenging video domain generalization benchmarks reveal that causal inference is a promising tool for action recognition as it already achieves state-of-the-art results on Kinetics2Mimetics, the benchmark with the largest domain shift.","tags":["Action and behavior recognition"],"title":"Background no more: Action recognition across domains by causal interventions","type":"publication"},{"authors":["Miltiadis Kofinas","Boris Knyazev","Yan Zhang","Yunlu Chen","Gertjan J. Burghouts","Efstratios Gavves","Cees G. M. Snoek","David W. Zhang"],"categories":[],"content":"","date":1714521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714521600,"objectID":"91dffe2c1986381c117adc211e0dc1c1","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/miltiadis-iclr-2024/","publishdate":"2024-05-01T00:00:00Z","relpermalink":"/vislab/publication/miltiadis-iclr-2024/","section":"publication","summary":"Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to learn from neural graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalization performance, and learning to optimize, while consistently outperforming state-of-the-art methods. The source code is open-sourced at https://github.com/mkofinas/neural-graphs.","tags":["Graph neural networks","deep weight space"],"title":"Graph Neural Networks for Learning Equivariant Representations of Neural Networks","type":"publication"},{"authors":["Shashanka Venkataramanan","Mamshad Nayeem Rizve","João Carreira","Yuki M. Asano","Yannis Avrithis"],"categories":[],"content":"","date":1714521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714521600,"objectID":"07845de1e0bcbe1b21483ec1eca88024","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yuki-iclr-2024-3/","publishdate":"2024-05-01T00:00:00Z","relpermalink":"/vislab/publication/yuki-iclr-2024-3/","section":"publication","summary":"Self-supervised learning has unlocked the potential of scaling up pretraining to billions of images, since annotation is unnecessary. But are we making the best use of data? How more economical can we be? In this work, we attempt to answer this question by making two contributions. First, we investigate first-person videos and introduce a 'Walking Tours' dataset. These videos are high-resolution, hours-long, captured in a single uninterrupted take, depicting a large number of objects and actions with natural scene transitions. They are unlabeled and uncurated, thus realistic for self-supervision and comparable with human learning. Second, we introduce a novel self-supervised image pretraining method tailored for learning from continuous videos. Existing methods typically adapt image-based pretraining approaches to incorporate more frames. Instead, we advocate a 'tracking to learn to recognize' approach. Our method called DoRA, leads to attention maps that Discover and tRAck objects over time in an end-to-end manner, using transformer cross-attention. We derive multiple views from the tracks and use them in a classical self-supervised distillation loss. Using our novel approach, a single Walking Tours video remarkably becomes a strong competitor to ImageNet for several image and video downstream tasks.","tags":["Self-supervised learning"],"title":"Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video","type":"publication"},{"authors":["Yingjun Du","Haoliang Sun","Xiantong Zhen","Jun Xu","Yilong Yin","Ling Shao","Cees Snoek"],"categories":null,"content":"","date":1714521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714521600,"objectID":"a9c5f9fd005387874369ebacf296d3c8","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yingjun-pami-2024/","publishdate":"2024-05-01T00:00:00Z","relpermalink":"/vislab/publication/yingjun-pami-2024/","section":"publication","summary":"Few-shot learning deals with the fundamental and challenging problem of learning from a few annotated samples, while being able to generalize well on new tasks. The crux of few-shot learning is to extract prior knowledge from related tasks to enable fast adaptation to a new task with a limited amount of data. In this paper, we propose meta-learning kernels with random Fourier features for few-shot learning, we call MetaKernel. Speciﬁcally, we propose learning variational random features in a data-driven manner to obtain task-speciﬁc kernels by leveraging the shared knowledge provided by related tasks in a meta-learning setting. We treat the random feature basis as the latent variable, which is estimated by variational inference. The shared knowledge from related tasks is incorporated into a context inference of the posterior, which we achieve via a long-short term memory module. To establish more expressive kernels, we deploy conditional normalizing ﬂows based on coupling layers to achieve a richer posterior distribution over random Fourier bases. The resultant kernels are more informative and discriminative, which further improves the few-shot learning. To evaluate our method, we conduct extensive experiments on both few-shot image classiﬁcation and regression tasks. A thorough ablation study demonstrates that the effectiveness of each introduced component in our method. The benchmark results on fourteen datasets demonstrate MetaKernel consistently delivers at least comparable and often better performance than state-of-the-art alternatives.","tags":["Transfer","low-shot","semi- and un-supervised learning"],"title":"MetaKernel: Learning Variational Random Features with Limited Labels","type":"publication"},{"authors":["Duy-Kien Nguyen","Vaibhav Aggarwal","Yanghao Li","Martin R. Oswald","Alexander Kirillov","Cees G. M. Snoek","Xinlei Chen"],"categories":[],"content":"","date":1714521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714521600,"objectID":"ada117a8f93a700b0a240c02a7d0fd91","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/nguyen-iclr-2024/","publishdate":"2024-05-01T00:00:00Z","relpermalink":"/vislab/publication/nguyen-iclr-2024/","section":"publication","summary":"In this work, we explore regions as a potential visual analogue of words for self-supervised image representation learning. Inspired by Masked Autoencoding (MAE), a generative pre-training baseline, we propose masked region autoencoding to learn from groups of pixels or regions. Specifically, we design an architecture which efficiently addresses the one-to-many mapping between images and regions, while being highly effective especially with high-quality regions. When integrated with MAE, our approach (R-MAE) demonstrates consistent improvements across various pre-training datasets and downstream detection and segmentation benchmarks, with negligible computational overheads. Beyond the quantitative evaluation, our analysis indicates the models pre-trained with masked region autoencoding unlock the potential for interactive segmentation.","tags":["Self-supervised learning"],"title":"R-MAE: Regions Meet Masked Autoencoders","type":"publication"},{"authors":["Shashanka Venkataramanan","Amir Ghodrati","Yuki M Asano","Fatih Porikli","Amirhossein Habibian"],"categories":[],"content":"","date":1714521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714521600,"objectID":"e6c4fb08327e6153b78e21aed09e7a62","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yuki-iclr-2024/","publishdate":"2024-05-01T00:00:00Z","relpermalink":"/vislab/publication/yuki-iclr-2024/","section":"publication","summary":"This work aims to improve the efficiency of vision transformers (ViT). While ViTs use computationally expensive self-attention operations in every layer, we identify that these operations are highly correlated across layers – a key redundancy that causes unnecessary computations. Based on this observation, we propose SKIPAT, a method to reuse self-attention computation from preceding layers to approximate attention at one or more subsequent layers. To ensure that reusing self-attention blocks across layers does not degrade the performance, we introduce a simple parametric function, which outperforms the baseline transformer’s performance while running computationally faster. We show the effectiveness of our method in image classification and self-supervised learning on ImageNet-1K, semantic segmentation on ADE20K, image denoising on SIDD, and video denoising on DAVIS. We achieve improved throughput at the same-or-higher accuracy levels in all these tasks.","tags":["Neural network architecture"],"title":"Skip-Attention: Improving Vision Transformers by Paying Less Attention","type":"publication"},{"authors":["Dawid Kopiczko","Tijmen Blankevoort","Yuki M. Asano"],"categories":[],"content":"","date":1714521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714521600,"objectID":"5bad133678302643786113ae8dfd95c9","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yuki-iclr-2024-2/","publishdate":"2024-05-01T00:00:00Z","relpermalink":"/vislab/publication/yuki-iclr-2024-2/","section":"publication","summary":"Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA), which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models.","tags":["Large Language Models"],"title":"VeRA: Vector-based Random Matrix Adaptation","type":"publication"},{"authors":["Pascal Mettes","Mina Ghadimi Atigh","Martin Keller-Ressel","Jeffrey Gu","Serena Yeung"],"categories":[],"content":"","date":1711929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711929600,"objectID":"dc294096b8c3eef14db961cad24196ed","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pascal-ijcv-2024/","publishdate":"2024-04-01T00:00:00Z","relpermalink":"/vislab/publication/pascal-ijcv-2024/","section":"publication","summary":"Deep representation learning is a ubiquitous part of modern computer vision. While Euclidean space has been the de facto standard manifold for learning visual representations, hyperbolic space has recently gained rapid traction for learning in computer vision. Specifically, hyperbolic learning has shown a strong potential to embed hierarchical structures, learn from limited samples, quantify uncertainty, add robustness, limit error severity, and more. In this paper, we provide a categorization and in-depth overview of current literature on hyperbolic learning for computer vision. We research both supervised and unsupervised literature and identify three main research themes in each direction. We outline how hyperbolic learning is performed in all themes and discuss the main research problems that benefit from current advances in hyperbolic learning for computer vision. Moreover, we provide a high-level intuition behind hyperbolic geometry and outline open research questions to further advance research in this direction.","tags":["hyperbolic computer vision"],"title":"Hyperbolic Deep Learning in Computer Vision: A Survey","type":"publication"},{"authors":["Vincent Tao Hu","Di Wu","Yuki M Asano","Pascal Mettes","Basura Fernando","Björn Ommer","Cees G M Snoek"],"categories":null,"content":"","date":1710892800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710892800,"objectID":"6e943e9075da04cb57047554ed937e0a","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/tao-eacl-2024/","publishdate":"2024-03-20T00:00:00Z","relpermalink":"/vislab/publication/tao-eacl-2024/","section":"publication","summary":"Diffusion models are a promising tool for high-quality text generation. However, current models face multiple drawbacks including slow sampling, noise schedule sensitivity, and misalignment between the training and sampling stages. In this paper, we introduce FlowSeq, which bypasses all current drawbacks by leveraging flow matching for conditional text generation. FlowSeq can generate text in a few steps by training with a novel anchor loss, alleviating the need for expensive hyperparameter optimization of the noise schedule prevalent in diffusion models. We extensively evaluate our proposed method and show competitive performance in tasks such as question generation, open-domain dialogue, and paraphrasing.","tags":["generative model"],"title":"Flow Matching for Conditional Text Generation in a Few Sampling Steps","type":"publication"},{"authors":["Vincent Tao Hu","David W Zhang","Mang Tang","Pascal Mettes","Deli Zhao","Cees G M Snoek"],"categories":null,"content":"","date":1708387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708387200,"objectID":"ee634c7e0d99d22d065afe0274cf06af","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/tao-aaai-2024/","publishdate":"2024-02-20T00:00:00Z","relpermalink":"/vislab/publication/tao-aaai-2024/","section":"publication","summary":"This paper strives for image editing via generative models. Flow Matching is an emerging generative modeling technique that offers the advantage of simple and efficient training. Simultaneously, a new transformer-based U-ViT has recently been proposed to replace the commonly used UNet for better scalability and performance in generative modeling. Hence, Flow Matching with a transformer backbone offers the potential for scalable and high-quality generative modeling, but their latent structure and editing ability are as of yet unknown. Hence, we adopt this setting and explore how to edit images through latent space manipulation. We introduce an editing space, which we call u-space, that can be manipulated in a controllable, accumulative, and composable manner. Additionally, we propose a tailored sampling solution to enable sampling with the more efficient adaptive step-size ODE solvers. Lastly, we put forth a straightforward yet powerful method for achieving fine-grained and nuanced editing using text prompts. Our framework is simple and efficient, all while being highly effective at editing images while preserving the essence of the original content.","tags":["generative model"],"title":"Latent Space Editing in Transformer-Based Flow Matching","type":"publication"},{"authors":["Georgia A. Milne","Matteo Lisi","Aisha McLean","Rosie Zheng","Iris I.A. Groen","Tessa M. Dekker"],"categories":null,"content":"","date":1708041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708041600,"objectID":"00d7631683bc2394914b30a17c70f06b","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/iris-iscienve-2024/","publishdate":"2024-02-16T00:00:00Z","relpermalink":"/vislab/publication/iris-iscienve-2024/","section":"publication","summary":"Human vision relies heavily on prior knowledge. Here, we show for the first time that prior-knowledge-induced reshaping of visual inputs emerges gradually in late childhood. To isolate the effects of prior knowledge on perception, we presented 4- to 12-year-olds and adults with two-tone images – hard-to-recognize degraded photos. In adults, seeing the original photo triggers perceptual reorganization, causing mandatory recognition of the two-tone version. This involves top-down signaling from higher-order brain areas to early visual cortex. We show that children younger than 7–9 years do not experience this knowledge-guided shift, despite viewing the original photo immediately before each two-tone. To assess computations underlying this development, we compared human performance to three neural networks with varying architectures. The best-performing model behaved much like 4- to 5-year-olds, displaying feature-based rather than holistic processing strategies. The reconciliation of prior knowledge with sensory input undergoes a striking age-related shift, which may underpin the development of many perceptual abilities.","tags":["Human visual perception"],"title":"Perceptual reorganization from prior knowledge emerges late in childhood","type":"publication"},{"authors":["Zenglin Shi","Pascal Mettes","Cees G. M. Snoek"],"categories":[],"content":"","date":1706918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706918400,"objectID":"a5e76ff5710d7a7262145513f6c9cb03","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/zenglin-ijcv-2024/","publishdate":"2024-02-03T00:00:00Z","relpermalink":"/vislab/publication/zenglin-ijcv-2024/","section":"publication","summary":"This work considers supervised learning to count from images and their corresponding point annotations. Where density-based counting methods typically use the point annotations only to create Gaussian-density maps, which act as the supervision signal, the starting point of this work is that point annotations have counting potential beyond density map generation. We introduce two methods that repurpose the available point annotations to enhance counting performance. The first is a counting-specific augmentation that leverages point annotations to simulate occluded objects in both input and density images to enhance the network's robustness to occlusions. The second method, foreground distillation, generates foreground masks from the point annotations, from which we train an auxiliary network on images with blacked-out backgrounds. By doing so, it learns to extract foreground counting knowledge without interference from the background. These methods can be seamlessly integrated with existing counting advances and are adaptable to different loss functions. We demonstrate complementary effects of the approaches, allowing us to achieve robust counting results even in challenging scenarios such as background clutter, occlusion, and varying crowd densities. Our proposed approach achieves strong counting results on multiple datasets, including ShanghaiTech Part_A and Part_B, UCF_QNRF, JHU-Crowd++, and NWPU-Crowd.","tags":["Recognition (object detection, categorization)"],"title":"Focus for Free in Density-Based Counting","type":"publication"},{"authors":["Rob Romijnders","Yuki M Asano","Christos Louizos","Max Welling"],"categories":null,"content":"","date":1706745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706745600,"objectID":"ed2158db389f55941e834f000fcb7b3b","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/rob-aaai-2024/","publishdate":"2024-02-01T00:00:00Z","relpermalink":"/vislab/publication/rob-aaai-2024/","section":"publication","summary":"The pandemic in 2020 and 2021 had enormous economic and societal consequences, and studies show that contact tracing algorithms can be key in the early containment of the virus. While large strides have been made towards more effective contact tracing algorithms, we argue that privacy concerns currently hold deployment back. The essence of a contact tracing algorithm constitutes the communication of a risk score. Yet, it is precisely the communication and release of this score to a user that an adversary can leverage to gauge the private health status of an individual. We pinpoint a realistic attack scenario and propose a contact tracing algorithm with differential privacy guarantees against this attack. The algorithm is tested on the two most widely used agent-based COVID19 simulators and demonstrates superior performance in a wide range of settings. Especially for realistic test scenarios and while releasing each risk score with epsilon=1 differential privacy, we achieve a two to ten-fold reduction in the infection rate of the virus. To the best of our knowledge, this presents the first contact tracing algorithm with differential privacy guarantees when revealing risk scores for COVID19.","tags":["Distributed Learning"],"title":"Protect Your Score: Contact-tracing With Differential Privacy Guarantees","type":"publication"},{"authors":["Carlo Bretti","Pascal Mettes","Hendrik Vincent Koops","Daan Odijk","Nanne van Noord"],"categories":[],"content":"","date":1706400000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706400000,"objectID":"581aa797999765820450d6f4eb9519a6","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pascal-mm-2024/","publishdate":"2024-01-28T00:00:00Z","relpermalink":"/vislab/publication/pascal-mm-2024/","section":"publication","summary":"Creating a trailer requires carefully picking out and piecing together brief enticing moments out of a longer video, making it a challenging and time-consuming task. This requires selecting moments based on both visual and dialogue information. We introduce a multi-modal method for predicting the trailerness to assist editors in selecting trailer- worthy moments from long-form videos. We present results on a newly introduced soap opera dataset, demonstrating that predicting trailerness is a challenging task that benefits from multi-modal information. Code is available at https://github.com/carlobretti/cliffhanger.","tags":["Video analysis and understanding"],"title":"Find the Cliffhanger: Multi-modal Trailerness in Soap Operas","type":"publication"},{"authors":["PG Tréemuel","E Gavves","C Würsch","K Frick, R Vetsch"],"categories":null,"content":"","date":1702425600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702425600,"objectID":"d0435bd35927e7b44f5fff4ffe7e77b9","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/statios-icecs-2023/","publishdate":"2023-12-13T00:00:00Z","relpermalink":"/vislab/publication/statios-icecs-2023/","section":"publication","summary":"This paper presents a novel method for resolution free, free-form shape optimization for nonuniform transmission lines using artificial neural networks. We use a low dimensional representation of the geometries by learning a neural field embedding with a contrastive loss function to group similar geometries. A second neural network predicts the scattering parameters from the encoded geometry for a given frequency point, that are used to define the optimization objective in the frequency domain. The whole pipeline is fully-differentiable enabling the use of fast gradient based optimization methods. The proposed model architecture shows promising results for the simple test case of optimizing a transmission line taper. The method is fast, stable and flexible to apply to different geometries with different constraints and requirements.","tags":["AI for Electronics"],"title":"Parameter-free Neural Field-based Optimal Design of Nonuniform Transmission Lines","type":"publication"},{"authors":["Clemens Bartnik","Iris Groen"],"categories":null,"content":"","date":1700611200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700611200,"objectID":"02cccd0da914a399b73e6b4bbbf2a2b4","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/iris-neuroscience-2023/","publishdate":"2023-11-22T00:00:00Z","relpermalink":"/vislab/publication/iris-neuroscience-2023/","section":"publication","summary":"How humans perceive and understand real-world scenes is a long-standing question in neuroscience, cognitive psychology, and artificial intelligence. Initially, it was thought that scenes are constructed and represented by their component objects. An alternative view proposed that scene perception starts by extracting global features (e.g., spatial layout) first and individual objects in later stages. A third framework focuses on how the brain not only represents objects and layout but how this information combines to allow determining possibilities for (inter)action that the environment offers us. The discovery of scene-selective regions in the human visual system sparked interest in how scenes are represented in the brain. Experiments using functional magnetic resonance imaging show that multiple types of information are encoded in the scene-selective regions, while electroencephalography and magnetoencephalography measurements demonstrate links between the rapid extraction of different scene features and scene perception behavior. Computational models such as deep neural networks offer further insight by how training networks on different scene recognition tasks results in the computation of diagnostic features that can then be tested for their ability to predict activity in human brains when perceiving a scene. Collectively, these findings suggest that the brain flexibly and rapidly extracts a variety of information from scenes using a distributed network of brain regions.","tags":["Human visual perception"],"title":"Visual Perception in the Human Brain: How the Brain Perceives and Understands Real-World Scenes","type":"publication"},{"authors":["Thomas Mensink","Pascal Mettes"],"categories":null,"content":"","date":1700438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700438400,"objectID":"4576d20824fec146a9ea2a0b72dd0cb2","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pascal-bmvc-2023/","publishdate":"2023-11-20T00:00:00Z","relpermalink":"/vislab/publication/pascal-bmvc-2023/","section":"publication","summary":"Mixup is a widely adopted strategy for training deep networks, where additional samples are augmented by interpolating inputs and labels of training pairs. Mixup has shown to improve classification performance, network calibration, and out-of-distribution generalisation. While effective, a cornerstone of Mixup, namely that networks learn linear behaviour patterns between classes, is only indirectly enforced since the output interpolation is performed at the probability level. This paper seeks to address this limitation by mixing the classifiers directly instead of mixing the labels for each mixed pair. We propose to define the target of each augmented sample as a uniquely new classifier, whose parameters are a linear interpolation of the classifier vectors of the input pair. The space of all possible classifiers is continuous and spans all interpolations between classifier pairs. To make optimisation tractable, we propose a dual-contrastive Infinite Class Mixup loss, where we contrast the classifier of a mixed pair to both the classifiers and the predicted outputs of other mixed pairs in a batch. Infinite Class Mixup is generic in nature and applies to many variants of Mixup. Empirically, we show that it outperforms standard Mixup and variants such as RegMixup and Remix on balanced, long-tailed, and data-constrained benchmarks, highlighting its broad applicability. ","tags":["Recognition (object detection, categorization)"],"title":"Infinite Class Mixup","type":"publication"},{"authors":["Max van Spengler","Philipp Wirth","Pascal Mettes"],"categories":null,"content":"","date":1698796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698796800,"objectID":"7e6d7b027aea3d22731090b5f6837ee1","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/max-mm-2023/","publishdate":"2023-11-01T00:00:00Z","relpermalink":"/vislab/publication/max-mm-2023/","section":"publication","summary":"Deep learning in hyperbolic space is quickly gaining traction in the fields of machine learning, multimedia, and computer vision. Deep networks commonly operate in Euclidean space, implicitly assuming that data lies on regular grids. Recent advances have shown that hyperbolic geometry provides a viable alternative foundation for deep learning, especially when data is hierarchical in nature and when working with few embedding dimensions. Currently however, no accessible open-source library exists to build hyperbolic network modules akin to well-known deep learning libraries. We present HypLL, the Hyperbolic Learning Library to bring the progress on hyperbolic deep learning together. HypLL is built on top of PyTorch, with an emphasis in its design for ease-of-use, in order to attract a broad audience towards this new and open-ended research direction. The code is available at: https://github.com/maxvanspengler/hyperbolic_learning_library.","tags":["Hyperbolic learning"],"title":"HypLL: The Hyperbolic Learning Library","type":"publication"},{"authors":["Miltiadis Kofinas","Erik J Bekkers","Naveen Shankar Nagaraja","Efstratios Gavves"],"categories":[],"content":"","date":1698796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698796800,"objectID":"fbe92ec5a32fae0e36380ffc8791c2e6","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/miltiadis-neurips-2023/","publishdate":"2023-11-01T00:00:00Z","relpermalink":"/vislab/publication/miltiadis-neurips-2023/","section":"publication","summary":"Systems of interacting objects often evolve under the influence of underlying field effects that govern their dynamics, yet previous works have abstracted away from such effects, and assume that systems evolve in a vacuum. In this work, we focus on discovering these fields, and infer them from the observed dynamics alone, without directly observing them. We theorize the presence of latent force fields, and propose neural fields to learn them. Since the observed dynamics constitute the net effect of local object interactions and global field effects, recently popularized equivariant networks are inapplicable, as they fail to capture global information. To address this, we propose to disentangle local object interactions –which are SE(3) equivariant and depend on relative states– from external global field effects –which depend on absolute states. We model the interactions with equivariant graph networks, and combine them with neural fields in a novel graph network that integrates field forces. Our experiments show that we can accurately discover the underlying fields in charged particles settings, traffic scenes, and gravitational n-body problems, and effectively use them to learn the system and forecast future trajectories.","tags":["Interacting dynamical systems"],"title":"Latent Field Discovery in Interacting Dynamical Systems with Neural Fields","type":"publication"},{"authors":["Sarah Rastegar","Hazel Doughty","Cees G M Snoek"],"categories":[],"content":"","date":1698796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698796800,"objectID":"817feafa470ccb41c79c9ecabd7301d7","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/sarah-neurips-2023/","publishdate":"2023-11-01T00:00:00Z","relpermalink":"/vislab/publication/sarah-neurips-2023/","section":"publication","summary":"In the quest for unveiling novel categories at test time, we confront the inherent limitations of traditional supervised recognition models that are restricted by a predefined category set. While strides have been made in the realms of self-supervised and open-world learning towards test-time category discovery, a crucial yet often overlooked question persists: what exactly delineates a category? In this paper, we conceptualize a category through the lens of optimization, viewing it as an optimal solution to a well-defined problem. Harnessing this unique conceptualization, we propose a novel, efficient and self-supervised method capable of discovering previously unknown categories at test time. A salient feature of our approach is the assignment of minimum length category codes to individual data instances, which encapsulates the implicit category hierarchy prevalent in real-world datasets. This mechanism affords us enhanced control over category granularity, thereby equipping our model to handle fine-grained categories adeptly. Experimental evaluations, bolstered by state-of-the-art benchmark comparisons, testify to the efficacy of our solution in managing unknown categories at test time. Furthermore, we fortify our proposition with a theoretical foundation, providing proof of its optimality.","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery","type":"publication"},{"authors":["Yunhua Zhang","Hazel Doughty","Cees Snoek"],"categories":null,"content":"","date":1698796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698796800,"objectID":"127df8b827ca59f2bb126cd2a82e8e79","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yunhua-neurips-2023/","publishdate":"2023-11-01T00:00:00Z","relpermalink":"/vislab/publication/yunhua-neurips-2023/","section":"publication","summary":"Multimodal learning assumes all modality combinations of interest are available during training to learn cross-modal correspondences. In this paper, we challenge this modality-complete assumption for multimodal learning and instead strive for generalization to unseen modality combinations during inference. We pose the problem of unseen modality interaction and introduce a first solution. It exploits a module that projects the multidimensional features of different modalities into a common space with rich information preserved. This allows the information to be accumulated with a simple summation operation across available modalities. To reduce overfitting to less discriminative modality combinations during training, we further improve the model learning with pseudo-supervision indicating the reliability of a modality's prediction. We demonstrate that our approach is effective for diverse tasks and modalities by evaluating it for multimodal video classification, robot state regression, and multimedia retrieval.","tags":["Multi-modal learning"],"title":"Learning Unseen Modality Interaction","type":"publication"},{"authors":["Ilze Amanda Auzina","Cagatay Yildiz","Sara Magliacane","Matthias Bethge","Efstratios Gavves"],"categories":null,"content":"","date":1698796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698796800,"objectID":"c944d3561d4cc8e793cb018d79f89c63","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/ilze-neurips-2023/","publishdate":"2023-11-01T00:00:00Z","relpermalink":"/vislab/publication/ilze-neurips-2023/","section":"publication","summary":"Neural ordinary differential equations (NODEs) have been proven useful for learning non-linear dynamics of arbitrary trajectories. However, current NODE methods capture variations across trajectories only via the initial state value or by auto-regressive encoder updates. In this work, we introduce Modulated Neural ODEs (MoNODEs), a novel framework that sets apart dynamics states from underlying static factors of variation and improves the existing NODE methods. In particular, we introduce time-invariant modulator variables that are learned from the data. We incorporate our proposed framework into four existing NODE variants. We test MoNODE on oscillating systems, videos and human walking trajectories, where each trajectory has trajectory-specific modulation. Our framework consistently improves the existing model ability to generalize to new dynamic parameterizations and to perform far-horizon forecasting. In addition, we verify that the proposed modulator variables are informative of the true unknown factors of variation as measured by R^2 scores.","tags":["dynamical systems, NODE, disentanglement"],"title":"Modulated Neural ODEs","type":"publication"},{"authors":["Phillip Lippe","Bastiaan S. Veeling","Paris Perdikaris","Richard E. Turner","Johannes Brandstetter"],"categories":null,"content":"","date":1698796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698796800,"objectID":"378d9458bdd3be15f983f814e3fcc1bc","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/phillip-neurips-2023/","publishdate":"2023-11-01T00:00:00Z","relpermalink":"/vislab/publication/phillip-neurips-2023/","section":"publication","summary":"Time-dependent partial differential equations (PDEs) are ubiquitous in science and engineering. Recently, mostly due to the high computational cost of traditional solution techniques, deep neural network based surrogates have gained increased interest. The practical utility of such neural PDE solvers relies on their ability to provide accurate, stable predictions over long time horizons, which is a notoriously hard problem. In this work, we present a large-scale analysis of common temporal rollout strategies, identifying the neglect of non-dominant spatial frequency information, often associated with high frequencies in PDE solutions, as the primary pitfall limiting stable, accurate rollout performance. Based on these insights, we draw inspiration from recent advances in diffusion models to introduce PDE-Refiner; a novel model class that enables more accurate modeling of all frequency components via a multistep refinement process. We validate PDE-Refiner on challenging benchmarks of complex fluid dynamics, demonstrating stable and accurate rollouts that consistently outperform state-of-the-art models, including neural, numerical, and hybrid neural-numerical architectures. We further demonstrate that PDE-Refiner greatly enhances data efficiency, since the denoising objective implicitly induces a novel form of spectral data augmentation. Finally, PDE-Refiner's connection to diffusion models enables an accurate and efficient assessment of the model's predictive uncertainty, allowing us to estimate when the surrogate becomes inaccurate.","tags":["AI4Science"],"title":"PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers","type":"publication"},{"authors":["Yingjun Du","Zehao Xiao","Shengcai Liao","Cees Snoek"],"categories":null,"content":"","date":1698796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698796800,"objectID":"2d80a89aeabec2b697f150a5832d2c3f","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yingjun-neurips-2023/","publishdate":"2023-11-01T00:00:00Z","relpermalink":"/vislab/publication/yingjun-neurips-2023/","section":"publication","summary":"Prototype-based meta-learning has emerged as a powerful technique for addressing few-shot learning challenges. However, estimating a deterministic prototype using a simple average function from a limited number of examples remains a fragile process. To overcome this limitation, we introduce ProtoDiff, a novel framework that leverages a task-guided diffusion model during the meta-training phase to gradually generate prototypes, thereby providing efficient class representations. Specifically, a set of prototypes is optimized to achieve per-task prototype overfitting, enabling accurately obtaining the overfitted prototypes for individual tasks. Furthermore, we introduce a task-guided diffusion process within the prototype space, enabling the meta-learning of a generative process that transitions from a vanilla prototype to an overfitted prototype. ProtoDiff gradually generates taskspecific prototypes from random noise during the meta-test stage, conditioned on the limited samples available for the new task. Furthermore, to expedite training and enhance ProtoDiff’s performance, we propose the utilization of residual prototype learning, which leverages the sparsity of the residual prototype. We conduct thorough ablation studies to demonstrate its ability to accurately capture the underlying prototype distribution and enhance generalization. The new state-of-the-art performance on within-domain, cross-domain, and few-task few-shot classification further substantiates the benefit of ProtoDiff.","tags":["Transfer","low-shot","semi- and un-supervised learning"],"title":"ProtoDiff: Learning to Learn Prototypical Networks by Task-Guided Diffusion","type":"publication"},{"authors":["Sindy Löwe","Phillip Lippe","Francesco Locatello","Max Welling"],"categories":null,"content":"","date":1698796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698796800,"objectID":"f619e47da891d57bd2af25e976f06316","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/phillip-neurips-2023_2/","publishdate":"2023-11-01T00:00:00Z","relpermalink":"/vislab/publication/phillip-neurips-2023_2/","section":"publication","summary":"The binding problem in human cognition, concerning how the brain represents and connects objects within a fixed network of neural connections, remains a subject of intense debate. Most machine learning efforts addressing this issue in an unsupervised setting have focused on slot-based methods, which may be limiting due to their discrete nature and difficulty to express uncertainty. Recently, the Complex AutoEncoder was proposed as an alternative that learns continuous and distributed object-centric representations. However, it is only applicable to simple toy data. In this paper, we present Rotating Features, a generalization of complex-valued features to higher dimensions, and a new evaluation procedure for extracting objects from distributed representations. Additionally, we show the applicability of our approach to pre-trained features. Together, these advancements enable us to scale distributed object-centric representations from simple toy to real-world data. We believe this work advances a new paradigm for addressing the binding problem in machine learning and has the potential to inspire further innovation in the field.","tags":["Visual reasoning and logical representation"],"title":"Rotating Features for Object Discovery","type":"publication"},{"authors":["Mohammad Mahdi Derakhshani","Enrique Sanchez","Adrian Bulat","Victor Guilherme Turrisi da Costa","Cees G. M. Snoek","Georgios Tzimiropoulos","Brais Martinez"],"categories":null,"content":"","date":1696377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696377600,"objectID":"0baf118361dde219aa709cf6881436c8","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mohammad-iccv-2023/","publishdate":"2023-10-04T00:00:00Z","relpermalink":"/vislab/publication/mohammad-iccv-2023/","section":"publication","summary":"Foundational image-language models have generated considerable interest due to their efficient adaptation to downstream tasks by prompt learning. Prompt learning treats part of the language model input as trainable while freezing the rest, and optimizes an Empirical Risk Minimization objective. However, Empirical Risk Minimization is known to suffer from distributional shifts which hurt generalizability to prompts unseen during training. By leveraging the regularization ability of Bayesian methods, we frame prompt learning from the Bayesian perspective and formulate it as a variational inference problem. Our approach regularizes the prompt space, reduces overfitting to the seen prompts and improves the prompt generalization on unseen prompts. Our framework is implemented by modeling the input prompt space in a probabilistic manner, as an a priori distribution which makes our proposal compatible with prompt learning approaches that are unconditional or conditional on the image. We demonstrate empirically on 15 benchmarks that Bayesian prompt learning provides an appropriate coverage of the prompt space, prevents learning spurious features, and exploits transferable invariant features. This results in better generalization of unseen prompts, even across different datasets and domains.","tags":["Multi-modal learning"],"title":"Bayesian Prompt Learning for Image-Language Model Generalization","type":"publication"},{"authors":["Aritra Bhowmik","Yu Wang","Nora Baka","Martin R. Oswald","Cees G. M. Snoek"],"categories":null,"content":"","date":1696377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696377600,"objectID":"40342a9269af584c16c1854f086d6455","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/aritra-iccv-2023/","publishdate":"2023-10-04T00:00:00Z","relpermalink":"/vislab/publication/aritra-iccv-2023/","section":"publication","summary":"The goal of this paper is to detect objects by exploiting their interrelationships. Contrary to existing methods, which learn objects and relations separately, our key idea is to learn the object-relation distribution jointly. We first propose a novel way of creating a graphical representation of an image from inter-object relation priors and initial class predictions, we call a context-likelihood graph. We then learn the joint distribution with an energy-based modeling technique which allows to sample and refine the context-likelihood graph iteratively for a given image. Our formulation of jointly learning the distribution enables us to generate a more accurate graph representation of an image which leads to a better object detection performance. We demonstrate the benefits of our context-likelihood graph formulation and the energy-based graph refinement via experiments on the Visual Genome and MS-COCO datasets where we achieve a consistent improvement over object detectors like DETR and Faster-RCNN, as well as alternative methods modeling object interrelationships separately. Our method is detector agnostic, end-to-end trainable, and especially beneficial for rare object classes.","tags":["Recognition (object detection, categorization)"],"title":"Detecting Objects with Context-Likelihood Graphs and Graph Refinement","type":"publication"},{"authors":["Mengmeng Jing","Xiantong Zhen","Jingjing Li","Cees Snoek"],"categories":[],"content":"","date":1696377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696377600,"objectID":"705d1bdb69ca71bba11d5cd31ccf1906","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/jing-iccv-2023/","publishdate":"2023-10-04T00:00:00Z","relpermalink":"/vislab/publication/jing-iccv-2023/","section":"publication","summary":"Deep learning models fail on cross-domain challenges if the model is oversensitive to domain-specific attributes, e.g., lightning, background, camera angle, etc. To alleviate this problem, data augmentation coupled with consistency regularization are commonly adopted to make the model less sensitive to domain-specific attributes. Consistency regularization enforces the model to output the same representation or prediction for two views of one image. These constraints, however, are either too strict or not order-preserving for the classification probabilities. In this work, we propose the Order-preserving Consistency Regularization (OCR) for cross-domain tasks. The order-preserving property for the prediction makes the model robust to task-irrelevant transformations. As a result, the model becomes less sensitive to the domain-specific attributes. The comprehensive experiments show that our method achieves clear advantages on five different cross-domain tasks.","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"Order-preserving Consistency Regularization for Domain Adaptation and Generalization","type":"publication"},{"authors":["Max van Spengler","Erwin Berkhout","Pascal Mettes"],"categories":null,"content":"","date":1696377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696377600,"objectID":"fccfdb5e96b321f43c3adedc1a436f80","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/max-iccv-2023/","publishdate":"2023-10-04T00:00:00Z","relpermalink":"/vislab/publication/max-iccv-2023/","section":"publication","summary":"This paper introduces an end-to-end residual network that operates entirely on the Poincaré ball model of hyperbolic space. Hyperbolic learning has recently shown great potential for visual understanding, but is currently only performed in the penultimate layer(s) of deep networks. All visual representations are still learned through standard Euclidean networks. In this paper we investigate how to learn hyperbolic representations of visual data directly from the pixel-level. We propose Poincaré ResNet, a hyperbolic counterpart of the celebrated residual network, starting from Poincaré 2D convolutions up to Poincaré residual connections. We identify three roadblocks for training convolutional networks entirely in hyperbolic space and propose a solution for each: (i) Current hyperbolic network initializations collapse to the origin, limiting their applicability in deeper networks. We provide an identity-based initialization that preserves norms over many layers. (ii) Residual networks rely heavily on batch normalization, which comes with expensive Fréchet mean calculations in hyperbolic space. We introduce Poincaré midpoint batch normalization as a faster and equally effective alternative. (iii) Due to the many intermediate operations in Poincaré layers, the computation graphs of deep learning libraries blow up, limiting our ability to train on deep hyperbolic networks. We provide manual backward derivations of core hyperbolic operations to maintain manageable computation graphs.","tags":["Visual reasoning and logical representation"],"title":"Poincaré ResNet","type":"publication"},{"authors":["Pengwan Yang","Cees G. M. Snoek","Yuki M. Asano"],"categories":null,"content":"","date":1696377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696377600,"objectID":"b0bd2a7fb6f42d8b8feec63c2b4ea6ce","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pengwan-iccv-2023/","publishdate":"2023-10-04T00:00:00Z","relpermalink":"/vislab/publication/pengwan-iccv-2023/","section":"publication","summary":"In this paper we address the task of finding representative subsets of points in a 3D point cloud by means of a point-wise ordering. Only a few works have tried to address this challenging vision problem, all with the help of hard to obtain point and cloud labels. Different from these works, we introduce the task of point-wise ordering in 3D point clouds through self-supervision, which we call self-ordering. We further contribute the first end-to-end trainable network that learns a point-wise ordering in a self-supervised fashion. It utilizes a novel differentiable point scoring-sorting strategy and it constructs an hierarchical contrastive scheme to obtain self-supervision signals. We extensively ablate the method and show its superior performance even compared to supervised ordering methods on multiple datasets and tasks including zero-shot ordering of point clouds from unseen categories.","tags":["Self-supervised learning"],"title":"Self-Ordering Point Clouds","type":"publication"},{"authors":["Mohammadreza Salehi","Efstratios Gavves","Cees G. M. Snoek","Yuki M. Asano"],"categories":[],"content":"","date":1696377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696377600,"objectID":"3a588732a1d9249e0112e12bea149200","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mohammadreza-iccv-2023/","publishdate":"2023-10-04T00:00:00Z","relpermalink":"/vislab/publication/mohammadreza-iccv-2023/","section":"publication","summary":"Spatially dense self-supervised learning is a rapidly growing problem domain with promising applications for unsupervised segmentation and pretraining for dense downstream tasks. Despite the abundance of temporal data in the form of videos, this information-rich source has been largely overlooked. Our paper aims to address this gap by proposing a novel approach that incorporates temporal consistency in dense self-supervised learning. While methods designed solely for images face difficulties in achieving even the same performance on videos, our method improves not only the representation quality for videos-but also images. Our approach, which we call time-tuning, starts from image-pretrained models and fine-tunes them with a novel self-supervised temporal-alignment clustering loss on unlabeled videos. This effectively facilitates the transfer of high-level information from videos to image representations. Time-tuning improves the state-of-the-art by 8-10% for unsupervised semantic segmentation on videos and matches it for images. We believe this method paves the way for further self-supervised scaling by leveraging the abundant availability of videos. ","tags":["Self-supervised learning"],"title":"Time Does Tell: Self-Supervised Time-Tuning of Dense Image Representations","type":"publication"},{"authors":["Haochen Wang","Cilin Yan","Shuai Wang","Xiaolong Jiang","Xu Tang","Yao Hu","Weidi Xie","Efstratios Gavves"],"categories":null,"content":"","date":1696377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696377600,"objectID":"06a134285a9d17e63938bc20a26b32f4","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/haochen-iccv-2023/","publishdate":"2023-10-04T00:00:00Z","relpermalink":"/vislab/publication/haochen-iccv-2023/","section":"publication","summary":"Video Instance Segmentation (VIS) aims at segmenting and categorizing objects in videos from a closed set of training categories, lacking the generalization ability to handle novel categories in real-world videos. To address this limitation, we make the following three contributions. First, we introduce the novel task of Open-Vocabulary Video Instance Segmentation, which aims to simultaneously segment, track, and classify objects in videos from open-set categories, including novel categories unseen during training. Second, to benchmark Open-Vocabulary VIS, we collect a Large-Vocabulary Video Instance Segmentation dataset (LV-VIS), that contains well-annotated objects from 1,196 diverse categories, significantly surpassing the category size of existing datasets by more than one order of magnitude. Third, we propose an efficient Memory-Induced Transformer architecture, OV2Seg, to first achieve Open-Vocabulary VIS in an end-to-end manner with near real-time inference speed. Extensive experiments on LV-VIS and four existing VIS datasets demonstrate the strong zero-shot generalization ability of OV2Seg on novel categories. The dataset and code are released here https://github.com/haochenheheda/LVVIS.","tags":["Video analysis and understanding"],"title":"Towards Open-Vocabulary Video Instance Segmentation","type":"publication"},{"authors":["Fida Mohammad Thoker","Hazel Doughty","Cees Snoek"],"categories":null,"content":"","date":1696204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696204800,"objectID":"e1eed3c87d057ff644dad9a5ba4a2a3c","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/fida-iccv-2023/","publishdate":"2023-10-02T00:00:00Z","relpermalink":"/vislab/publication/fida-iccv-2023/","section":"publication","summary":"We propose a self-supervised method for learning motion-focused video representations. Existing approaches minimize distances between temporally augmented videos, which maintain high spatial similarity. We instead propose to learn similarities between videos with identical local motion dynamics but an otherwise different appearance. We do so by adding synthetic motion trajectories to videos which we refer to as tubelets. By simulating different tubelet motions and applying transformations, such as scaling and rotation, we introduce motion patterns beyond what is present in the pretraining data. This allows us to learn a video representation that is remarkably data efficient: our approach maintains performance when using only 25% of the pretraining videos. Experiments on 10 diverse downstream settings demonstrate our competitive performance and generalizability to new domains and fine-grained actions.","tags":["Self-supervised learning"],"title":"Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization","type":"publication"},{"authors":["Kenichi Yuasa","Iris Groen","Giovanni Piantoni","Stephanie Montenegro","Adeen Flinker","Sasha Devore","Orrin Devinsky","Werner Doyle, Patricia Dugan","Daniel Friedman","Nick Ramsey","Natalia Petridou","Jonathan Winawer"],"categories":null,"content":"","date":1693526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693526400,"objectID":"c1bcb640fac9d0e180f9530ded7c17df","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/iris-elife-2023/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/vislab/publication/iris-elife-2023/","section":"publication","summary":"Neuronal oscillations at about 10 Hz, called alpha oscillations, are often thought to arise from synchronous activity across occipital cortex, reflecting general cognitive states such as arousal and alertness. However, there is also evidence that modulation of alpha oscillations in visual cortex can be spatially specific. Here, we used intracranial electrodes in human patients to measure alpha oscillations in response to visual stimuli whose location varied systematically across the visual field. We separated the alpha oscillatory power from broadband power changes. The variation in alpha oscillatory power with stimulus position was then fit by a population receptive field (pRF) model. We find that the alpha pRFs have similar center locations to pRFs estimated from broadband power (70–180 Hz), but are several times larger. The results demonstrate that alpha suppression in human visual cortex can be precisely tuned. Finally, we show how the pattern of alpha responses can explain several features of exogenous visual attention. ","tags":["Human visual processing"],"title":"Precise Spatial Tuning of Visually Driven Alpha Oscillations in Human Visual Cortex","type":"publication"},{"authors":["Pascal Mettes"],"categories":[],"content":"","date":1690070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690070400,"objectID":"dcf2f7902e99328c6e80a35f61f59e8c","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pascal-ijcv-2023/","publishdate":"2023-07-23T00:00:00Z","relpermalink":"/vislab/publication/pascal-ijcv-2023/","section":"publication","summary":"This work addresses the problem of recognizing action categories in videos when no training examples are available. The current state-of-the-art enables such a zero-shot recognition by learning universal mappings from videos to a semantic space, either trained on large-scale seen actions or on objects. While effective, we find that universal action and object mappings are biased to specific regions in the semantic space. These biases lead to a fundamental problem: many unseen action categories are simply never inferred during testing. For example on UCF-101, a quarter of the unseen actions are out of reach with a state-of-the-art universal action model. To that end, this paper introduces universal prototype transport for zero-shot action recognition. The main idea is to re-position the semantic prototypes of unseen actions by matching them to the distribution of all test videos. For universal action models, we propose to match distributions through a hyperspherical optimal transport from unseen action prototypes to the set of all projected test videos. The resulting transport couplings in turn determine the target prototype for each unseen action. Rather than directly using the target prototype as final result, we re-position unseen action prototypes along the geodesic spanned by the original and target prototypes as a form of semantic regularization. For universal object models, we outline a variant that defines target prototypes based on an optimal transport between unseen action prototypes and object prototypes. Empirically, we show that universal prototype transport diminishes the biased selection of unseen action prototypes and boosts both universal action and object models for zero-shot classification and spatio-temporal localization.","tags":["Action and behavior recognition"],"title":"Universal Prototype Transport for Zero-Shot Action Recognition and Localization","type":"publication"},{"authors":["Shuo Chen","Yingjun Du","Pascal Mettes","Cees G. M. Snoek"],"categories":null,"content":"","date":1687564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687564800,"objectID":"9cc8378d638e15ef95363cdc98406557","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yingjun-icmr-2023/","publishdate":"2023-06-24T00:00:00Z","relpermalink":"/vislab/publication/yingjun-icmr-2023/","section":"publication","summary":"This paper investigates the problem of scene graph generation in videos with the aim of capturing semantic relations between subjects and objects in the form of ⟨subject, predicate, object⟩ triplets. Recognizing the predicate between subject and object pairs is imbalanced and multi-label in nature, ranging from ubiquitous interactions such as spatial relationships (e.g. in front of ) to rare interactions such as twisting. In widely-used benchmarks such as Action Genome and VidOR, the imbalance ratio between the most and least frequent predicates reaches 3,218 and 3,408, respectively, surpassing even benchmarks specifically designed for long-tailed recognition. Due to the long-tailed distributions and label co-occurrences, recent state-of-the-art methods predominantly focus on the most frequently occurring predicate classes, ignoring those in the long tail. In this paper, we analyze the limitations of current approaches for scene graph generation in videos and identify a one-to-one correspondence between predicate frequency and recall performance. To make the step towards unbiased scene graph generation in videos, we introduce a multi-label meta-learning framework to deal with the biased predicate distribution. Our meta-learning framework learns a meta-weight network for each training sample over all possible label losses. We evaluate our approach on the Action Genome and VidOR benchmarks by building upon two current state-of-theart methods for each benchmark. The experiments demonstrate that the multi-label meta-weight network improves the performance for predicates in the long tail without compromising performance for head classes, resulting in better overall performance and favorable generalizability.","tags":["Recognition (object detection, categorization)"],"title":"Multi-Label Meta Weighting for Long-Tailed Dynamic Scene Graph Generation","type":"publication"},{"authors":["Yingjun Du","Jiayi Shen","Xiantong Zhen","Cees G.M. Snoek"],"categories":null,"content":"","date":1687219200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687219200,"objectID":"047edc35aa33e5728b09d46b8d9fec5b","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yingjun-collas-2023/","publishdate":"2023-06-20T00:00:00Z","relpermalink":"/vislab/publication/yingjun-collas-2023/","section":"publication","summary":"Few-shot meta-learning presents a challenge for gradient descent optimization due to the limited number of training samples per task. To address this issue, we propose an episodic memory optimization for meta-learning, we call \u001bmph{EMO}, which is inspired by the human ability to recall past learning experiences from the brain's memory. EMO retains the gradient history of past experienced tasks in external memory, enabling few-shot learning in a memory-augmented way. By learning to retain and recall the learning process of past training tasks, EMO nudges parameter updates in the right direction, even when the gradients provided by a limited number of examples are uninformative. We prove theoretically that our algorithm converges for smooth, strongly convex objectives. EMO is generic, flexible, and model-agnostic, making it a simple plug-and-play optimizer that can be seamlessly embedded into existing optimization-based few-shot meta-learning approaches. Empirical results show that EMO scales well with most few-shot classification benchmarks and improves the performance of optimization-based meta-learning methods, resulting in accelerated convergence.","tags":["Transfer","low-shot","semi- and un-supervised learning"],"title":"EMO: Episodic Memory Optimization for Few-Shot Meta-Learning","type":"publication"},{"authors":["Yongtuo Liu","Sara Magliacane","Miltiadis Kofinas","Efstratios Gavves"],"categories":null,"content":"","date":1687132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687132800,"objectID":"686f8fdbe5e07f5e23df515969950eb6","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yongtuo-icml-2023/","publishdate":"2023-06-19T00:00:00Z","relpermalink":"/vislab/publication/yongtuo-icml-2023/","section":"publication","summary":"Dynamical systems with complex behaviours, e.g. immune system cells interacting with a pathogen, are commonly modelled by splitting the behaviour into different regimes, or modes, each with simpler dynamics, and then learning the switching behaviour from one mode to another. Switching Dynamical Systems (SDS) are a powerful tool that automatically discovers these modes and mode-switching behaviour from time series data. While effective, these methods focus on independent objects, where the modes of one object are independent of the modes of the other objects. In this paper, we focus on the more general interacting object setting for switching dynamical systems, where the per-object dynamics also depends on an unknown and dynamically changing subset of other objects and their modes. To this end, we propose a novel graph-based approach for switching dynamical systems, GRAph Switching dynamical Systems (GRASS), in which we use a dynamic graph to characterize interactions between objects and learn both intra-object and inter-object mode-switching behaviour. We introduce two new datasets for this setting, a synthesized ODE-driven particles dataset and a real-world Salsa Couple Dancing dataset. Experiments show that GRASS can consistently outperforms previous state-of-the-art methods.","tags":["Action and behavior recognition"],"title":"Graph Switching Dynamical Systems","type":"publication"},{"authors":["Wenfang Sun","Yingjun Du","Xiantong Zhen","Fan Wang","Ling Wang","Cees G.M. Snoek"],"categories":null,"content":"","date":1687132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687132800,"objectID":"47178d26fa2043a360860d174e1c9dc5","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yingjun-icml-2023/","publishdate":"2023-06-19T00:00:00Z","relpermalink":"/vislab/publication/yingjun-icml-2023/","section":"publication","summary":"Meta-learning algorithms are able to learn a new task using previously learned knowledge, but they often require a large number of meta-training tasks which may not be readily available. To address this issue, we propose a method for few-shot learning with fewer tasks, which we call MetaModulation. The key idea is to use a neural network to increase the density of the meta-training tasks by modulating batch normalization parameters during meta-training. Additionally, we modify parameters at various network levels, rather than just a single layer, to increase task diversity. To account for the uncertainty caused by the limited training tasks, we propose a variational MetaModulation where the modulation parameters are treated as latent variables. We also introduce learning variational feature hierarchies by the variational MetaModulation, which modulates features at all layers and can consider task uncertainty and generate more diverse tasks. The ablation studies illustrate the advantages of utilizing a learnable task modulation at different levels and demonstrate the benefit of incorporating probabilistic variants in few-task meta-learning. Our MetaModulation and its variational variants consistently outperform state-of-the-art alternatives on four few-task meta-learning benchmarks.","tags":["Transfer","low-shot","semi- and un-supervised learning"],"title":"MetaModulation: Learning Variational Feature Hierarchies for Few-Shot Learning with Fewer Tasks","type":"publication"},{"authors":["Yan Zhang*","David W. Zhang*","Simon Lacoste-Julien","Gertjan J. Burghouts","Cees G. M. Snoek"],"categories":null,"content":"","date":1687132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687132800,"objectID":"a8b69b424bff13fad3349350a75e4a60","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/david-icml-2023/","publishdate":"2023-06-19T00:00:00Z","relpermalink":"/vislab/publication/david-icml-2023/","section":"publication","summary":"Slot attention is a powerful method for object-centric modeling in images and videos. However, its set-equivariance limits its ability to handle videos with a dynamic number of objects because it cannot break ties. To overcome this limitation, we first establish a connection between slot attention and optimal transport. Based on this new perspective we propose MESH (Minimize Entropy of Sinkhorn): a cross-attention module that combines the tiebreaking properties of unregularized optimal transport with the speed of regularized optimal transport. We evaluate slot attention using MESH on multiple object-centric learning benchmarks and find significant improvements over slot attention in every setting.","tags":["object-centric learning"],"title":"Unlocking Slot Attention by Changing Optimal Transport Costs","type":"publication"},{"authors":["Rob Romijnders","Yuki M Asano","Christos Louizos","Max Welling"],"categories":null,"content":"","date":1682380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682380800,"objectID":"ec5f3ce8c9432ab41cd39f70a48dd2d6","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yuki-aaai-2023/","publishdate":"2023-04-25T00:00:00Z","relpermalink":"/vislab/publication/yuki-aaai-2023/","section":"publication","summary":"Pandemics have a major impact on society and the economy. In the case of a new virus, such as COVID-19, high-grade tests and vaccines might be slow to develop and scarce in the crucial initial phase. With no time to waste and lock-downs being expensive, contact tracing is thus an essential tool for policymakers. In theory, statistical inference on a virus transmission model can provide an effective method for tracing infections. However, in practice, such algorithms need to run decentralized, rendering existing methods – that require hundreds or even thousands of daily messages per person – infeasible. In this paper, we develop an algorithm that (i) requires only a few (2-5) daily messages, (ii) works with extremely low bandwidths (3-5 bits) and (iii) enables quarantining and targeted testing that drastically reduces the peak and length of the pandemic. We compare the effectiveness of our algorithm using two agent-based simulators of realistic contact patterns and pandemic parameters and show that it performs well even with low bandwidth, imprecise tests, and incomplete population coverage.","tags":["Federated Learning"],"title":"No time to waste: practical statistical contact tracing with few low-bit messages","type":"publication"},{"authors":["Vincent Tao Hu*","David W Zhang*,","Yuki M. Asano","Gertjan J. Burghouts","Cees G. M. Snoek",""],"categories":null,"content":"","date":1680739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680739200,"objectID":"2414ad616d12fbb56fa2bd7ff1d0f0dd","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/tao-cvpr-2023/","publishdate":"2023-04-06T00:00:00Z","relpermalink":"/vislab/publication/tao-cvpr-2023/","section":"publication","summary":"Diffusion models have demonstrated remarkable progress in image generation quality, especially when guidance is used to control the generative process. However, guidance requires a large amount of image-annotation pairs for training and is thus dependent on their availability, correctness and unbiasedness. In this paper, we eliminate the need for such annotation by instead leveraging the flexibility of self-supervision signals to design a framework for self-guided diffusion models. By leveraging a feature extraction function and a self-annotation function, our method provides guidance signals at various image granularities: from the level of holistic images to object boxes and even segmentation masks. Our experiments on single-label and multi-label image datasets demonstrate that self-labeled guidance always outperforms diffusion models without guidance and may even surpass guidance based on ground-truth labels, especially on unbalanced data. When equipped with self-supervised box or mask proposals, our method further generates visually diverse yet semantically consistent images, without the need for any class, box, or segment label annotation. Self-guided diffusion is simple, flexible and expected to profit from deployment at scale.","tags":["self-supervised learning","generative modelling"],"title":"Self-Guided Diffusion Models","type":"publication"},{"authors":["Yingjun Du","Jiayi Shen","Xiantong Zhen","Cees Snoek"],"categories":null,"content":"","date":1680739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680739200,"objectID":"b9dc74ddafc696f09e3fbf7c61357c29","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yingjun-cvpr-2023/","publishdate":"2023-04-06T00:00:00Z","relpermalink":"/vislab/publication/yingjun-cvpr-2023/","section":"publication","summary":"Modern image classiﬁers perform well on populated classes, while degrading considerably on tail classes with only a few instances. Humans, by contrast, effortlessly handle the long-tailed recognition challenge, since they can learn the tail representation based on different levels of semantic abstraction, making the learned tail features more discriminative. This phenomenon motivated us to propose SuperDisco, an algorithm that discovers super-class representations for long-tailed recognition using a graph model. We learn to construct the super-class graph to guide the representation learning to deal with long-tailed distributions. Through message passing on the super-class graph, image representations are rectiﬁed and reﬁned by attending to the most relevant entities based on the semantic similarity among their super-classes. Moreover, we propose to meta-learn the super-class graph under the supervision of a prototype graph constructed from a small amount of imbalanced data. By doing so, we obtain a more robust superclass graph that further improves the long-tailed recognition performance. The consistent state-of-the-art experiments on the long-tailed CIFAR-100, ImageNet, Places and iNaturalist demonstrate the beneﬁt of the discovered superclass graph for dealing with long-tailed distributions.","tags":["Recognition (object detection, categorization)"],"title":"SuperDisco: Super-Class Discovery Improves Visual Recognition for the Long-Tail","type":"publication"},{"authors":["Piyush Bagad","Makarand Tapaswi","Cees G.M. Snoek"],"categories":null,"content":"","date":1680739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680739200,"objectID":"5953eab50c09db4e7caf377cc3c22365","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/piyush-cvpr-2023/","publishdate":"2023-04-06T00:00:00Z","relpermalink":"/vislab/publication/piyush-cvpr-2023/","section":"publication","summary":"Modelling and understanding time remains a challenge in contemporary video understanding models. With language emerging as a key driver towards powerful generalization, it is imperative for foundational video-language models to have a sense of time. In this paper, we consider a specific aspect of temporal understanding: consistency of time order as elicited by before/after relations. We establish that seven existing video-language models struggle to understand even such simple temporal relations. We then question whether it is feasible to equip these foundational models with temporal awareness without re-training them from scratch. Towards this, we propose a temporal adaptation recipe on top of one such model, VideoCLIP, based on post-pretraining on a small amount of video-text data. We conduct a zero-shot evaluation of the adapted models on six datasets for three downstream tasks which require varying degrees of time awareness. We observe encouraging performance gains especially when the task needs higher time awareness. Our work serves as a first step towards probing and instilling a sense of time in existing video-language models without the need for data and compute-intense training from scratch.","tags":["Multi-modal learning"],"title":"Test of Time: Instilling Video-Language Models with a Sense of Time","type":"publication"},{"authors":["Phillip Lippe","Sara Magliacane","Sindy Löwe","Yuki M. Asano","Taco Cohen","Efstratios Gavves"],"categories":null,"content":"","date":1677110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677110400,"objectID":"6dd666dd91d7c64f4f54751334f7c257","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yuki-uai-2023/","publishdate":"2023-02-23T00:00:00Z","relpermalink":"/vislab/publication/yuki-uai-2023/","section":"publication","summary":"Identifying the causal variables of an environment and how to intervene on them is of core value in applications such as robotics and embodied AI. While an agent can commonly interact with the environment and may implicitly perturb the behavior of some of these causal variables, often the targets it affects remain unknown. In this paper, we show that causal variables can still be identified for many common setups, e.g., additive Gaussian noise models, if the agent's interactions with a causal variable can be described by an unknown binary variable. This happens when each causal variable has two different mechanisms, e.g., an observational and an interventional one. Using this identifiability result, we propose BISCUIT, a method for simultaneously learning causal variables and their corresponding binary interaction variables. On three robotic-inspired datasets, BISCUIT accurately identifies causal variables and can even be scaled to complex, realistic environments for embodied AI.","tags":["Visual reasoning and logical representation"],"title":"BISCUIT: Causal Representation Learning from Binary Interactions","type":"publication"},{"authors":["Wenzhe Yin","Jan Jakob Sonke","Efstratios Gavves"],"categories":null,"content":"","date":1675296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675296000,"objectID":"29d1bf0cffc4518ae324bdfbf14ad621","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/wenze-mia-2023/","publishdate":"2023-02-02T00:00:00Z","relpermalink":"/vislab/publication/wenze-mia-2023/","section":"publication","summary":"Deformable image registration plays an important role in medical image analysis. Deep neural networks such as VoxelMorph and TransMorph are fast, but limited to small deformations and face challenges in the presence of large deformations. To tackle large deformations in medical image registration, we propose PC-Reg, a pyramidal Prediction and Correction method for deformable registration, which treats multi-scale registration akin to solving an ordinary differential equation (ODE) across scales. Starting with a zero-initialized deformation at the coarse level, PC-Reg follows the predictor-corrector regime and progressively predicts a residual flow and a correction flow to update the deformation vector field through different scales. The prediction in each scale can be regarded as a single step of ODE integration. PC-Reg can be easily extended to diffeomorphic registration and is able to alleviate the multiscale accumulated upsampling and diffeomorphic integration error. Further, to transfer details from full resolution to low scale, we introduce a distillation loss, where the output is used as the target label for intermediate outputs. Experiments on inter-patient deformable registration show that the proposed method significantly improves registration not only for large but also for small deformations.","tags":["Medical Image Analysis"],"title":"PC-Reg: A pyramidal prediction–correction approach for large deformation image registration","type":"publication"},{"authors":["Lucas de Vries","Bart J Emmer","Charles BLM Majoie","Henk A Marquering","Efstratios Gavves"],"categories":null,"content":"","date":1675296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675296000,"objectID":"4c9bfa6de94909104459a18cfebdd1ce","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/statios-mia-2023/","publishdate":"2023-02-02T00:00:00Z","relpermalink":"/vislab/publication/statios-mia-2023/","section":"publication","summary":"CT perfusion imaging is commonly used for infarct core quantification in acute ischemic stroke patients. The outcomes and perfusion maps of CT perfusion software, however, show many discrepancies between vendors. We aim to perform infarct core segmentation directly from CT perfusion source data using machine learning, excluding the need to use the perfusion maps from standard CT perfusion software. To this end, we present a symmetry-aware spatio-temporal segmentation model that encodes the micro-perfusion dynamics in the brain, while decoding a static segmentation map for infarct core assessment. Our proposed spatio-temporal PerfU-Net employs an attention module on the skip-connections to match the dimensions of the encoder and decoder. We train and evaluate the method on 94 and 62 scans, respectively, using the Ischemic Stroke Lesion Segmentation (ISLES) 2018 challenge data. We achieve state-of-the-art results compared to methods that only use CT perfusion source imaging with a Dice score of 0.46. We are almost on par with methods that use perfusion maps from third party software, whilst it is known that there is a large variation in these perfusion maps from various vendors. Moreover, we achieve improved performance compared to simple perfusion map analysis, which is used in clinical practice.","tags":["Medical Image Analysis"],"title":"PerfU-Net: Baseline infarct estimation from CT perfusion source data for acute ischemic stroke","type":"publication"},{"authors":["Lucas de Vries","Rudolf LM van Herten","Jan W Hoving","Ivana Išgum","Bart J Emmer","Charles BLM Majoie","Henk A Marquering","Efstratios Gavves"],"categories":null,"content":"","date":1675296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675296000,"objectID":"b7843ea3f7d4b1d5d79fa53f0ecc6e44","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/statios-mia-2023-2/","publishdate":"2023-02-02T00:00:00Z","relpermalink":"/vislab/publication/statios-mia-2023-2/","section":"publication","summary":"CT perfusion imaging is important in the imaging workup of acute ischemic stroke for evaluating affected cerebral tissue. CT perfusion analysis software produces cerebral perfusion maps from commonly noisy spatio-temporal CT perfusion data. High levels of noise can influence the results of CT perfusion analysis, necessitating software tuning. This work proposes a novel approach for CT perfusion analysis that uses physics-informed learning, an optimization framework that is robust to noise. In particular, we propose SPPINN: Spatio-temporal Perfusion Physics-Informed Neural Network and research spatio-temporal physics-informed learning. SPPINN learns implicit neural representations of contrast attenuation in CT perfusion scans using the spatio-temporal coordinates of the data and employs these representations to estimate a continuous representation of the cerebral perfusion parameters. We validate the approach on simulated data to quantify perfusion parameter estimation performance. Furthermore, we apply the method to in-house patient data and the public Ischemic Stroke Lesion Segmentation 2018 benchmark data to assess the correspondence between the perfusion maps and reference standard infarct core segmentations. Our method achieves accurate perfusion parameter estimates even with high noise levels and differentiates healthy tissue from infarcted tissue. Moreover, SPPINN perfusion maps accurately correspond with reference standard infarct core segmentations. Hence, we show that using spatio-temporal physics-informed learning for cerebral perfusion estimation is accurate, even in noisy CT perfusion data. The code for this work is available at https://github.com/lucasdevries/SPPINN.","tags":["Medical Image Analysis"],"title":"Spatio-temporal physics-informed learning: A novel approach to CT perfusion analysis in acute ischemic stroke","type":"publication"},{"authors":["Phillip Lippe","Sara Magliacane","Sindy Löwe","Yuki M Asano","Taco Cohen","Efstratios Gavves"],"categories":null,"content":"","date":1675209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675209600,"objectID":"21619b87b561aa3e967eb1dde07080e7","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/philip-iclr-2023/","publishdate":"2023-02-01T00:00:00Z","relpermalink":"/vislab/publication/philip-iclr-2023/","section":"publication","summary":"Causal representation learning is the task of identifying the underlying causal variables and their relations from high-dimensional observations, such as images. Recent work has shown that one can reconstruct the causal variables from temporal sequences of observations under the assumption that there are no instantaneous causal relations between them. In practical applications, however, our measurement or frame rate might be slower than many of the causal effects. This effectively creates ``instantaneous' effects and invalidates previous identifiability results. To address this issue, we propose iCITRIS, a causal representation learning method that allows for instantaneous effects in intervened temporal sequences when intervention targets can be observed, e.g., as actions of an agent. iCITRIS identifies the potentially multidimensional causal variables from temporal observations, while simultaneously using a differentiable causal discovery method to learn their causal graph. In experiments on three datasets of interactive systems, iCITRIS accurately identifies the causal variables and their causal graph.","tags":["Causal Representation Learning"],"title":"Causal Representation Learning for Instantaneous and Temporal Effects in Interactive Systems","type":"publication"},{"authors":["Adeel Pervez","Phillip Lippe","Efstratios Gavves"],"categories":null,"content":"","date":1675209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675209600,"objectID":"8fdbd5c5257f385d8f7f0195165a4040","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pervez2-iclr-2023/","publishdate":"2023-02-01T00:00:00Z","relpermalink":"/vislab/publication/pervez2-iclr-2023/","section":"publication","summary":"We propose topology-aware feature partitioning into $k$ disjoint partitions for given scene features as a method for object-centric representation learning. To this end, we propose to use minimum $s$-$t$ graph cuts as a partitioning method which is represented as a linear program. The method is topologically aware since it explicitly encodes neighborhood relationships in the image graph. To solve the graph cuts our solution relies on an efficient, scalable, and differentiable quadratic programming approximation. Optimizations specific to cut problems allow us to solve the quadratic programs and compute their gradients significantly more efficiently compared with the general quadratic programming approach. Our results show that our approach is scalable and outperforms existing methods on object discovery tasks with textured scenes and objects.","tags":["Object-Centric Representation Learning"],"title":"Differentiable Mathematical Programming for Object-Centric Representation Learning","type":"publication"},{"authors":["Zehao Xiao","Xiantong Zhen","Shengcai Liao","Cees G. M. Snoek"],"categories":null,"content":"","date":1675209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675209600,"objectID":"1fe73bcbe3083c83b9a78632fe9b4da9","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/zehao-iclr-2023/","publishdate":"2023-02-01T00:00:00Z","relpermalink":"/vislab/publication/zehao-iclr-2023/","section":"publication","summary":"In this paper, we propose energy-based sample adaptation at test time for domain generalization. Where previous works adapt their models to target domains, we adapt the unseen target samples to source-trained models. To this end, we design a discriminative energy-based model, which is trained on source domains to jointly model the conditional distribution for classification and data distribution for sample adaptation. The model is optimized to simultaneously learn a classifier and an energy function. To adapt target samples to source distributions, we iteratively update the samples by energy minimization with stochastic gradient Langevin dynamics. Moreover, to preserve the categorical information in the sample during adaptation, we introduce a categorical latent variable into the energy-based model. The latent variable is learned from the original sample before adaptation by variational inference and fixed as a condition to guide the sample update. Experiments on six benchmarks for classification of images and microblog threads demonstrate the effectiveness of our proposal.","tags":["Domain generalization","test-time adaptation"],"title":"Energy-Based Test Sample Adaptation for Domain Generalization","type":"publication"},{"authors":["Hossein Mirzaei","Mohammadreza Salehi","Sajjad Shahabi","Efstratios Gavves","Cees G. M. Snoek","Mohammad Sabokrou","Mohammad Hossein Rohban"],"categories":null,"content":"","date":1675209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675209600,"objectID":"f879572eb183eec0023455d7325d22cb","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mirzaei-iclr-2023/","publishdate":"2023-02-01T00:00:00Z","relpermalink":"/vislab/publication/mirzaei-iclr-2023/","section":"publication","summary":"We aim for image-based novelty detection. Despite considerable progress, existing models either fail or face a dramatic drop under the so-called \"near-distribution\" setting, where the differences between normal and anomalous samples are subtle. We first demonstrate existing methods experience up to 20% decrease in performance in the near-distribution setting. Next, we propose to exploit a score-based generative model to produce synthetic near-distribution anomalous data. Our model is then fine-tuned to distinguish such data from the normal samples. We provide a quantitative as well as qualitative evaluation of this strategy, and compare the results with a variety of GAN-based models. Effectiveness of our method for both the near-distribution and standard novelty detection is assessed through extensive experiments on datasets in diverse applications such as medical images, object classification, and quality control. This reveals that our method considerably improves over existing models, and consistently decreases the gap between the near-distribution and standard novelty detection performance.","tags":["Recognition (object detection, categorization)"],"title":"Fake It Till You Make It: Towards Accurate Near-Distribution Novelty Detection","type":"publication"},{"authors":["David M Knigge","David W Romero","Albert Gu","Efstratios Gavves","Erik J Bekkers","Jakub Mikolaj Tomczak","Mark Hoogendoorn","Jan-jakob Sonke",""],"categories":null,"content":"","date":1675209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675209600,"objectID":"c9c88ddbee1130836bd292d1423ce26a","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/d.m.knigge-iclr-2023/","publishdate":"2023-02-01T00:00:00Z","relpermalink":"/vislab/publication/d.m.knigge-iclr-2023/","section":"publication","summary":"Performant Convolutional Neural Network (CNN) architectures must be tailored to specific tasks in order to consider the length, resolution, and dimensionality of the input data. In this work, we tackle the need for problem-specific CNN architectures. We present the Continuous Convolutional Neural Network (CCNN): a single CNN able to process data of arbitrary resolution, dimensionality and length without any structural changes. Its key component are its continuous convolutional kernels which model long-range dependencies at every layer, and thus remove the need of current CNN architectures for task-dependent downsampling and depths. We showcase the generality of our method by using the same architecture for tasks on sequential (1D), visual (2D) and point-cloud (3D) data. Our CCNN matches and often outperforms the current state-of-the-art across all tasks considered.","tags":["Continuous methods"],"title":"Modelling Long Range Dependencies in N-D: From Task-Specific to a General Purpose CNN","type":"publication"},{"authors":["David W. Zhang","Corrado Rainone","Markus Peschl","Roberto Bondesan"],"categories":null,"content":"","date":1675209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675209600,"objectID":"dc5a040cf8f19b1b46ba325077f34bef","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/david-iclr-2023/","publishdate":"2023-02-01T00:00:00Z","relpermalink":"/vislab/publication/david-iclr-2023/","section":"publication","summary":"Finding the best way to schedule operations in a computation graph is a classical NP-hard problem which is central to compiler optimization. However, evaluating the goodness of a schedule on the target hardware can be very time-consuming. Traditional approaches as well as previous machine learning ones typically optimize proxy metrics, which are fast to evaluate but can lead to bad schedules when tested on the target hardware. In this work, we propose a new approach to scheduling by sampling proportionally to the proxy metric using a novel GFlowNet method. We introduce a technique to control the trade-off between diversity and goodness of the proposed schedules at inference time and demonstrate empirically that the pure optimization baselines can lead to subpar performance with respect to our approach when tested on a target model. Furthermore, we show that conditioning the GFlowNet on the computation graph enables generalization to unseen scheduling problems for both synthetic and real-world compiler datasets.","tags":["machine learning for combinatorial optimization"],"title":"Robust Scheduling with GFlowNets","type":"publication"},{"authors":["Adeel Pervez","Phillip Lippe","Efstratios Gavves"],"categories":null,"content":"","date":1675209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675209600,"objectID":"9262294deafcb14292a574d2f2dd8646","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pervez-iclr-2023/","publishdate":"2023-02-01T00:00:00Z","relpermalink":"/vislab/publication/pervez-iclr-2023/","section":"publication","summary":"A number of problems in learning can be formulated in terms of the basic primitive of sampling $k$ elements out of a universe of $n$ elements. This subset sampling operation cannot directly be included in differentiable models, and approximations are essential. Current approaches take an order sampling approach to sampling subsets and depend on differentiable approximations of the Top-$k$ operator for selecting the largest $k$ elements from a set. We present a simple alternative method for sampling subsets based on conditional Poisson sampling. Unlike order sampling approaches, the complexity of the proposed method is independent of the subset size, which makes the method scalable to large subset sizes. We adapt the procedure to make it efficient and amenable to discrete gradient approximations for use in differentiable models. Furthermore, the method allows the subset size parameter $k$ to be differentiable. We validate our approach extensively, on image and text model explanation, image subsampling and stochastic $k$-nearest neighbor tasks outperforming existing methods in accuracy, efficiency and scalability.","tags":["Discrete Sampling"],"title":"Scalable Subset Sampling with Neural Conditional Poisson Networks","type":"publication"},{"authors":["Yuki M. Asano","Aaqib Saeed"],"categories":null,"content":"","date":1675209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675209600,"objectID":"5fcc065500b8911a15ba54adff031d12","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yuki-iclr-2023/","publishdate":"2023-02-01T00:00:00Z","relpermalink":"/vislab/publication/yuki-iclr-2023/","section":"publication","summary":"What can neural networks learn about the visual world when provided with only a single image as input? While any image obviously cannot contain the multitudes of all existing objects, scenes and lighting conditions - within the space of all 256^(3x224x224) possible 224-sized square images, it might still provide a strong prior for natural images. To analyze this 'augmented image prior' hypothesis, we develop a simple framework for training neural networks from scratch using a single image and augmentations using knowledge distillation from a supervised pretrained teacher. With this, we find the answer to the above question to be: 'surprisingly, a lot'. In quantitative terms, we find accuracies of 94%/74% on CIFAR-10/100, 69% on ImageNet, and by extending this method to video and audio, 51% on Kinetics-400 and 84% on SpeechCommands. In extensive analyses spanning 13 datasets, we disentangle the effect of augmentations, choice of data and network architectures and also provide qualitative evaluations that include lucid `panda neurons' in networks that have never even seen one.","tags":["Self-supervised learning"],"title":"The Augmented Image Prior: Distilling 1000 Classes by Extrapolating from Single Image","type":"publication"},{"authors":["P A Zegeling","M W F van Spengler"],"categories":null,"content":"","date":1673222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673222400,"objectID":"2bda7939c33f49fe090a931fa089e9a4","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/max-jcam-2023/","publishdate":"2023-01-09T00:00:00Z","relpermalink":"/vislab/publication/max-jcam-2023/","section":"publication","summary":"We present a class of Boundary Value Methods (BVMs) that can be applied to semi-stable and unstable Partial Differential Equation (PDE) models. Step-by-step (initial value) methods, such as Runge–Kutta and linear multistep methods have numerical stability regions which do not intersect with certain regions of the complex plane that are significant to the time-integration of unstable PDEs. BVMs, which need extra numerical conditions at the final time, are global methods and are, in some sense, free of such barriers. We discuss BVMs based on generalized midpoint methods, combined with appropriate numerical initial and final conditions. The stability regions of these methods intersect with a significant part of the complex plane. In several numerical experiments we will illustrate the usefulness of such methods when applied to PDE models, such as a dispersive wave equation, a space-fractional PDE and the backward heat equation.","tags":["Numerical methods for PDEs"],"title":"A generalized midpoint-based boundary value method for unstable partial differential equations","type":"publication"},{"authors":["Mohammadreza Salehi","Hossein Mirzaei","Dan Hendrycks","Yixuan Li","Mohammad Hossein Rohban","Mohammad Sabokrou"],"categories":null,"content":"","date":1673136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673136000,"objectID":"451babaf0af227f1929d05fe10694cb9","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/s.salehidehnavi-tmlr-2022/","publishdate":"2023-01-08T00:00:00Z","relpermalink":"/vislab/publication/s.salehidehnavi-tmlr-2022/","section":"publication","summary":"Machine learning models often encounter samples that are diverged from the training distribution. Failure to recognize an out-of-distribution (OOD) sample, and consequently assign that sample to an in-class label, significantly compromises the reliability of a model. The problem has gained significant attention due to its importance for safety deploying models in open-world settings. Detecting OOD samples is challenging due to the intractability of modeling all possible unknown distributions. To date, several research domains tackle the problem of detecting unfamiliar samples, including anomaly detection, novelty detection, one-class learning, open set recognition, and out-of-distribution detection. Despite having similar and shared concepts, out-of-distribution, open-set, and anomaly detection have been investigated independently. Accordingly, these research avenues have not cross-pollinated, creating research barriers. While some surveys intend to provide an overview of these approaches, they seem to only focus on a specific domain without examining the relationship between different domains. This survey aims to provide a cross domain and comprehensive review of numerous eminent works in respective areas while identifying their commonalities. Researchers can benefit from the overview of research advances in different fields and develop future methodology synergistically. Furthermore, to the best of our knowledge, while there are surveys in anomaly detection or one-class learning, there is no comprehensive or up-todate survey on out-of-distribution detection, which this survey covers extensively. Finally, having a unified cross-domain perspective, this study discusses and sheds light on future lines of research, intending to bring these fields closer together","tags":["Recognition (object detection, categorization)"],"title":"A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges","type":"publication"},{"authors":["Sindy Löwe","Phillip Lippe","Maja Rudolph","Max Welling"],"categories":null,"content":"","date":1673136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673136000,"objectID":"23d8802b6c985815fce0f4139af03103","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/sindy-tmlr-2022/","publishdate":"2023-01-08T00:00:00Z","relpermalink":"/vislab/publication/sindy-tmlr-2022/","section":"publication","summary":"Object-centric representations form the basis of human perception, and enable us to reason about the world and to systematically generalize to new settings. Currently, most works on unsupervised object discovery focus on slot-based approaches, which explicitly separate the latent representations of individual objects. While the result is easily interpretable, it usually requires the design of involved architectures. In contrast to this, we propose a comparatively simple approach - the Complex AutoEncoder (CAE) - that creates distributed object-centric representations. Following a coding scheme theorized to underlie object representations in biological neurons, its complex-valued activations represent two messages: their magnitudes express the presence of a feature, while the relative phase differences between neurons express which features should be bound together to create joint object representations. In contrast to previous approaches using complex-valued activations for object discovery, we present a fully unsupervised approach that is trained end-to-end - resulting in significant improvements in performance and efficiency. Further, we show that the CAE achieves competitive or better unsupervised object discovery performance on simple multi-object datasets compared to a state-of-the-art slot-based approach while being up to 100 times faster to train.","tags":["Visual reasoning and logical representation"],"title":"Complex-Valued Autoencoders for Object Discovery","type":"publication"},{"authors":["Yoni Schirris","Efstratios Gavves","Iris Nederlof","Hugo Mark Horling","Jonas Teuwen"],"categories":null,"content":"","date":1673136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673136000,"objectID":"cffc61e046e6de87c625d638dc5c5bfd","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/y.schirris-mia-2022/","publishdate":"2023-01-08T00:00:00Z","relpermalink":"/vislab/publication/y.schirris-mia-2022/","section":"publication","summary":"We propose a Deep learning-based weak label learning method for analyzing whole slide images (WSIs) of Hematoxylin and Eosin (H\u0026E) stained tumor tissue not requiring pixel-level or tile-level annotations using Self-supervised pre-training and heterogeneity-aware deep Multiple Instance LEarning (DeepSMILE). We apply DeepSMILE to the task of Homologous recombination deficiency (HRD) and microsatellite instability (MSI) prediction. We utilize contrastive self-supervised learning to pre-train a feature extractor on histopathology tiles of cancer tissue. Additionally, we use variability-aware deep multiple instance learning to learn the tile feature aggregation function while modeling tumor heterogeneity. For MSI prediction in a tumor-annotated and color normalized subset of TCGA-CRC (n=360 patients), contrastive self-supervised learning improves the tile supervision baseline from 0.77 to 0.87 AUROC, on par with our proposed DeepSMILE method. On TCGA-BC (n=1041 patients) without any manual annotations, DeepSMILE improves HRD classification performance from 0.77 to 0.81 AUROC compared to tile supervision with either a self-supervised or ImageNet pre-trained feature extractor. Our proposed methods reach the baseline performance using only 40% of the labeled data on both datasets. These improvements suggest we can use standard self-supervised learning techniques combined with multiple instance learning in the histopathology domain to improve genomic label classification performance with fewer labeled data.","tags":["AI for Health"],"title":"DeepSMILE: Contrastive self-supervised pre-training benefits MSI and HRD classification directly from H\u0026E whole-slide images in colorectal and breast cancer","type":"publication"},{"authors":["Haochen Wang","Jie Liu","Yongtuo Liu","Subhransu Maji","Jan-Jakob Sonke","Efstratios Gavves"],"categories":[],"content":"","date":1673136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673136000,"objectID":"d9113b720875d5c910363928b991630c","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/haochen-acmmm-2022/","publishdate":"2020-12-29T00:00:00Z","relpermalink":"/vislab/publication/haochen-acmmm-2022/","section":"publication","summary":"Few-shot instance segmentation aims to train an instance segmentation model that can fast adapt to novel classes with only a few reference images. Existing methods are usually derived from standard detection models and tackle few-shot instance segmentation indirectly by conducting classification, box regression, and mask prediction on a large set of redundant proposals followed by indispensable post-processing, e.g., Non-Maximum Suppression. Such complicated hand-crafted procedures and hyperparameters lead to degraded optimization and insufficient generalization ability. In this work, we propose an end-to-end Dynamic Transformer Network, DTN for short, to directly segment all target object instances from arbitrary categories given by reference images, relieving the requirements of dense proposal generation and post-processing. Specifically, a small set of Dynamic Queries, conditioned on reference images, are exclusively assigned to target object instances and generate all the instance segmentation masks of reference categories simultaneously. Moreover, a Semantic-induced Transformer Decoder is introduced to constrain the cross-attention between dynamic queries and target images within the pixels of the reference category, which suppresses the noisy interaction with the background and irrelevant categories. Extensive experiments are conducted on the COCO-20 dataset. The experiment results demonstrate that our proposed Dynamic Transformer Network significantly outperforms the state-of-the-arts.","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"Dynamic Transformer for Few-shot Instance Segmentation","type":"publication"},{"authors":["Yoni Schirris","Hugo Mark Horlings"],"categories":null,"content":"","date":1673136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673136000,"objectID":"c9b6b483cf60352c0c28979fd9c9e2c6","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/y.schirris-crm-2022-/","publishdate":"2023-01-08T00:00:00Z","relpermalink":"/vislab/publication/y.schirris-crm-2022-/","section":"publication","summary":"Lazard et al. predict homologous recombination deficiency from hematoxylin and eosin-stained slides of breast cancer tissue using deep learning. By controlling for technical artifacts on a curated dataset, the model puts forward novel HRD-related morphologies in luminal breast cancers.","tags":["AI for Health"],"title":"HRD-related morphology discovery in breast cancer by controlling for confounding factors","type":"publication"},{"authors":["T Preijers","MWF van Spengler","K Meijer","K Fijnvandraat","K Fischer","FWG Leebeek","MH Cnossen","RAA Mathôt"],"categories":null,"content":"","date":1673136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673136000,"objectID":"6d7e7e63b9c6c14fe3385248ea0c4378","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/max-ejcp-2022/","publishdate":"2023-01-08T00:00:00Z","relpermalink":"/vislab/publication/max-ejcp-2022/","section":"publication","summary":"Purpose Hemophilia B is a bleeding disorder, caused by a factor IX (FIX) deficiency. Recently, FIX concentrates with extended half-life (EHL) have become available. Prophylactic dosing of EHL-FIX concentrates can be optimized by assessment of individual pharmacokinetic (PK) parameters. To determine these parameters, limited sampling strategies (LSSs) may be applied. The study aims to establish adequate LSSs for estimating individual PK parameters of EHL-FIX concentrates using in silico evaluation. Methods Monte Carlo simulations were performed to obtain FIX activity versus time profiles using published population PK models for N9-GP (Refixia), rFIXFc (Alprolix), and rIX-FP (Idelvion). Fourteen LSSs, containing three or four samples taken within 8 days after administration, were formulated. Bayesian analysis was applied to obtain estimates for clearance (CL), half-life (t1/2), time to 1% (Time1%), and calculated weekly dose (Dose1%). Bias and precision of these estimates were assessed to determine which LSS was adequate. Results For all PK parameters of N9-GP, rFIXFc and rIX-FP bias was generally acceptable (range: −5% to 5%). For N9-GP, precision of all parameters for all LSSs was acceptable (","tags":["Pharmacology"],"title":"In silico evaluation of limited sampling strategies for individualized dosing of extended half-life factor IX concentrates in hemophilia B patients","type":"publication"},{"authors":["Yoni Schirris","Mendel Engelaer","Andreas Panteli","Hugo Mark Horlings","Efstratios Gavves","Jonas Teuwen"],"categories":null,"content":"","date":1673136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673136000,"objectID":"b3f75f3be87f07c96341776d4e1901bc","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/y.schirris-mi-2022-/","publishdate":"2023-01-08T00:00:00Z","relpermalink":"/vislab/publication/y.schirris-mi-2022-/","section":"publication","summary":"We present WeakSTIL, an interpretable two-stage weak label deep learning pipeline for scoring the percentage of stromal tumor infiltrating lymphocytes (sTIL%) in H\u0026E-stained whole-slide images (WSIs) of breast cancer tissue. The sTIL% score is a prognostic and predictive biomarker for many solid tumor types. However, due to the high labeling efforts and high intra- and interobserver variability within and between expert annotators, this biomarker is currently not used in routine clinical decision making. WeakSTIL compresses tiles of a WSI using a feature extractor pre-trained with self-supervised learning on unlabeled histopathology data and learns to predict precise sTIL% scores for each tile in the tumor bed by using a multiple instance learning regressor that only requires a weak WSI-level label. By requiring only a weak label, we overcome the large annotation efforts required to train currently existing TIL detection methods. We show that WeakSTIL is at least as good as other TIL detection methods when predicting the WSI-level sTIL% score, reaching a coefficient of determination of 0.45 ± 0.15 when compared to scores generated by an expert pathologist, and an AUC of 0.89 ± 0.05 when treating it as the clinically interesting sTIL-high vs sTIL-low classification task. Additionally, we show that the intermediate tile-level predictions of WeakSTIL are highly interpretable, which suggests that WeakSTIL pays attention to latent features related to the number of TILs and the tissue type. In the future, WeakSTIL may be used to provide consistent and interpretable sTIL% predictions to stratify breast cancer patients into targeted therapy arms.","tags":["AI for Health"],"title":"WeakSTIL: weak whole-slide image level stromal tumor infiltrating lymphocyte scores are all you need","type":"publication"},{"authors":["Petr Byvshev","Pascal Mettes","Yu Xiao"],"categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"08d0584c14ce0d87546299eebbad50c7","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pascal-cviu-2022/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/vislab/publication/pascal-cviu-2022/","section":"publication","summary":"3D convolutional networks, as direct inheritors of 2D convolutional networks for images, have placed their mark on action recognition in videos. Combined with pretraining on large-scale video data, high classification accuracies have been obtained on numerous video benchmarks. In an effort to better understand why 3D convolutional networks are so effective, several works have highlighted their bias towards static appearance and towards the scenes in which actions occur. In this work, we seek to find the source of this bias and question whether the observed biases towards static appearances are inherent to 3D convolutional networks or represent limited significance of motion in the training data. We resolve this by presenting temporality measures that estimate the data-to-model motion dependency at both the layer-level and the kernel-level. Moreover, we introduce two synthetic datasets where motion and appearance are decoupled by design, which allows us to directly observe their effects on the networks. Our analysis shows that 3D architectures are not inherently biased towards appearance. When trained on the most prevalent video sets, 3D convolutional networks are indeed biased throughout, especially in the final layers of the network. However, when training on data with motions and appearances explicitly decoupled and balanced, such networks adapt to varying levels of temporality. To this end, we see the proposed measures as a reliable method to estimate motion relevance for activity classification in datasets and use them to uncover the differences between popular pre-training video collections, such as Kinetics, IG-65M and Howto100 m.","tags":["Action and behavior recognition"],"title":"Are 3D convolutional networks inherently biased towards appearance?","type":"publication"},{"authors":["William Thong","Cees G M Snoek"],"categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"cf6bf9ae78f639abc43fb3df97c3ec6b","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/william-tomm-2022/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/vislab/publication/william-tomm-2022/","section":"publication","summary":"This paper strives for a diversely-supervised visual product search, where queries specify a diverse set of labels to search for. Where previous works have focused on representing attribute, instance or category labels individually, we consider them together to create a diverse set of labels for visually describing products. We learn an embedding from the supervisory signal provided by every label to encode their interrelationships. Once trained, every label has a corresponding visual representation in the embedding space, which is an aggregation of selected items from the training set. At search time, composite query representations retrieve images that match a specific set of diverse labels. We form composite query representations by averaging over the aggregated representations of each diverse label in the specific set. For evaluation, we extend existing product datasets of cars and clothes with a diverse set of labels. Experiments show the benefits of our embedding for diversely-supervised visual product search in seen and unseen product combinations, and for discovering product design styles.","tags":["Image retrieval"],"title":"Diversely-Supervised Visual Product Search","type":"publication"},{"authors":["Pascal Mettes"],"categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"4e76522d978fe0534f89cdd67a337381","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pascal-icml-2022/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/vislab/publication/pascal-icml-2022/","section":"publication","summary":"This work proposes codebook encodings for graph networks that operate on hyperbolic manifolds. Where graph networks commonly learn node representations in Euclidean space, recent work has provided a generalization to Riemannian manifolds, with a particular focus on the hyperbolic space. Expressive node representations are obtained by repeatedly performing a logarithmic map, followed by message passing in the tangent space and an exponential map back to the manifold at hand. Where current hyperbolic graph approaches predominantly focus on node representation, we propose a way to aggregate over nodes for graph-level inference. We introduce Hyperbolic Graph Codebooks, a family of graph encodings where a shared codebook is learned and used to aggregate over nodes. The resulting representations are permutation invariant and fixed-size, yet expressive. We show how to obtain zeroth-order codebook encodings through soft assignments over hyperbolic distances, first-order encodings with anchored logarithmic mappings, and second-order encodings by computing variance information in the tangent space. Empirically, we highlight the effectiveness of our approach, especially when few examples and embedding dimensions are available.","tags":["Recognition (object detection, categorization)"],"title":"Hyperbolic Graph Codebooks","type":"publication"},{"authors":["Jelle A. van Dijk","Maartje C. de Jong","Gio Piantoni","Alessio Fracasso","Mariska J. Vansteensel","Iris. I. A. Groen","Natalia Petridou","Serge O. Dumoulin",""],"categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"f9868ede5e481d37d717c7d2c5ea83d2","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/jelle-plos-2022/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/vislab/publication/jelle-plos-2022/","section":"publication","summary":"Numerosity is the set size of a group of items. Numerosity perception is a trait shared across numerous species. Numerosity-selective neural populations are thought to underlie numerosity perception. These neurons have been identified primarily using electrical recordings in animal models and blood oxygen level dependent (BOLD) functional magnetic resonance imaging (fMRI) in humans. Here we use electrical intracranial recordings to investigate numerosity tuning in humans, focusing on high-frequency transient activations. These recordings combine a high spatial and temporal resolution and can bridge the gap between animal models and human recordings. In line with previous studies, we find numerosity-tuned responses at parietal sites in two out of three participants. Neuronal populations at these locations did not respond to other visual stimuli, i.e. faces, houses, and letters, in contrast to several occipital sites. Our findings further corroborate the specificity of numerosity tuning of in parietal cortex, and further link fMRI results and electrophysiological recordings.","tags":["Neural information processing"],"title":"Intracranial recordings show evidence of numerosity tuning in human parietal cortex","type":"publication"},{"authors":["Mohammad Mahdi Derakhshani","Ivona Najdenkoska","Tom van Sonsbeek","Xiantong Zhen","Dwarikanath Mahapatra","Marcel Worring","Cees G M Snoek"],"categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"e5c5c540ca26292c9e9313b9b716ce50","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mohammad-miccai-2022/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/vislab/publication/mohammad-miccai-2022/","section":"publication","summary":"Deep learning models have shown a great effectiveness in recognition of findings in medical images. However, they cannot handle the ever-changing clinical environment, bringing newly annotated medical data from different sources. To exploit the incoming streams of data, these models would benefit largely from sequentially learning from new samples, without forgetting the previously obtained knowledge. In this paper we introduce LifeLonger, a benchmark for continual disease classification on the MedMNIST collection, by applying existing state-of-the-art continual learning methods. In particular, we consider three continual learning scenarios, namely, task and class incremental learning and the newly defined cross-domain incremental learning. Task and class incremental learning of diseases address the issue of classifying new samples without re-training the models from scratch, while cross-domain incremental learning addresses the issue of dealing with datasets originating from different institutions while retaining the previously obtained knowledge. We perform a thorough analysis of the performance and examine how the well-known challenges of continual learning, such as the catastrophic forgetting exhibit themselves in this setting. The encouraging results demonstrate that continual learning has a major potential to advance disease classification and to produce a more robust and efficient learning framework for clinical settings. The code repository, data partitions and baseline results for the complete benchmark are publicly available.","tags":["Continual learning"],"title":"LifeLonger: A Benchmark for Continual Disease Classification","type":"publication"},{"authors":["Osman Ülger","Julian Wiederer","Mohsen Ghafoorian","Vasileios Belagiannis","Pascal Mettes"],"categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"7dba47e5a8daa385a9c70dcbe9c6364a","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pascal-bmvc-2022/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/vislab/publication/pascal-bmvc-2022/","section":"publication","summary":"Graph neural networks have shown to learn effective node representations, enabling node-, link-, and graph-level inference. Conventional graph networks assume static relations between nodes, while relations between entities in a video often evolve over time, with nodes entering and exiting dynamically. In such temporally-dynamic graphs, a core problem is inferring the future state of spatio-temporal edges, which can constitute multiple types of relations. To address this problem, we propose MTD-GNN, a graph network for predicting temporally-dynamic edges for multiple types of relations. We propose a factorized spatio-temporal graph attention layer to learn dynamic node representations and present a multi-task edge prediction loss that models multiple relations simultaneously. The proposed architecture operates on top of scene graphs that we obtain from videos through object detection and spatio-temporal linking. Experimental evaluations on ActionGenome and CLEVRER show that modeling multiple relations in our temporally-dynamic graph network can be mutually beneficial, outperforming existing static and spatio-temporal graph neural networks, as well as state-of-the-art predicate classification methods.","tags":["Video analysis and understanding"],"title":"Multi-Task Edge Prediction in Temporally-Dynamic Video Graphs","type":"publication"},{"authors":["David W Zhang","Gertjan J Burghouts","Cees G M Snoek"],"categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"35f2c3587d2a1ee99aaa7be1c0a5f484","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/david-log-2022-/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/vislab/publication/david-log-2022-/","section":"publication","summary":"This paper aims for set-to-hypergraph prediction, where the goal is to infer the set of relations for a given set of entities. This is a common abstraction for applications in particle physics, biological systems and combinatorial optimization. We address two common scaling problems encountered in set-to-hypergraph tasks that limit the size of the input set: the exponentially growing number of hyperedges and the run-time complexity, both leading to higher memory requirements. We make three contributions. First, we propose to predict and supervise the positive edges only, which changes the asymptotic memory scaling from exponential to linear. Second, we introduce a training method that encourages iterative refinement of the predicted hypergraph, which allows us to skip iterations in the backward pass for improved efficiency and constant memory usage. Third, we combine both contributions in a single set-to-hypergraph model that enables us to address problems with larger input set sizes. We provide ablations for our main technical contributions and show that our model outperforms prior state-of-the-art, especially for larger sets.","tags":["Hypergraphs"],"title":"Pruning Edges and Gradients to Learn Hypergraphs from Larger Sets","type":"publication"},{"authors":["Pascal Mettes"],"categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"ae51b2abec98c632b7741c066c63979f","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pascal-icmr-2022/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/vislab/publication/pascal-icmr-2022/","section":"publication","summary":"This work proposes codebook encodings for graph networks that operate on hyperbolic manifolds. Where graph networks commonly learn node representations in Euclidean space, recent work has provided a generalization to Riemannian manifolds, with a particular focus on the hyperbolic space. Expressive node representations are obtained by repeatedly performing a logarithmic map, followed by message passing in the tangent space and an exponential map back to the manifold at hand. Where current hyperbolic graph approaches predominantly focus on node representation, we propose a way to aggregate over nodes for graph-level inference. We introduce Hyperbolic Graph Codebooks, a family of graph encodings where a shared codebook is learned and used to aggregate over nodes. The resulting representations are permutation invariant and fixed-size, yet expressive. We show how to obtain zeroth-order codebook encodings through soft assignments over hyperbolic distances, first-order encodings with anchored logarithmic mappings, and second-order encodings by computing variance information in the tangent space. Empirically, we highlight the effectiveness of our approach, especially when few examples and embedding dimensions are available.","tags":["Video analysis and understanding"],"title":"Teaching a New Dog Old Tricks: Contrastive Random Walks in Videos with Unsupervised Priors","type":"publication"},{"authors":["Groen IIA","Piantoni G","Montenegro S","Flinker A","Devore S","Devinsky O","Doyle W","Dugan P","Friedman D","Ramsey N","Petridou N","Winawer JA"],"categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"26ebefe0427e4ee8d5b62a0e17f3e7db","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/groen-iia-2022/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/vislab/publication/groen-iia-2022/","section":"publication","summary":"Neural responses to visual stimuli exhibit complex temporal dynamics, including subadditive temporal summation, response reduc-tion with repeated or sustained stimuli (adaptation), and slower dynamics at low contrast. These phenomena are often studied in-dependently. Here, we demonstrate these phenomena within the same experiment and model the underlying neural computations with a single computational model. We extracted time-varying responses from electrocorticographic recordings from patients pre-sented with stimuli that varied in duration, interstimulus interval (ISI) and contrast. Aggregating data across patients from both sexes yielded 98 electrodes with robust visual responses, covering both earlier (V1–V3) and higher-order (V3a/b, LO, TO, IPS) reti-notopic maps. In all regions, the temporal dynamics of neural responses exhibit several nonlinear features. Peak response ampli-tude saturates with high contrast and longer stimulus durations, the response to a second stimulus is suppressed for short ISIs and recovers for longer ISIs, and response latency decreases with increasing contrast. These features are accurately captured by a computational model composed of a small set of canonical neuronal operations, that is, linear filtering, rectification, exponentia-tion, and a delayed divisive normalization. We find that an increased normalization term captures both contrast- and adaptation-related response reductions, suggesting potentially shared underlying mechanisms. We additionally demonstrate both changes and invariance in temporal response dynamics between earlier and higher-order visual areas. Together, our results reveal the presence of a wide range of temporal and contrast-dependent neuronal dynamics in the human visual cortex and demonstrate that a simple model captures these dynamics at millisecond resolution.","tags":["Neural information processing"],"title":"Temporal dynamics of neural responses in human visual cortex","type":"publication"},{"authors":["Jiayi Shen","Zehao Xiao","Xiantong Zhen","Cees G. M. Snoek","Marcel Worring"],"categories":[],"content":"","date":1666396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666396800,"objectID":"606161ed4b184c948ea5ace74cd425aa","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/shen-neurips-2022/","publishdate":"2022-10-22T00:00:00Z","relpermalink":"/vislab/publication/shen-neurips-2022/","section":"publication","summary":"In this paper, we focus on multi-task classification, where related classification tasks share the same label space and are learned simultaneously. In particular, we tackle a new setting, which is more realistic than currently addressed in the literature, where categories shift from training to test data. Hence, individual tasks do not contain complete training data for the categories in the test set. To generalize to such test data, it is crucial for individual tasks to leverage knowledge from related tasks. To this end, we propose learning an association graph to transfer knowledge among tasks for missing classes. We construct the association graph with nodes representing tasks, classes and instances, and encode the relationships among the nodes in the edges to guide their mutual knowledge transfer. By message passing on the association graph, our model enhances the categorical information of each instance, making it more discriminative. To avoid spurious correlations between task and class nodes in the graph, we introduce an assignment entropy maximization that encourages each class node to balance its edge weights. This enables all tasks to fully utilize the categorical information from related tasks. An extensive evaluation on three general benchmarks and a medical dataset for skin lesion classification reveals that our method consistently performs better than representative baselines.","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"Association Graph Learning for Multi-Task Classification with Category Shifts","type":"publication"},{"authors":["ChangYong Oh","Roberto Bondesan","Efstratios Gavves","Max Welling"],"categories":[],"content":"","date":1666396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666396800,"objectID":"cc19a6a637efd600b9b1719ef59a6f5f","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/oh-neurips-2022-/","publishdate":"2022-10-22T00:00:00Z","relpermalink":"/vislab/publication/oh-neurips-2022-/","section":"publication","summary":"In this work we propose a batch Bayesian optimization method for combinatorial problems on permutations, which is well suited for expensive-to-evaluate objectives. We first introduce LAW, an efficient batch acquisition method based on determinantal point processes using the acquisition weighted kernel. Relying on multiple parallel evaluations, LAW enables accelerated search on combinatorial spaces. We then apply the framework to permutation problems, which have so far received little attention in the Bayesian Optimization literature, despite their practical importance. We call this method LAW2ORDER.On the theoretical front, we prove that LAW2ORDER has vanishing simple regret by showing that the batch cumulative regret is sublinear. Empirically, we assess the method on several standard combinatorial problems involving permutations such as quadratic assignment, flowshop scheduling and the traveling salesman, as well as on a structure learning task.","tags":["Bayesian Optimization"],"title":"Batch Bayesian Optimization on Permutations using the Acquisition Weighted Kernel","type":"publication"},{"authors":["Artem Moskalev","Anna Sepliarskaia","Ivan Sosnovik","Arnold Smeulders"],"categories":[],"content":"","date":1666396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666396800,"objectID":"8929d88edc416830ee20c78fb1eabf52","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/moskalev-neurips-2022/","publishdate":"2022-10-22T00:00:00Z","relpermalink":"/vislab/publication/moskalev-neurips-2022/","section":"publication","summary":"Symmetries built into a neural network have appeared to be very beneficial for a wide range of tasks as it saves the data to learn them. We depart from the position that when symmetries are not built into a model a priori, it is advantageous for robust networks to learn symmetries directly from the data to fit a task function. In this paper, we present a method to extract symmetries learned by a neural network and to evaluate the degree to which a network is invariant to them. With our method, we are able to explicitly retrieve learned invariances in a form of the generators of corresponding Lie-groups without prior knowledge of symmetries in the data. We use the proposed method to study how symmetrical properties depend on a neural network's parameterization and configuration. We found that the ability of a network to learn symmetries generalizes over a range of architectures. However, the quality of learned symmetries depends on the depth and the number of parameters.","tags":["Interpretability","Symmetry learning","Invariance"],"title":"LieGG: Studying Learned Lie Group Generators","type":"publication"},{"authors":["Tejaswi Kasarla","Gertjan Burghouts","Max van Spengler","Elise van der Pol","Rita Cucchiara","Pascal Mettes"],"categories":[],"content":"","date":1666396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666396800,"objectID":"8065a92ece7602da379aed95d64492b4","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/kasarla-neurips-2022/","publishdate":"2022-10-22T00:00:00Z","relpermalink":"/vislab/publication/kasarla-neurips-2022/","section":"publication","summary":"Maximizing the separation between classes constitutes a well-known inductive bias in machine learning and a pillar of many traditional algorithms. By default, deep networks are not equipped with this inductive bias and therefore many alternative solutions have been proposed through differential optimization. Current approaches tend to optimize classification and separation jointly: aligning inputs with class vectors and separating class vectors angularly. This paper proposes a simple alternative: encoding maximum separation as an inductive bias in the network by adding one fixed matrix multiplication before computing the softmax activations. The main observation behind our approach is that separation does not require optimization but can be solved in closed-form prior to training and plugged into a network. We outline a recursive approach to obtain the matrix consisting of maximally separable vectors for any number of classes, which can be added with negligible engineering effort and computational overhead. Despite its simple nature, this one matrix multiplication provides real impact. We show that our proposal directly boosts classification, long-tailed recognition, out-of-distribution detection, and open-set recognition, from CIFAR to ImageNet. We find empirically that maximum separation works best as a fixed bias; making the matrix learnable adds nothing to the performance. The closed-form implementation and code to reproduce the experiments are on github.","tags":["Maximum Separation","Inductive Bias","Classification","Long-tailed recognition","Out-of-Distribution Detection","Open-set recognition"],"title":"Maximum Class Separation as Inductive Bias in One Matrix","type":"publication"},{"authors":["Mengmeng Jing","Xiantong Zhen","Jingjing Li","Cees G. M. Snoek"],"categories":[],"content":"","date":1666396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666396800,"objectID":"d1dcd3bd6fc532e1eb0bcaaf436421c9","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/jing-neurips-2022/","publishdate":"2022-10-22T00:00:00Z","relpermalink":"/vislab/publication/jing-neurips-2022/","section":"publication","summary":"We aim for source-free domain adaptation, where the task is to deploy a model pre-trained on source domains to target domains. The challenges stem from the distribution shift from the source to the target domain, coupled with the unavailability of any source data and labeled target data for optimization. Rather than fine-tuning the model by updating the parameters, we propose to perturb the source model to achieve adaptation to target domains. We introduce perturbations into the model parameters by variational Bayesian inference in a probabilistic framework. By doing so, we can effectively adapt the model to the target domain while largely preserving the discriminative ability. Importantly, we demonstrate the theoretical connection to learning Bayesian neural networks, which proves the generalizability of the perturbed model to target domains. To enable more efficient optimization, we further employ a parameter sharing strategy, which substantially reduces the learnable parameters compared to a fully Bayesian neural network. Our model perturbation provides a new probabilistic way for domain adaptation which enables efficient adaptation to target domains while maximally preserving knowledge in source models. Experiments on several source-free benchmarks under three different evaluation settings verify the effectiveness of the proposed variational model perturbation for source-free domain adaptation.","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"Variational Model Perturbation for Source-Free Domain Adaptation","type":"publication"},{"authors":["Johann Brehmer","Pim De Haan","Phillip Lippe","Taco Cohen"],"categories":[],"content":"","date":1666396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666396800,"objectID":"06b5d00ff24caf6b725a5449dc5bf0e0","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/brehmer-neurips-2022/","publishdate":"2022-10-22T00:00:00Z","relpermalink":"/vislab/publication/brehmer-neurips-2022/","section":"publication","summary":"Learning high-level causal representations together with a causal model from unstructured low-level data such as pixels is impossible from observational data alone. We prove under mild assumptions that this representation is however identifiable in a weakly supervised setting. This involves a dataset with paired samples before and after random, unknown interventions, but no further labels. We then introduce implicit latent causal models, variational autoencoders that represent causal variables and causal structure without having to optimize an explicit discrete graph structure. On simple image data, including a novel dataset of simulated robotic manipulation, we demonstrate that such models can reliably identify the causal structure and disentangle causal variables.","tags":["Causal Representation Learning"],"title":"Weakly supervised causal representation learning","type":"publication"},{"authors":["Yunlu Chen","Basura Fernando","Hakan Bilen","Matthias Niessner","Efstratios Gavves"],"categories":null,"content":"","date":1663977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663977600,"objectID":"a7c91952be23838c58c0a069e7020cea","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yunlu-eccv-2022/","publishdate":"2022-09-24T00:00:00Z","relpermalink":"/vislab/publication/yunlu-eccv-2022/","section":"publication","summary":"In recent years, neural implicit representations have made remarkable progress in modeling of 3D shapes with arbitrary topology. In this work, we address two key limitations of such representations, in failing to capture local 3D geometric fine details, and to learn from and generalize to shapes with unseen 3D transformations. To this end, we introduce a novel family of graph implicit functions with equivariant layers that facilitates modeling fine local details and guaranteed robustness to various groups of geometric transformations, through local k-NN graph embeddings with sparse point set observations at multiple resolutions. Our method improves over the existing rotation-equivariant implicit function from 0.69 to 0.89 (IoU) on the ShapeNet reconstruction task. We also show that our equivariant implicit function can be extended to other types of similarity transformations and generalizes to unseen translations and scaling.","tags":["3D computer vision"],"title":"3D Equivariant Graph Implicit Functions","type":"publication"},{"authors":["Artem Moskalev","Ivan Sosnovik","Volker Fischer","Arnold Smeulders"],"categories":null,"content":"","date":1663977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663977600,"objectID":"10834e639c6a0b9c8560cf83ec92f2f4","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/artem-eccv-2022/","publishdate":"2022-09-24T00:00:00Z","relpermalink":"/vislab/publication/artem-eccv-2022/","section":"publication","summary":"The standard approach to contrastive learning is to maximize the agreement between different views of the data. The views are ordered in pairs, such that they are either positive, encoding different views of the same object, or negative, corresponding to views of different objects. The supervisory signal comes from maximizing the total similarity over positive pairs, while the negative pairs are needed to avoid collapse. In this work, we note that the approach of considering individual pairs cannot account for both intra-set and inter-set similarities when the sets are formed from the views of the data. It thus limits the information content of the supervisory signal available to train representations. We propose to go beyond contrasting individual pairs of objects by focusing on contrasting objects as sets. For this, we use combinatorial quadratic assignment theory designed to evaluate set and graph similarities and derive set-contrastive objective as a regularizer for contrastive learning methods. We conduct experiments and demonstrate that our method improves learned representations for the tasks of metric learning and self-supervised classification.","tags":["Self-supervised learning"],"title":"Contrasting quadratic assignments for set-based representation learning","type":"publication"},{"authors":["Amirhossein Habibian","Haitam Ben Yahia","Davide Abati","Efstratios Gavves","Fatih Porikli"],"categories":null,"content":"","date":1663977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663977600,"objectID":"102680fb7ebce4762c9136c0707ebab0","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/amirhossein-eccv-2022/","publishdate":"2022-09-24T00:00:00Z","relpermalink":"/vislab/publication/amirhossein-eccv-2022/","section":"publication","summary":"This paper aims to accelerate video stream processing, such as object detection and semantic segmentation, by leveraging the temporal redundancies that exist between video frames. Instead of relying on explicit motion alignment, such as optical flow warping, we propose a novel knowledge distillation schema coined as Delta Distillation. In our proposal, the student learns the variations in the teacher's intermediate features over time. We demonstrate that these temporal variations can be effectively distilled due to the temporal redundancies within video frames. During inference, both teacher and student cooperate for providing predictions, the former by providing initial representations extracted only on the key-frame, and the latter by iteratively estimating and applying deltas for the successive frames. Moreover, we consider various design choices to learn optimal student architectures including an end-to-end learnable architecture search. By extensive experiments on a wide range of architectures, including the most efficient ones, we demonstrate that delta distillation sets a new state of the art in terms of accuracy vs. efficiency tradeoff for semantic segmentation and object detection in videos. Finally, we show that, as a by-product, delta distillation improves the temporal consistency of the teacher model.","tags":["Motion and tracking"],"title":"Delta Distillation for Efficient Video Processing","type":"publication"},{"authors":["Fida Mohammad Thoker","Hazel Doughty","Piyush Bagad","Cees Snoek"],"categories":null,"content":"","date":1663977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663977600,"objectID":"feff67828179596cec0320534b4c2c8a","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/fida-eccv-2022/","publishdate":"2022-09-24T00:00:00Z","relpermalink":"/vislab/publication/fida-eccv-2022/","section":"publication","summary":"Despite the recent success of video self-supervised learning models, there is much still to be understood about their generalization capability. In this paper, we investigate how sensitive video self-supervised learning is to the current conventional benchmark and whether methods generalize beyond the canonical evaluation setting. We do this across four different factors of sensitivity: domain, samples, actions and task. Our study which encompasses over 500 experiments on 7 video datasets, 9 self-supervised methods and 6 video understanding tasks, reveals that current benchmarks in video self-supervised learning are not good indicators of generalization along these sensitivity factors. Further, we find that self-supervised methods considerably lag behind vanilla supervised pre-training, especially when domain shift is large and the amount of available downstream samples are low. From our analysis, we distill the SEVERE-benchmark, a subset of our experiments, and discuss its implication for evaluating the generalizability of representations obtained by existing and future self-supervised video learning methods.","tags":["Self-supervised learning"],"title":"How Severe is Benchmark-Sensitivity in Video Self-Supervised Learning?","type":"publication"},{"authors":["Pengwan Yang","Yuki M. Asano","Pascal Mettes","Cees G. M. Snoek"],"categories":null,"content":"","date":1663977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663977600,"objectID":"744e129a0d333c97711dfd9898778e4f","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pengwan-eccv-2022/","publishdate":"2022-09-24T00:00:00Z","relpermalink":"/vislab/publication/pengwan-eccv-2022/","section":"publication","summary":"The goal of this paper is to bypass the need for labelled examples in few-shot video understanding at run time. While proven effective, in many practical video settings even labelling a few examples appears unrealistic. This is especially true as the level of details in spatio-temporal video understanding and with it, the complexity of annotations continues to increase. Rather than performing few-shot learning with a human oracle to provide a few densely labelled support videos, we propose to automatically learn to find appropriate support videos given a query. We call this self-shot learning and we outline a simple self-supervised learning method to generate an embedding space well-suited for unsupervised retrieval of relevant samples. To showcase this novel setting, we tackle, for the first time, video instance segmentation in a self-shot (and few-shot) setting, where the goal is to segment instances at the pixel-level across the spatial and temporal domains. We provide strong baseline performances that utilize a novel transformer-based model and show that self-shot learning can even surpass few-shot and can be positively combined for further performance gains. Experiments on new benchmarks show that our approach achieves strong performance, is competitive to oracle support in some settings, scales to large unlabelled video collections, and can be combined in a semi-supervised setting.","tags":["Video analysis and understanding"],"title":"Less than Few: Self-Shot Video Instance Segmentation","type":"publication"},{"authors":["Laura Hanu","Yuki M. Asano","James Thewlis","Christian Rupprecht"],"categories":null,"content":"","date":1663977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663977600,"objectID":"a4abbccb3a45ff60212d9635e7d1c489","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/laura-eccv-2022/","publishdate":"2022-09-24T00:00:00Z","relpermalink":"/vislab/publication/laura-eccv-2022/","section":"publication","summary":"Multi-modal retrieval is an important problem for many applications, such as recommendation and search. Current benchmarks and even datasets are often manually constructed and consist of mostly clean samples where all modalities are well-correlated with the content. Thus, current video-text retrieval literature largely focuses on video titles or audio transcripts, while ignoring user comments, since users often tend to discuss topics only vaguely related to the video. Despite the ubiquity of user comments online, there is currently no multi-modal representation learning datasets that includes comments. In this paper, we a) introduce a new dataset of videos, titles and comments; b) present an attention-based mechanism that allows the model to learn from sometimes irrelevant data such as comments; c) show that by using comments, our method is able to learn better, more contextualised, representations for image, video and audio representations. Project page: https://unitaryai.github.io/vtc-paper","tags":["Multi-modal learning"],"title":"VTC: Improving Video-Text Retrieval with User Comments","type":"publication"},{"authors":["Leonard Bereska","Efstratios Gavves"],"categories":null,"content":"","date":1657411200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657411200,"objectID":"fee6927d4efc7c595b1e6cd1d3bde383","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/leonard-icml-2022-/","publishdate":"2022-07-10T00:00:00Z","relpermalink":"/vislab/publication/leonard-icml-2022-/","section":"publication","summary":"Machine learning recently proved efficient in learning differential equations and dynamical systems from data. However, the data is commonly assumed to originate from a single never-changing system. In contrast, when modeling real-world dynamical processes, the data distribution often shifts due to changes in the underlying system dynamics. Continual learning of these processes aims to rapidly adapt to abrupt system changes without forgetting previous dynamical regimes. This work proposes an approach to continual learning based on reservoir computing, a state-of-the-art method for training recurrent neural networks on complex spatiotemporal dynamical systems. Reservoir computing fixes the recurrent network weights - hence these cannot be forgotten - and only updates linear projection heads to the output. We propose to train multiple competitive prediction heads concurrently. Inspired by neuroscience’s predictive coding, only the most predictive heads activate, laterally inhibiting and thus protecting the inactive heads from forgetting induced by interfering parameter updates. We show that this multi-head reservoir minimizes interference and catastrophic forgetting on several dynamical systems, including the Van-der-Pol oscillator, the chaotic Lorenz attractor, and the high-dimensional Lorenz-96 weather model. Our results suggest that reservoir computing is a promising candidate framework for the continual learning of dynamical systems. We provide our code for data generation, method and comparisons at https://github.com/leonardbereska/multiheadreservoir","tags":["Continual Learning"],"title":"Continual Learning of Dynamical Systems With Competitive Federated Reservoir Computing","type":"publication"},{"authors":["Jie Liu","Yanqi Bao","Wenzhe Yin","Haochen Wang","Yang Gao","Jan-Jakob Sonke","Efstratios Gavves"],"categories":null,"content":"","date":1654992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654992000,"objectID":"998355fc0abdb4cd1db7313255f941fc","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/jieliu-bmvc-2022/","publishdate":"2022-06-12T00:00:00Z","relpermalink":"/vislab/publication/jieliu-bmvc-2022/","section":"publication","summary":"Few-shot semantic segmentation (FSS) aims to achieve novel objects segmentation with only a few annotated samples and has made great progress recently. Most of the existing FSS models focus on the feature matching between support and query to tackle FSS. However, the appearance variations between objects from the same category could be extremely large, leading to unreliable feature matching and query mask prediction. To this end, we propose a Support-induced Graph Convolutional Network (SiGCN) to explicitly excavate latent context structure in query images. Specifically, we propose a Support-induced Graph Reasoning (SiGR) module to capture salient query object parts at different semantic levels with a Support-induced GCN. Furthermore, an instance association (IA) module is designed to capture high-order instance context from both support and query instances. By integrating the proposed two modules, SiGCN can learn rich query context representation, and thus being more robust to appearance variations. Extensive experiments on PASCAL-5i and COCO-20i demonstrate that our SiGCN achieves state-of-the-art performance.","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"Few-shot Semantic Segmentation with Support-induced Graph Convolutional Network","type":"publication"},{"authors":["Phillip Lippe","Sara Magliacane","Sindy Löwe","Yuki M. Asano","Taco Cohen","Efstratios Gavves"],"categories":null,"content":"","date":1653004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653004800,"objectID":"662dcdf81695921cd3ea214dbd4e0a4c","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/phillip-icml-2022/","publishdate":"2022-05-20T00:00:00Z","relpermalink":"/vislab/publication/phillip-icml-2022/","section":"publication","summary":"Understanding the latent causal factors of a dynamical system from visual observations is a crucial step towards agents reasoning in complex environments. In this paper, we propose CITRIS, a variational autoencoder framework that learns causal representations from temporal sequences of images in which underlying causal factors have possibly been intervened upon. In contrast to the recent literature, CITRIS exploits temporality and observing intervention targets to identify scalar and multidimensional causal factors, such as 3D rotation angles. Furthermore, by introducing a normalizing flow, CITRIS can be easily extended to leverage and disentangle representations obtained by already pretrained autoencoders. Extending previous results on scalar causal factors, we prove identifiability in a more general setting, in which only some components of a causal factor are affected by interventions. In experiments on 3D rendered image sequences, CITRIS outperforms previous methods on recovering the underlying causal variables. Moreover, using pretrained autoencoders, CITRIS can even generalize to unseen instantiations of causal factors, opening future research areas in sim-to-real generalization for causal representation learning.","tags":["Causal Representation Learning"],"title":"CITRIS - Causal Identifiability from Temporal Intervened Sequences","type":"publication"},{"authors":["David M. Knigge","David W. Romero","Erik J. Bekkers"],"categories":null,"content":"","date":1653004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653004800,"objectID":"7e791e5b6c19e51bf35f93720bd64251","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/david-icml-2022/","publishdate":"2022-05-20T00:00:00Z","relpermalink":"/vislab/publication/david-icml-2022/","section":"publication","summary":"Group convolutional neural networks (G-CNNs) have been shown to increase parameter efficiency and model accuracy by incorporating geometric inductive biases. In this work, we investigate the properties of representations learned by regular G-CNNs, and show considerable parameter redundancy in group convolution kernels. This finding motivates further weight-tying by sharing convolution kernels over subgroups. To this end, we introduce convolution kernels that are separable over the subgroup and channel dimensions. In order to obtain equivariance to arbitrary affine Lie groups we provide a continuous parameterisation of separable convolution kernels. We evaluate our approach across several vision datasets, and show that our weight sharing leads to improved performance and computational efficiency. In many settings, separable G-CNNs outperform their non-separable counterpart, while only using a fraction of their training time. In addition, thanks to the increase in computational efficiency, we are able to implement G-CNNs equivariant to the Sim(2)-group; the group of dilations, rotations and translations. Sim(2)-equivariance further improves performance on all tasks considered.","tags":["Equivariance","Efficient Deep Learning","Regular Group Convolutions"],"title":"Exploiting Redundancy: Separable Group Convolutional Networks on Lie Groups","type":"publication"},{"authors":["Yunhua Zhang","Hazel Doughty","Ling Shao","Cees Snoek"],"categories":null,"content":"","date":1652054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652054400,"objectID":"44f559711b8fa1b7f8a89b16af128208","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yunhua-cvpr-2022/","publishdate":"2022-05-09T00:00:00Z","relpermalink":"/vislab/publication/yunhua-cvpr-2022/","section":"publication","summary":"This paper strives for activity recognition under domain shift, for example caused by change of scenery or camera viewpoint. The leading approaches reduce the shift in activity appearance by adversarial training and self-supervised learning. Different from these vision-focused works we leverage activity sounds for domain adaptation as they have less variance across domains and can reliably indicate which activities are not happening. We propose an audio-adaptive encoder and associated learning methods that discriminatively adjust the visual feature representation as well as addressing shifts in the semantic distribution. To further eliminate domain-specific features and include domain-invariant activity sounds for recognition, an audio-infused recognizer is proposed, which effectively models the cross-modal interaction across domains. We also introduce the new task of actor shift, with a corresponding audio-visual dataset, to challenge our method with situations where the activity appearance changes dramatically. Experiments on this dataset, EPIC-Kitchens and CharadesEgo show the effectiveness of our approach.","tags":["Multi-modal learning"],"title":"Audio-Adaptive Activity Recognition Across Video Domains","type":"publication"},{"authors":["Duy-Kien Nguyen","Jihong Ju","Olaf Booij","Martin R. Oswald","Cees Snoek"],"categories":null,"content":"","date":1652054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652054400,"objectID":"4b321ebbe9d7a5e2068f3c39562af61e","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/nguyen-cvpr-2022/","publishdate":"2022-05-09T00:00:00Z","relpermalink":"/vislab/publication/nguyen-cvpr-2022/","section":"publication","summary":"In this paper, we propose a simple attention mechanism, we call box-attention. It enables spatial interaction between grid features, as sampled from boxes of interest, and improves the learning capability of transformers for several vision tasks. Specifically, we present BoxeR, short for Box Transformer, which attends to a set of boxes by predicting their transformation from a reference window on an input feature map. The BoxeR computes attention weights on these boxes by considering its grid structure. Notably, BoxeR-2D naturally reasons about box information within its attention module, making it suitable for end-to-end instance detection and segmentation tasks. By learning invariance to rotation in the box-attention module, BoxeR-3D is capable of generating discriminative information from a bird's-eye view plane for 3D end-to-end object detection. Our experiments demonstrate that the proposed BoxeR-2D achieves state-of-the-art results on COCO detection and instance segmentation. Besides, BoxeR-3D improves over the end-to-end 3D object detection baseline and already obtains a compelling performance for the vehicle category of Waymo Open, without any class-specific optimization.","tags":["Recognition (object detection, categorization)"],"title":"BoxeR: Box-Attention for 2D and 3D Transformers","type":"publication"},{"authors":["Jie Liu","Yanqi Bao","Guosen Xie","Huan Xiong","Jan-Jakob Sonke","Efstratios Gavves"],"categories":null,"content":"","date":1652054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652054400,"objectID":"93c9672140d427b4f750539c269a22be","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/jieliu-cvpr-2022/","publishdate":"2022-05-09T00:00:00Z","relpermalink":"/vislab/publication/jieliu-cvpr-2022/","section":"publication","summary":"The key challenge for few-shot semantic segmentation (FSS) is how to tailor a desirable interaction among sup- port and query features and/or their prototypes, under the episodic training scenario. Most existing FSS methods im- plement such support/query interactions by solely leverag- ing plain operations – e.g., cosine similarity and feature concatenation – for segmenting the query objects. How- ever, these interaction approaches usually cannot well cap- ture the intrinsic object details in the query images that are widely encountered in FSS, e.g., if the query object to be segmented has holes and slots, inaccurate segmentation al- most always happens. To this end, we propose a dynamic prototype convolution network (DPCN) to fully capture the aforementioned intrinsic details for accurate FSS. Specifi- cally, in DPCN, a dynamic convolution module (DCM) is firstly proposed to generate dynamic kernels from support foreground, then information interaction is achieved by con- volution operations over query features using these kernels. Moreover, we equip DPCN with a support activation mod- ule (SAM) and a feature filtering module (FFM) to generate pseudo mask and filter out background information for the query images, respectively. SAM and FFM together can mine enriched context information from the query features. Our DPCN is also flexible and efficient under the k-shot FSS setting. Extensive experiments on PASCAL-5i and COCO- 20i show that DPCN yields superior performances under both 1-shot and 5-shot settings.","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"Dynamic Prototype Convolution Network for Few-shot Semantic Segmentation","type":"publication"},{"authors":["Hazel Doughty","Cees G. M. Snoek"],"categories":null,"content":"","date":1652054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652054400,"objectID":"8d47a32abbfd8bfb3dce0ea6c4caf399","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/hazel-cvpr-2022-/","publishdate":"2022-05-09T00:00:00Z","relpermalink":"/vislab/publication/hazel-cvpr-2022-/","section":"publication","summary":"We aim to understand how actions are performed and identify subtle differences, such as ‘fold firmly’ vs. ‘fold gently’. To this end, we propose a method which recognizes adverbs across different actions. However, such fine-grained annotations are difficult to obtain and their long-tailed nature makes it challenging to recognize adverbs in rare action-adverb compositions. Our approach therefore uses semi-supervised learning with multiple adverb pseudo-labels to leverage videos with only action labels. Combined with adaptive thresholding of these pseudo-adverbs we are able to make efficient use of the available data while tackling the long-tailed distribution. Additionally, we gather adverb annotations for three existing video retrieval datasets, which allows us to introduce the new tasks of recognizing adverbs in unseen action-adverb compositions and unseen domains. Experiments demonstrate the effectiveness of our method, which outperforms prior work in recognizing adverbs and semi-supervised works adapted for adverb recognition. We also show how adverbs can relate fine-grained actions.","tags":["Video analysis and understanding"],"title":"How Do You Do It? Fine-Grained Action Understanding with Pseudo-Adverbs","type":"publication"},{"authors":["Mina Ghadimi Atigh","Julian Schoep","Erman Acar","Nanne van Noord","Pascal Mettes"],"categories":null,"content":"","date":1652054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652054400,"objectID":"ec984be6ab075651e8adcf335cd4826b","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mina-cvpr-2022/","publishdate":"2022-05-09T00:00:00Z","relpermalink":"/vislab/publication/mina-cvpr-2022/","section":"publication","summary":"For image segmentation, the current standard is to perform pixel-level optimization and inference in Euclidean output embedding spaces through linear hyperplanes. In this work, we show that hyperbolic manifolds provide a valuable alternative for image segmentation and propose a tractable formulation of hierarchical pixel-level classification in hyperbolic space. Hyperbolic Image Segmentation opens up new possibilities and practical benefits for segmentation, such as uncertainty estimation and boundary information for free, zero-label generalization, and increased performance in low-dimensional output embeddings.","tags":["Image segmentation"],"title":"Hyperbolic Image Segmentation","type":"publication"},{"authors":["Haochen Wang","Jiayi Shen","Yongtuo Liu","Yan Gao","Efstratios Gavves"],"categories":null,"content":"","date":1652054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652054400,"objectID":"209c369cbaafe15b9fc94da39a6d1bc1","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/haochen-cvpr-2022/","publishdate":"2022-05-09T00:00:00Z","relpermalink":"/vislab/publication/haochen-cvpr-2022/","section":"publication","summary":"Person re-identification aims to retrieve persons in highly varying settings across different cameras and scenarios, in which robust and discriminative representation learning is crucial. Most research considers learning representations from single images, ignoring any potential interactions between them. However, due to the high intra-identity variations, ignoring such interactions typically leads to outlier features. To tackle this issue, we propose a Neighbor Transformer Network, or NFormer, which explicitly models interactions across all input images, thus suppressing outlier features and leading to more robust representations overall. As modelling interactions between enormous amount of images is a massive task with lots of distractors, NFormer introduces two novel modules, the Landmark Agent Attention, and the Reciprocal Neighbor Softmax. Specifically, the Landmark Agent Attention efficiently models the relation map between images by a low-rank factorization with a few landmarks in feature space. Moreover, the Reciprocal Neighbor Softmax achieves sparse attention to relevant -- rather than all -- neighbors only, which alleviates interference of irrelevant representations and further relieves the computational burden. In experiments on four large-scale datasets, NFormer achieves a new state-of-the-art. The code is released at https://github.com/haochenheheda/NFormer.","tags":["Image retrieval"],"title":"NFormer: Robust Person Re-identification with Neighbor Transformer","type":"publication"},{"authors":["Adrian Ziegler","Yuki M. Asano"],"categories":null,"content":"","date":1652054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652054400,"objectID":"92e7b19b05c5c58b11765a43ceee6389","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/adrian-cvpr-2022/","publishdate":"2022-05-09T00:00:00Z","relpermalink":"/vislab/publication/adrian-cvpr-2022/","section":"publication","summary":"Progress in self-supervised learning has brought strong general image representation learning methods. Yet so far, it has mostly focused on image-level learning. In turn, tasks such as unsupervised image segmentation have not benefited from this trend as they require spatially-diverse representations. However, learning dense representations is challenging, as in the unsupervised context it is not clear how to guide the model to learn representations that correspond to various potential object categories. In this paper, we argue that self-supervised learning of object parts is a solution to this issue. Object parts are generalizable: they are a priori independent of an object definition, but can be grouped to form objects a posteriori. To this end, we leverage the recently proposed Vision Transformer's capability of attending to objects and combine it with a spatially dense clustering task for fine-tuning the spatial tokens. Our method surpasses the state-of-the-art on three semantic segmentation benchmarks by 17%-3%, showing that our representations are versatile under various object definitions. Finally, we extend this to fully unsupervised segmentation - which refrains completely from using label information even at test-time - and demonstrate that a simple method for automatically merging discovered object parts based on community detection yields substantial gains.","tags":["Self-supervised learning"],"title":"Self-Supervised Learning of Object Parts for Semantic Segmentation","type":"publication"},{"authors":["Triantafyllos Afouras","Yuki M. Asano","Francois Fagan","Andrea Vedaldi","Florian Metze"],"categories":null,"content":"","date":1652054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652054400,"objectID":"e9b72a68f3a64966a60eaaa8fac10da5","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yuki-cvpr-2022/","publishdate":"2022-05-09T00:00:00Z","relpermalink":"/vislab/publication/yuki-cvpr-2022/","section":"publication","summary":"We tackle the problem of learning object detectors without supervision. Differently from weakly-supervised object detection, we do not assume image-level class labels. Instead, we extract a supervisory signal from audio-visual data, using the audio component to 'teach' the object detector. While this problem is related to sound source localisation, it is considerably harder because the detector must classify the objects by type, enumerate each instance of the object, and do so even when the object is silent. We tackle this problem by first designing a self-supervised framework with a contrastive objective that jointly learns to classify and localise objects. Then, without using any supervision, we simply use these self-supervised labels and boxes to train an image-based object detector. With this, we outperform previous unsupervised and weakly-supervised detectors for the task of object detection and sound source localization. We also show that we can align this detector to ground-truth classes with as little as one label per pseudo-class, and show how our method can learn to detect generic objects that go beyond instruments, such as airplanes and cats.","tags":["Multi-modal learning"],"title":"Self-supervised object detection from audio-visual correspondence","type":"publication"},{"authors":["Jiaojiao Zhao","Yanyi Zhang","Xinyu Li","Hao Chen","Shuai Bing","Mingze Xu","Chunhui Liu","Kaustav Kundu","Yuanjun Xiong","Davide Modolo","Ivan Marsic","Cees G.M. Snoek","Joseph Tighe"],"categories":null,"content":"","date":1652054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652054400,"objectID":"ae5abbbb2b401d828cf0a1c2e9c65442","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/jiaojiao-cvpr-2022/","publishdate":"2022-05-09T00:00:00Z","relpermalink":"/vislab/publication/jiaojiao-cvpr-2022/","section":"publication","summary":"We propose TubeR: a simple solution for spatio-temporal video action detection. Different from existing methods that depend on either an off-line actor detector or hand-designed actor-positional hypotheses like proposals or anchors, we propose to directly detect an action tubelet in a video by simultaneously performing action localization and recognition from a single representation. TubeR learns a set of tubelet-queries and utilizes a tubelet-attention module to model the dynamic spatio-temporal nature of a video clip, which effectively reinforces the model capacity compared to using actor-positional hypotheses in the spatio-temporal space. For videos containing transitional states or scene changes, we propose a context aware classification head to utilize short-term and long-term context to strengthen action classification, and an action switch regression head for detecting the precise temporal action extent. TubeR directly produces action tubelets with variable lengths and even maintains good results for long video clips. TubeR outperforms the previous state-of-the-art on commonly used action detection datasets AVA, UCF101-24 and JHMDB51-21.","tags":["Action and behavior recognition"],"title":"TubeR: Tubelet Transformer for Video Action Detection","type":"publication"},{"authors":["Phillip Lippe","Taco Cohen","Efstratios Gavves"],"categories":null,"content":"","date":1649894400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649894400,"objectID":"a7aff6b7b2a3b2a9fe64edd0b7563c78","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/phillip-iclr-2022/","publishdate":"2022-04-14T00:00:00Z","relpermalink":"/vislab/publication/phillip-iclr-2022/","section":"publication","summary":"Learning the structure of a causal graphical model using both observational and interventional data is a fundamental problem in many scientific fields. A promising direction is continuous optimization for score-based methods, which efficiently learns the causal graph in a data-driven manner. However, to date, those methods require constrained optimization to enforce acyclicity or lack convergence guarantees. In this paper, we present ENCO, an efficient structure learning method for directed, acyclic causal graph leveraging observational and interventional data. ENCO formulates the graph search as an optimization of independent edge likelihoods, with the edge orientation being modeled as a separate parameter. Consequently, we can provide convergence guarantees of ENCO under mild conditions without constraining the score function with respect to acyclicity. In experiments, we show that ENCO can efficiently recover graphs with hundreds of nodes, an order of magnitude larger than what was previously possible.","tags":["Causal Discovery"],"title":"Efficient Neural Causal Discovery without Acyclicity Constraints","type":"publication"},{"authors":["Yingjun Du","Xiantong Zhen","Ling Shao","Cees G. M. Snoek"],"categories":null,"content":"","date":1649894400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649894400,"objectID":"26af69b5dbebe26b17409d43fa415a89","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yingjun-iclr-2022/","publishdate":"2022-04-14T00:00:00Z","relpermalink":"/vislab/publication/yingjun-iclr-2022/","section":"publication","summary":"Neural memory enables fast adaptation to new tasks with just a few training samples. Existing memory models store features only from the single last layer, which does not generalize well in presence of a domain shift between training and test distributions. Rather than relying on a flat memory, we propose a hierarchical alternative that stores features at different semantic levels. We introduce a hierarchical prototype model, where each level of the prototype fetches corresponding information from the hierarchical memory. The model is endowed with the ability to flexibly rely on features at different semantic levels if the domain shift circumstances so demand. We meta-learn the model by a newly derived hierarchical variational inference framework, where hierarchical memory and prototypes are jointly optimized. To explore and exploit the importance of different semantic levels, we further propose to learn the weights associated with the prototype at each level in a data-driven way, which enables the model to adaptively choose the most generalizable features. We conduct thorough ablation studies to demonstrate the effectiveness of each component in our model. The new state-of-the-art performance on cross-domain and competitive performance on traditional few-shot classification further substantiates the benefit of hierarchical variational memory.","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"Hierarchical Variational Memory for Few-shot Learning Across Domains","type":"publication"},{"authors":["Zehao Xiao","Xiantong Zhen","Ling Shao","Cees G. M. Snoek"],"categories":null,"content":"","date":1649894400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649894400,"objectID":"d9436e159a02e10014d52ae1fdc6d474","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/zehao-iclr-2022/","publishdate":"2022-04-14T00:00:00Z","relpermalink":"/vislab/publication/zehao-iclr-2022/","section":"publication","summary":"We strive to learn a model from a set of source domains that generalizes well to unseen target domains. The main challenge in such a domain generalization scenario is the unavailability of any target domain data during training, resulting in the learned model not being explicitly adapted to the unseen target domains. We propose learning to generalize across domains on single test samples. We leverage a meta-learning paradigm to learn our model to acquire the ability of adaptation with single samples at training time so as to further adapt itself to each single test sample at test time. We formulate the adaptation to the single test sample as a variational Bayesian inference problem, which incorporates the test sample as a conditional into the generation of model parameters. The adaptation to each test sample requires only one feed-forward computation at test time without any fine-tuning or self-supervised training on additional data from the unseen domains. Extensive ablation studies demonstrate that our model learns the ability to adapt models to each single sample by mimicking domain shifts during training. Further, our model achieves at least comparable -- and often better -- performance than state-of-the-art methods on multiple benchmarks for domain generalization.","tags":["Domain generalization"],"title":"Learning to Generalize across Domains on Single Test Samples","type":"publication"},{"authors":["Iro Laina","Yuki M. Asano","Andrea Vedaldi"],"categories":null,"content":"","date":1649894400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649894400,"objectID":"344189d9c4cdb060cc1f6ecdbef8cc82","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yuki-iclr-2022/","publishdate":"2022-04-14T00:00:00Z","relpermalink":"/vislab/publication/yuki-iclr-2022/","section":"publication","summary":"Self-supervised visual representation learning has recently attracted significant research interest. While a common way to evaluate self-supervised representations is through transfer to various downstream tasks, we instead investigate the problem of measuring their interpretability, i.e. understanding the semantics encoded in raw representations. We formulate the latter as estimating the mutual information between the representation and a space of manually labelled concepts. To quantify this we introduce a decoding bottleneck: information must be captured by simple predictors, mapping concepts to clusters in representation space. This approach, which we call reverse linear probing, provides a single number sensitive to the semanticity of the representation. This measure is also able to detect when the representation contains combinations of concepts (e.g., \"red apple') instead of just individual attributes (\"red' and \"apple' independently). Finally, we propose to use supervised classifiers to automatically label large datasets in order to enrich the space of concepts used for probing. We use our method to evaluate a large number of self-supervised representations, ranking them by interpretability, highlight the differences that emerge compared to the standard evaluation with linear probes and discuss several qualitative insights. Code at: https://github.com/iro-cp/ssl-qrp.","tags":["Self-supervised learning"],"title":"Measuring the Interpretability of Unsupervised Representations via Quantized Reversed Probing","type":"publication"},{"authors":["Yan Zhang","David W. Zhang","Simon Lacoste-Julien","Gertjan J. Burghouts","Cees G. M. Snoek"],"categories":null,"content":"","date":1649894400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649894400,"objectID":"17a165d818342dc51f0c8f6fdb6cecfc","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/david-iclr-2022/","publishdate":"2022-04-14T00:00:00Z","relpermalink":"/vislab/publication/david-iclr-2022/","section":"publication","summary":"Most set prediction models in deep learning use set-equivariant operations, but they actually operate on multisets. We show that set-equivariant functions cannot represent certain functions on multisets, so we introduce the more appropriate notion of multiset-equivariance. We identify that the existing Deep Set Prediction Network (DSPN) can be multiset-equivariant without being hindered by set-equivariance and improve it with approximate implicit differentiation, allowing for better optimization while being faster and saving memory. In a range of toy experiments, we show that the perspective of multiset-equivariance is beneficial and that our changes to DSPN achieve better results in most cases. On CLEVR object property prediction, we substantially improve over the state-of-the-art Slot Attention from 8% to 77% in one of the strictest evaluation metrics because of the benefits made possible by implicit differentiation.","tags":["Set prediction","Object detection","Permutation equivariance"],"title":"Multiset-Equivariant Set Prediction with Approximate Implicit Differentiation","type":"publication"},{"authors":["Adeel Pervez","Efstratios Gavves"],"categories":null,"content":"","date":1649894400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649894400,"objectID":"16cfeaf45675de32e6dbf903ed94cf05","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/adeel-iclr-2022/","publishdate":"2022-04-14T00:00:00Z","relpermalink":"/vislab/publication/adeel-iclr-2022/","section":"publication","summary":"We present a method for training neural network models with discrete stochastic variables. The core of the method is stability regularization, which is a regularization procedure based on the idea of noise stability developed in Gaussian isoperimetric theory in the analysis of Gaussian functions. Stability regularization is a method to make the output of continuous functions of Gaussian random variables close to discrete, that is binary or categorical, without the need for significant manual tuning.  The method can be used standalone or in combination with existing continuous relaxation methods. We validate the method in a broad range of settings, showing competitive performance against the state-of-the-art.","tags":["Discrete Representations"],"title":"Stability Regularization for Discrete Representation Learning","type":"publication"},{"authors":["Anna Langedijk","Verna Dankers","Phillip Lippe","Sander Bos","Bryan Cardenas Guevara","Helen Yannakoudakis","Ekaterina Shutova"],"categories":null,"content":"","date":1649721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649721600,"objectID":"de5051df4a07eb39bf79e216f8dce26e","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/phillip-acl-2022-/","publishdate":"2022-04-12T00:00:00Z","relpermalink":"/vislab/publication/phillip-acl-2022-/","section":"publication","summary":"Meta-learning, or learning to learn, is a technique that can help to overcome resource scarcity in cross-lingual NLP problems, by enabling fast adaptation to new tasks. We apply model-agnostic meta-learning (MAML) to the task of cross-lingual dependency parsing. We train our model on a diverse set of languages to learn a parameter initialization that can adapt quickly to new languages. We find that meta-learning with pre-training can significantly improve upon the performance of language transfer and standard supervised learning baselines for a variety of unseen, typologically diverse, and low-resource languages, in a few-shot learning setup.","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"Meta-learning for fast cross-lingual adaptation in dependency parsing","type":"publication"},{"authors":["Zenglin Shi","Pascal Mettes","Subhransu Maji","Cees G. M. Snoek"],"categories":[],"content":"","date":1641168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641168000,"objectID":"b094264f96439d4e915800ef9055ac1c","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/zenglin-ijcv-2022/","publishdate":"2022-01-03T00:00:00Z","relpermalink":"/vislab/publication/zenglin-ijcv-2022/","section":"publication","summary":"The deep image prior showed that a randomly initialized network with a suitable architecture can be trained to solve inverse imaging problems by simply optimizing it's parameters to reconstruct a single degraded image. However, it suffers from two practical limitations. First, it remains unclear how to control the prior beyond the choice of the network architecture. Second, training requires an oracle stopping criterion as during the optimization the performance degrades after reaching an optimum value. To address these challenges we introduce a frequency-band correspondence measure to characterize the spectral bias of the deep image prior, where low-frequency image signals are learned faster and better than high-frequency counterparts. Based on our observations, we propose techniques to prevent the eventual performance degradation and accelerate convergence. We introduce a Lipschitz-controlled convolution layer and a Gaussian-controlled upsampling layer as plug-in replacements for layers used in the deep architectures. The experiments show that with these changes the performance does not degrade during optimization, relieving us from the need for an oracle stopping criterion. We further outline a stopping criterion to avoid superfluous computation. Finally, we show that our approach obtains favorable results compared to current approaches across various denoising, deblocking, inpainting, super-resolution and detail enhancement tasks.","tags":["Generative model"],"title":"On Measuring and Controlling the Spectral Bias of the Deep Image Prior","type":"publication"},{"authors":["Iris Groen","Tessa Dekker","Tomas Knapen","Edward Silson"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"a18184b1916101712be8411624ee65c8","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/iris-tcs-2022/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/vislab/publication/iris-tcs-2022/","section":"publication","summary":"For more than 100 years we have known that the visual field is mapped onto the surface of visual cortex, imposing an inherently spatial reference frame on visual information processing. Recent studies highlight visuospatial coding not only throughout visual cortex, but also brain areas not typically considered visual. Such widespread access to visuospatial coding raises important questions about its role in wider cognitive functioning. Here, we synthesise these recent developments and propose that visuospatial coding scaffolds human cognition by providing a reference frame through which neural computations interface with environmental statistics and task demands via perception–action loops.","tags":["Vision in the human brain"],"title":"Visuospatial coding as ubiquitous scaffolding for human cognition","type":"publication"},{"authors":["Jing Li","Qingwang Huang","Yingjun Du","Xiantong Zhen","Shengyong Chen","Ling Shao"],"categories":[],"content":"","date":1639699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639699200,"objectID":"a982aec0243640dbcf12f4e66a33a625","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/jing-tip-2021/","publishdate":"2021-12-17T00:00:00Z","relpermalink":"/vislab/publication/jing-tip-2021/","section":"publication","summary":"Abnormal crowd behavior detection has recently attracted increasing attention due to its wide applications in computer vision research areas. However, it is still an extremely challenging task due to the great variability of abnormal behavior coupled with huge ambiguity and uncertainty of video contents. To tackle these challenges, we propose a new probabilistic framework named variational abnormal behavior detection (VABD), which can detect abnormal crowd behavior in video sequences. We make three major contributions: (1) We develop a new probabilistic latent variable model that combines the strengths of the U-Net and conditional variational auto-encoder, which also are the backbone of our model; (2) We propose a motion loss based on an optical flow network to impose the motion consistency of generated video frames and input video frames; (3) We embed a Wasserstein generative adversarial network at the end of the backbone network to enhance the framework performance. VABD can accurately discriminate abnormal video frames from video sequences. Experimental results on UCSD, CUHK Avenue, IITB-Corridor, and ShanghaiTech datasets show that VABD outperforms the state-of-the-art algorithms on abnormal crowd behavior detection. Without data augmentation, our VABD achieves 72.24% in terms of AUC on IITB-Corridor, which surpasses the state-of-the-art methods by nearly 5%.","tags":["Video analysis and understanding"],"title":"Variational Abnormal Behavior Detection with Motion Consistency","type":"publication"},{"authors":["Sander R Klomp","Matthew van Rijn","Rob G J Wijnhoven","Cees G M Snoek","Peter H N de With"],"categories":[],"content":"","date":1639526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639526400,"objectID":"269922343881e9fe0b9ee80febe8b9ca","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/sander-icafgr-2021/","publishdate":"2021-12-15T00:00:00Z","relpermalink":"/vislab/publication/sander-icafgr-2021/","section":"publication","summary":"Since the introduction of the GDPR and CCPA privacy legislation, both public and private facial image datasets are increasingly scrutinized. Several datasets have been taken offline completely and some have been anonymized. However, it is unclear how anonymization impacts face detection performance. To our knowledge, this paper presents the first empirical study on the effect of image anonymization on supervised training of face detectors. We compare conventional face anonymizers with three state-of-the-art Generative Adversarial Network-based (GAN) methods, by training an off-the-shelf face detector on anonymized data. Our experiments investigate the suitability of anonymization methods for maintaining face detector performance, the effect of detectors overtraining on anonymization artefacts, dataset size for training an anonymizer, and the effect of training time of anonymization GANs. A final experiment investigates the correlation between common GAN evaluation metrics and the performance of a trained face detector. Although all tested anonymization methods lower the performance of trained face detectors, faces anonymized using GANs cause far smaller performance degradation than conventional methods. As the most important finding, the best-performing GAN, DeepPrivacy, removes identifiable faces for a face detector trained on anonymized data, resulting in a modest decrease from 91.0 to 88.3 mAP.  In the last few years, there have been rapid improvements in realism of GAN-generated faces. We expect that further progression in GAN research will allow the use of Deep Fake technology for privacy-preserving Safe Fakes, without any performance degradation for training face detectors.","tags":["Privacy protection"],"title":"Safe Fakes: Evaluating Face Anonymizers for Face Detectors","type":"publication"},{"authors":["Yoni Schirris*","Mendel Engelaer*","Andreas Pantelia","Hugo Mark Horlings","Efstratios Gavves","Jonas Teuwen"],"categories":[],"content":"","date":1639526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639526400,"objectID":"f1d582f3f5f4f7ca16beed27202668fa","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yoni-spie_mi-2021/","publishdate":"2021-12-15T00:00:00Z","relpermalink":"/vislab/publication/yoni-spie_mi-2021/","section":"publication","summary":"We present WeakSTIL, an interpretable two-stage weak label deep learning pipeline for scor- ing the percentage of stromal tumor infiltrating lymphocytes (sTIL%) in H\u0026E-stained whole-slide images (WSIs) of breast cancer tissue. The sTIL% score is a prognostic and predictive biomarker for many solid tumor types. However, due to the high labeling efforts and high intra- and inter-observer variability within and between expert annotators, this biomarker is currently not used in routine clinical decision making. WeakSTIL compresses tiles of a WSI using a feature extractor pre-trained with self-supervised learning on unlabeled histopathology data and learns to predict precise sTIL% scores for each tile in the tumor bed by using a multiple instance learning regressor that only requires a weak WSI-level label. By requiring only a weak label, we overcome the large annotation efforts required to train currently existing TIL detection methods. We show that Weak-STIL is at least as good as other TIL detection methods when predicting the WSI-level sTIL% score, reaching a coefficient of determination of 0.45 ±0.15 when compared to scores generated by an expert pathologist, and an AUC of 0.89 ±0.05 when treating it as the clinically interesting sTIL-high vs sTIL-low classification task. Additionally, we show that the intermediate tile-level predictions of WeakSTIL are highly interpretable, which suggests that WeakSTIL pays attention to latent features related to the number of TILs and the tissue type. In the future, WeakSTIL may be used to provide consistent and interpretable sTIL% predictions to stratify breast cancer patients into targeted therapy arms.","tags":["AI for Oncology"],"title":"WeakSTIL: Weak whole-slide image level stromal tumor infiltrating lymphocyte scores are all you need","type":"publication"},{"authors":["Hannah Kirk","Yennie Jun","Haider Iqbal","Elias Benussi","Filippo Volpin","Frederic A. Dreyer","Aleksandar Shtedritski","Yuki M. Asano"],"categories":[],"content":"","date":1638748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638748800,"objectID":"bef564c2c38b2a9cf5646f62df3c242b","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/hannah-neurips-2021/","publishdate":"2021-12-06T00:00:00Z","relpermalink":"/vislab/publication/hannah-neurips-2021/","section":"publication","summary":"The capabilities of natural language models trained on large-scale data have increased immensely over the past few years. Open source libraries such as HuggingFace have made these models easily available and accessible. While prior research has identified biases in large language models, this paper considers biases contained in the most popular versions of these models when applied `out-of-the-box' for downstream tasks. We focus on generative language models as they are well-suited for extracting biases inherited from training data. Specifically, we conduct an in-depth analysis of GPT-2, which is the most downloaded text generation model on HuggingFace, with over half a million downloads per month. We assess biases related to occupational associations for different protected categories by intersecting gender with religion, sexuality, ethnicity, political affiliation, and continental name origin. Using a template-based data collection pipeline, we collect 396K sentence completions made by GPT-2 and find: (i) The machine-predicted jobs are less diverse and more stereotypical for women than for men, especially for intersections; (ii) Intersectional interactions are highly relevant for occupational associations, which we quantify by fitting 262 logistic models; (iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity distribution found in US Labor Bureau data, and even pulls the societally-skewed distribution towards gender parity in cases where its predictions deviate from real labor market observations. This raises the normative question of what language models should learn - whether they should reflect or correct for existing inequalities.","tags":["Bias and Ethical AI"],"title":"Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models","type":"publication"},{"authors":["Mina Ghadimi Atigh","Martin Keller-Ressel","Pascal Mettes"],"categories":[],"content":"","date":1638748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638748800,"objectID":"11e120d164dca448557eb37bcf85f33a","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mina-neurips-2021/","publishdate":"2021-12-06T00:00:00Z","relpermalink":"/vislab/publication/mina-neurips-2021/","section":"publication","summary":"Hyperbolic space has become a popular choice of manifold for representation learning of various datatypes from tree-like structures and text to graphs. Building on the success of deep learning with prototypes in Euclidean and hyperspherical spaces, a few recent works have proposed hyperbolic prototypes for classification. Such approaches enable effective learning in low-dimensional output spaces and can exploit hierarchical relations amongst classes, but require privileged information about class labels to position the hyperbolic prototypes. In this work, we propose Hyperbolic Busemann Learning. The main idea behind our approach is to position prototypes on the ideal boundary of the Poincare ball, which does not require prior label knowledge. To be able to compute proximities to ideal prototypes, we introduce the penalised Busemann loss. We provide theory supporting the use of ideal prototypes and the proposed loss by proving its equivalence to logistic regression in the one-dimensional case. Empirically, we show that our approach provides a natural interpretation of classification confidence, while outperforming recent hyperspherical and hyperbolic prototype approaches.","tags":["prototype learning"],"title":"Hyperbolic Busemann Learning with Ideal Prototypes","type":"publication"},{"authors":["Mandela Patrick*","Dylan Campbell*","Yuki M. Asano*","Ishan Misra","Florian Metze","Christoph Feichtenhofer","Andrea Vedaldi","João F. Henriques"],"categories":[],"content":"","date":1638748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638748800,"objectID":"28c5f0d119c1bf6655fc17e0a1c1e72e","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mandela-neurips-2021/","publishdate":"2021-12-06T00:00:00Z","relpermalink":"/vislab/publication/mandela-neurips-2021/","section":"publication","summary":"In video transformers, the time dimension is often treated in the same way as the two spatial dimensions. However, in a scene where objects or the camera may move, a physical point imaged at one location in frame t may be entirely unrelated to what is found at that location in frame t+k. These temporal correspondences should be modeled to facilitate learning about dynamic scenes. To this end, we propose a new drop-in block for video transformers -- trajectory attention -- that aggregates information along implicitly determined motion paths. We additionally propose a new method to address the quadratic dependence of computation and memory on the input size, which is particularly important for high resolution or long videos. While these ideas are useful in a range of settings, we apply them to the specific task of video action recognition with a transformer model and obtain state-of-the-art results on the Kinetics, Something--Something V2, and Epic-Kitchens datasets.","tags":["Motion and tracking"],"title":"Keeping Your Eye On the Ball: Trajectory Attention in Video Transformers","type":"publication"},{"authors":["Yuki M. Asano","Christian Rupprecht","Andrew Zisserman","Andrea Vedaldi"],"categories":[],"content":"","date":1638748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638748800,"objectID":"953b8c65d024d7b2b2ca32808a898f61","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yuki-ds-2021/","publishdate":"2021-12-06T00:00:00Z","relpermalink":"/vislab/publication/yuki-ds-2021/","section":"publication","summary":"Computer vision has long relied on ImageNet and other large datasets of images sampled from the Internet for pretraining models. However, these datasets have ethical and technical shortcomings, such as containing personal information taken without consent, unclear license usage, biases, and, in some cases, even problematic image content. On the other hand, state-of-the-art pretraining is nowadays obtained with unsupervised methods, meaning that labelled datasets such as ImageNet may not be necessary, or perhaps not even optimal, for model pretraining. We thus propose an unlabelled dataset PASS: Pictures without humAns for Self-Supervision. PASS only contains images with CC-BY license and complete attribution metadata, addressing the copyright issue. Most importantly, it contains no images of people at all, and also avoids other types of images that are problematic for data protection or ethics. We show that PASS can be used for pretraining with methods such as MoCo-v2, SwAV and DINO. In the transfer learning setting, it yields similar downstream performances to ImageNet pretraining even on tasks that involve humans, such as human pose estimation. PASS does not make existing datasets obsolete, as for instance it is insufficient for benchmarking. However, it shows that model pretraining is often possible while using safer data, and it also provides the basis for a more robust evaluation of pretraining methods.","tags":["Self-supervised Learning","Privacy"],"title":"PASS: An ImageNet replacement for self-supervised pretraining without humans","type":"publication"},{"authors":["Miltiadis Kofinas","Naveen Shankar Nagaraja","Efstratios Gavves"],"categories":[],"content":"","date":1638748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638748800,"objectID":"c91f872373f4377085cfdc414f22f509","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/miltiadis-neurips-2021/","publishdate":"2021-12-06T00:00:00Z","relpermalink":"/vislab/publication/miltiadis-neurips-2021/","section":"publication","summary":"Modelling interactions is critical in learning complex dynamical systems, namely systems of interacting objects with highly non-linear and time-dependent behaviour. A large class of such systems can be formalized as geometric graphs, i.e., graphs with nodes positioned in the Euclidean space given an arbitrarily chosen global coordinate system, for instance vehicles in a traffic scene. Notwithstanding the arbitrary global coordinate system, the governing dynamics of the respective dynamical systems are invariant to rotations and translations, also known as Galilean invariance. As ignoring these invariances leads to worse generalization, in this work we propose local coordinate frames per node-object to induce roto-translation invariance to the geometric graph of the interacting dynamical system. Further, the local coordinate frames allow for a natural definition of anisotropic filtering in graph neural networks. Experiments in traffic scenes, 3D motion capture, and colliding particles demonstrate that the proposed approach comfortably outperforms the recent state-of-the-art.","tags":["Graph learning"],"title":"Roto-translated Local Coordinate Frames For Interacting Dynamical Systems","type":"publication"},{"authors":["Jiayi Shen","Xiantong Zhen","Marcel Worring","Ling Shao"],"categories":[],"content":"","date":1638748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638748800,"objectID":"236b5bf29330cda7f91ef084726bcb31","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/jiayi-neurips-2021/","publishdate":"2021-12-06T00:00:00Z","relpermalink":"/vislab/publication/jiayi-neurips-2021/","section":"publication","summary":"Multi-task learning aims to explore task relatedness to improve individual tasks, which is of particular significance in the challenging scenario that only limited data is available for each task. To tackle this challenge, we propose variational multi-task learning (VMTL), a general probabilistic inference framework for learning multiple related tasks. We cast multi-task learning as a variational Bayesian inference problem, in which task relatedness is explored in a unified manner by specifying priors. To incorporate shared knowledge into each task, we design the prior of a task to be a learnable mixture of the variational posteriors of other related tasks, which is learned by the Gumbel-Softmax technique. In contrast to previous methods, our VMTL can exploit task relatedness for both representations and classifiers in a principled way by jointly inferring their posteriors. This enables individual tasks to fully leverage inductive biases provided by related tasks, therefore improving the overall performance of all tasks. Experimental results demonstrate that the proposed VMTL is able to effectively tackle a variety of challenging multi-task learning settings with limited training data for both classification and regression. Our method consistently surpasses previous methods, including strong Bayesian approaches, and achieves state-of-the-art performance on five benchmark datasets.","tags":["Transfer, low-shot, semi- and un- supervised learning"],"title":"Variational Multi-Task Learning with Gumbel-Softmax Priors","type":"publication"},{"authors":["Shuo Chen","Pascal Mettes","Cees G.M. Snoek"],"categories":[],"content":"","date":1637539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637539200,"objectID":"5507a0eb4380af51f0d07276d2d324b2","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/shuo-bmvc-2021/","publishdate":"2021-11-22T00:00:00Z","relpermalink":"/vislab/publication/shuo-bmvc-2021/","section":"publication","summary":"Video relation detection forms a new and challenging problem in computer vision, where subjects and objects need to be localized spatio-temporally and a predicate label needs to be assigned if and only if there is an interaction between the two. Despite recent progress in video relation detection, overall performance is still marginal and it remains unclear what the key factors are towards solving the problem. Following examples set in the object detection and action localization literature, we perform a deep dive into the error diagnosis of current video relation detection approaches. We introduce a diagnostic tool for analyzing the sources of detection errors. Our tool evaluates and compares current approaches beyond the single scalar metric of mean Average Precision by defining different error types specific to video relation detection, used for false positive analyses. Moreover, we examine different factors of influence on the performance in a false negative analysis, including relation length, number of subject/object/predicate instances, and subject/object size. Finally, we present the effect on video relation performance when considering an oracle fix for each error type. On two video relation benchmarks, we show where current approaches excel and fall short, allowing us to pinpoint the most important future directions in the field. The tool is available at https://github.com/shanshuo/DiagnoseVRD.","tags":["Action and behavior recognition"],"title":"Diagnosing Errors in Video Relation Detectors","type":"publication"},{"authors":["Ivan Sosnovik","Artem Moskalev","Arnold Smeulders"],"categories":[],"content":"","date":1637539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637539200,"objectID":"bdd2860276b6bbcb62a6179c9d785c28","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/ivan-bmvc-2021/","publishdate":"2021-11-22T00:00:00Z","relpermalink":"/vislab/publication/ivan-bmvc-2021/","section":"publication","summary":"Scale is often seen as a given, disturbing factor in many vision tasks. When doing so it is one of the factors why we need more data during learning. In recent work scale equivariance was added to convolutional neural networks. It was shown to be effective for a range of tasks. We aim for accurate scale-equivariant convolutional neural networks (SE-CNNs) applicable for problems where high granularity of scale and small filter sizes are required. Current SE-CNNs rely on weight sharing and filter rescaling, the latter of which is accurate for integer scales only. To reach accurate scale equivariance, we derive general constraints under which scale-convolution remains equivariant to discrete rescaling. We find the exact solution for all cases where it exists, and compute the approximation for the rest. The discrete scale-convolution pays off, as demonstrated in a new state-of-the-art classification on MNIST-scale and improving the results on STL-10. With the same SE scheme, we also improve the computational effort of a scale-equivariant Siamese tracker on OTB-13.","tags":["Scale Symmetry"],"title":"DISCO: accurate Discrete Scale Convolutions","type":"publication"},{"authors":["William Thong","Cees G M Snoek"],"categories":[],"content":"","date":1637539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637539200,"objectID":"dc5974bff9a6338db16fc2fa2870e518","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/william-bmvc-2021/","publishdate":"2021-11-22T00:00:00Z","relpermalink":"/vislab/publication/william-bmvc-2021/","section":"publication","summary":"This paper strives to address image classifier bias, with a focus on both feature and label embedding spaces. Previous works have shown that spurious correlations from protected attributes, such as age, gender, or skin tone, can cause adverse decisions. To balance potential harms, there is a growing need to identify and mitigate image classifier bias. First, we identify in the feature space a bias direction. We compute class prototypes of each protected attribute value for every class, and reveal an existing subspace that captures the maximum variance of the bias. Second, we mitigate biases by mapping image inputs to label embedding spaces. Each value of the protected attribute has its projection head where classes are embedded through a latent vector representation rather than a common one-hot encoding. Once trained, we further reduce in the feature space the bias effect by removing its direction. Evaluation on biased image datasets, for multi-class, multi-label and binary classifications, shows the effectiveness of tackling both feature and label embedding spaces in improving the fairness of the classifier predictions, while preserving classification performance.","tags":["Image retrieval"],"title":"Feature and Label Embedding Spaces Matter in Addressing Image Classifier Bias","type":"publication"},{"authors":["Mert Kilickaya","Arnold Smeulders"],"categories":[],"content":"","date":1637539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637539200,"objectID":"aa159593e9b02360d862af9bbb121b23","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mert-bmvc-2021/","publishdate":"2021-11-22T00:00:00Z","relpermalink":"/vislab/publication/mert-bmvc-2021/","section":"publication","summary":"The goal of this paper is Human-object Interaction (HO-I) detection. HO-I detection aims to find interacting human-objects regions and classify their interaction from an image. Researchers obtain significant improvement in recent years by relying on strong HO-I alignment supervision from [5]. HO-I alignment supervision pairs humans with their interacted objects, and then aligns human-object pair(s) with their interaction categories. Since collecting such annotation is expensive, in this paper, we propose to detect HO-I without alignment supervision. We instead rely on image-level supervision that only enumerates existing interactions within the image without pointing where they happen. Our paper makes three contributions: i) We propose Align-Former, a visual-transformer based CNN that can detect HO-I with only image-level supervision. ii) Align-Former is equipped with HO-I align layer, that can learn to select appropriate targets to allow detector supervision. iii) We evaluate Align-Former on HICO-DET [5] and V-COCO [13], and show that Align-Former outperforms existing image-level supervised HO-I detectors by a large margin (4.71% mAP improvement from 16.14% to 20.85% on HICO-DET [5]).","tags":["Recognition (object detection, categorization)"],"title":"Human-Object Interaction Detection via Weak Supervision","type":"publication"},{"authors":["Shuo Chen","Tan Yu","Ping Li"],"categories":[],"content":"","date":1637539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637539200,"objectID":"88f5920482b3499d58804b632e965184","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/shuo-bmvc2-2021/","publishdate":"2021-11-22T00:00:00Z","relpermalink":"/vislab/publication/shuo-bmvc2-2021/","section":"publication","summary":"Inspired by the great success achieved by CNN in image recognition, view-based methods applied CNNs to model the projected views for 3D object understanding and achieved excellent performance. Nevertheless, multi-view CNN models cannot model the communications between patches from different views, limiting its effectiveness in 3D object recognition. Inspired by the recent success gained by vision Transformer in image recognition, we propose a Multi-view Vision Transformer (MVT) for 3D object recognition. Since each patch feature in a Transformer block has a global reception field, it naturally achieves communications between patches from different views. Meanwhile, it takes much less inductive bias compared with its CNN counterparts. Considering both effectiveness and efficiency, we develop a global-local structure for our MVT. Our experiments on two public benchmarks, ModelNet40 and ModelNet10, demonstrate the competitive performance of our MVT.","tags":["3D computer vision"],"title":"MVT: Multi-view Vision Transformer for 3D Object Recognition","type":"publication"},{"authors":["Carlo Bretti","Pascal Mettes"],"categories":[],"content":"","date":1637539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637539200,"objectID":"31d5079b6d0d4a72d1d1b5c9d12ec703","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/carlo-bmvc-2021/","publishdate":"2021-11-22T00:00:00Z","relpermalink":"/vislab/publication/carlo-bmvc-2021/","section":"publication","summary":"This paper investigates the problem of zero-shot action recognition, in the setting where no training videos with seen actions are available. For this challenging scenario, the current leading approach is to transfer knowledge from the image domain by recognizing objects in videos using pre-trained networks, followed by a semantic matching between objects and actions. Where objects provide a local view on the content in videos, in this work we also seek to include a global view of the scene in which actions occur. We find that scenes on their own are also capable of recognizing unseen actions, albeit more marginally than objects, and a direct combination of object-based and scene-based scores degrades the action recognition performance. To get the best out of objects and scenes, we propose to construct them as a Cartesian product of all possible compositions. We outline how to determine the likelihood of object-scene compositions in videos, as well as a semantic matching from object-scene compositions to actions that enforces diversity among the most relevant compositions for each action. While simple, our composition-based approach outperforms object-based approaches and even state-of-the-art zero-shot approaches that rely on large-scale video datasets with hundreds of seen actions for training and knowledge transfer.","tags":["Action and behavior recognition"],"title":"Zero-Shot Action Recognition from Diverse Object-Scene Compositions","type":"publication"},{"authors":["Edward Silson","Iris Groen","Chris Baker"],"categories":[],"content":"","date":1635811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635811200,"objectID":"2e24a626beb87c30a157603e2983118d","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/edward-bsf-2021/","publishdate":"2020-08-23T19:24:38.101105Z","relpermalink":"/vislab/publication/edward-bsf-2021/","section":"publication","summary":"Human visual cortex is organised broadly according to two major principles: retinotopy (the spatial mapping of the retina in cortex) and category-selectivity (preferential responses to specific categories of stimuli). Historically, these principles were considered anatomically separate, with retinotopy restricted to the occipital cortex and category-selectivity emerging in the lateral-occipital and ventral-temporal cortex. However, recent studies show that category-selective regions exhibit systematic retinotopic biases, for example exhibiting stronger activation for stimuli presented in the contra- compared to the ipsilateral visual field. It is unclear, however, whether responses within category-selective regions are more strongly driven by retinotopic location or by category preference, and if there are systematic differences between category-selective regions in the relative strengths of these preferences. Here, we directly compare contralateral and category preferences by measur- ing fMRI responses to scene and face stimuli presented in the left or right visual field and computing two bias indices: a contralateral bias (response to the contralateral minus ipsilateral visual field) and a face/scene bias (preferred response to scenes compared to faces, or vice versa). We compare these biases within and between scene- and face-selective regions and across the lateral and ventral surfaces of the visual cortex more broadly. We find an interaction between surface and bias: lateral surface regions show a stronger contralateral than face/scene bias, whilst ventral surface regions show the opposite. These effects are robust across and within subjects, and appear to reflect large-scale, smoothly varying gradients. Together, these findings support distinct functional roles for the lateral and ventral visual cortex in terms of the relative importance of the spatial location of stimuli during visual information processing.","tags":["Human Visual Perception"],"title":"Direct comparison of category and spatial selectivity in human occipitotemporal cortex","type":"publication"},{"authors":["Zheyun Qin","Xiankai Lu","Xiushan Nie","Xiantong Zhen","Yilong Yin"],"categories":[],"content":"","date":1634774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634774400,"objectID":"44be9954a1ca7c3d906b835ae802130c","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/zhequn-acmmm-2021/","publishdate":"2021-10-21T00:00:00Z","relpermalink":"/vislab/publication/zhequn-acmmm-2021/","section":"publication","summary":"In this paper, we address video instance segmentation using a new generative model that learns effective representations of the target and background appearance. We propose to exploit hierarchical structural embedding over spatio-temporal space, which is compact, powerful, and flexible in contrast to current tracking-by-detection methods. Specifically, our model segments and tracks instances across space and time in a single forward pass, which is formulated as hierarchical embedding learning. The model is trained to locate the pixels belonging to specific instances over a video clip. We firstly take advantage of a novel mixing function to better fuse spatiotemporal embeddings. Moreover, we introduce normalizing flows to further improve the robustness of the learned appearance embedding, which theoretically extends conventional generative flows to a factorized conditional scheme. Comprehensive experiments on the video instance segmentation benchmark, i.e., YouTube-VIS, demonstrate the effectiveness of the proposed approach. Furthermore, we evaluate our method on an unsupervised video object segmentation dataset to demonstrate its generalizability","tags":["Video analysis and understanding"],"title":"Learning Hierarchical Embedding for Video Instance Segmentation","type":"publication"},{"authors":["Fida Mohammad Thoker","Hazel Doughty","Cees G.M. Snoek"],"categories":[],"content":"","date":1634774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634774400,"objectID":"2f73358c8a1cdf3f9fe93d81cda9eedc","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/fida-acmmm-2021/","publishdate":"2021-10-21T00:00:00Z","relpermalink":"/vislab/publication/fida-acmmm-2021/","section":"publication","summary":"This paper strives for self-supervised learning of a feature space suitable for skeleton-based action recognition. Our proposal is built upon learning invariances to input skeleton representations and various skeleton augmentations via a noise contrastive estimation. In particular, we propose inter-skeleton contrastive learning, which learns from multiple different input skeleton representations in a cross-contrastive manner. In addition, we contribute several skeleton-specific spatial and temporal augmentations which further encourage the model to learn the spatio-temporal dynamics of skeleton data. By learning similarities between different skeleton representations as well as augmented views of the same sequence, the network is encouraged to learn higher-level semantics of the skeleton data than when only using the augmented views. Our approach achieves state-of-the-art performance for self-supervised learning from skeleton data on the challenging PKU and NTU datasets with multiple downstream tasks, including action recognition, action retrieval and semi-supervised learning. Code is available at https://github.com/fmthoker/skeleton-contrast.","tags":["Action and behavior recognition"],"title":"Skeleton-Contrastive 3D Action Representation Learning","type":"publication"},{"authors":["Dima Damen","Hazel Doughty","Giovanni Maria Farinella","Antonino Furnari","Evangelos Kazakos","Jian Ma","Davide Moltisanti","Jonathan Munro","Toby Perrett","Will Price","Michael Wray"],"categories":[],"content":"","date":1634688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634688000,"objectID":"e8336f78040872ba8526c0d763095668","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/dima-ijcv-2021/","publishdate":"2021-10-20T00:00:00Z","relpermalink":"/vislab/publication/dima-ijcv-2021/","section":"publication","summary":"This paper introduces the pipeline to extend the largest dataset in egocentric vision, EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (Damen in Scaling egocentric vision: ECCV, 2018), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection enables new challenges such as action detection and evaluating the “test of time”—i.e. whether models trained on data collected in 2018 can generalise to new footage collected two years later. The dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval (from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics.","tags":["Action and behavior recognition"],"title":"Rescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-KITCHENS-100","type":"publication"},{"authors":["Kirill Gavrilyuk","Mihir Jain","Ilia Karmanov","Cees G M Snoek"],"categories":[],"content":"","date":1633910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633910400,"objectID":"8c4c634d78d57ae8a00628cd4d8e0e05","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/kirill-iccv-2021/","publishdate":"2021-10-11T00:00:00Z","relpermalink":"/vislab/publication/kirill-iccv-2021/","section":"publication","summary":"The goal of this paper is to self-train a 3D convolutional neural network on an unlabeled video collection for deployment on small-scale video collections. As smaller video datasets benefit more from motion than appearance, we strive to train our network using optical flow, but avoid its computation during inference. We propose the first motion-augmented self-training regime, we call MotionFit. We start with supervised training of a motion model on a small, and labeled, video collection. With the motion model we generate pseudo-labels for a large unlabeled video collection, which enables us to transfer knowledge by learning to predict these pseudo-labels with an appearance model. Moreover, we introduce a multi-clip loss as a simple yet efficient way to improve the quality of the pseudo-labeling, even without additional auxiliary tasks. We also take into consideration the temporal granularity of videos during self-training of the appearance model, which was missed in previous works. As a result we obtain a strong motion-augmented representation model suited for video downstream tasks like action recognition and clip retrieval. On small-scale video datasets, MotionFit outperforms alternatives for knowledge transfer by 5%-8%, video-only self-supervision by 1%-7% and semi-supervised learning by 9%-18% using the same amount of class labels.","tags":["Video analysis and understanding"],"title":"Motion-Augmented Self-Training for Video Recognition at Smaller Scale","type":"publication"},{"authors":["Hongjun Chen","Jinbao Wang","Hong Cai Chen","Xiantong Zhen","Feng Zheng","Rongrong Ji","Ling Shao"],"categories":[],"content":"","date":1633910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633910400,"objectID":"814776a5758d0259672cdaf6b8f7e22e","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/hongjun-iccv-2021-/","publishdate":"2021-10-11T00:00:00Z","relpermalink":"/vislab/publication/hongjun-iccv-2021-/","section":"publication","summary":"Annotation burden has become one of the biggest barriers to semantic segmentation. Approaches based on click-level annotations have therefore attracted increasing attention due to their superior trade-off between supervision and annotation cost. In this paper, we propose seminar learning, a new learning paradigm for semantic segmentation with click-level supervision. The fundamental rationale of seminar learning is to leverage the knowledge from different networks to compensate for insufficient information provided in click-level annotations. Mimicking a seminar, our seminar learning involves a teacher-student and a student-student module, where a student can learn from both skillful teachers and other students. The teacher-student module uses a teacher network based on the exponential moving average to guide the training of the student network. In the student-student module, heterogeneous pseudo-labels are proposed to bridge the transfer of knowledge among students to enhance each other's performance. Experimental results demonstrate the effectiveness of seminar learning, which achieves the new state-of-the-art performance of 72.51%(mIOU), surpassing previous methods by a large margin of up to 16.88% on the Pascal VOC 2012 dataset.","tags":["Recognition (object detection, categorization)"],"title":"Seminar Learning for Click-Level Weakly Supervised Semantic Segmentation","type":"publication"},{"authors":["Shuo Chen","Zenglin Shi","Pascal Mettes","Cees G.M. Snoek"],"categories":[],"content":"","date":1633910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633910400,"objectID":"e4c3b7c692adf19896a399682b836362","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/shuo-iccv-2021/","publishdate":"2021-10-11T00:00:00Z","relpermalink":"/vislab/publication/shuo-iccv-2021/","section":"publication","summary":"This paper strives to classify and detect the relationship between object tubelets appearing within a video as a  triplet. Where existing works treat object proposals or tubelets as single entities and model their relations a posteriori, we propose to classify and detect predicates for pairs of object tubelets a priori. We also propose Social Fabric: an encoding that represents a pair of object tubelets as a composition of interaction primitives. These primitives are learned over all relations, resulting in a compact representation able to localize and classify relations from the pool of co-occurring object tubelets across all timespans in a video. The encoding enables our two-stage network. In the first stage, we train Social Fabric to suggest proposals that are likely interacting. We use the Social Fabric in the second stage to simultaneously fine-tune and predict predicate labels for the tubelets. Experiments demonstrate the benefit of early video relation modeling, our encoding and the two-stage architecture, leading to a new state-of-the-art on two benchmarks. We also show how the encoding enables query-by-primitive-example to search for spatio-temporal video relations. Code: https://github.com/shanshuo/Social-Fabric.","tags":["Action and behavior recognition"],"title":"Social Fabric: Tubelet Compositions for Video Relation Detection","type":"publication"},{"authors":["Mandela Patrick*","Yuki M. Asano*","Bernie Huang*","Ishan Misra","Florian Metze","João F. Henriques","Andrea Vedaldi"],"categories":[],"content":"","date":1633910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633910400,"objectID":"a14ed8079b32440eeb0b26f16e65b070","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mandela-iccv-2021-2/","publishdate":"2021-10-11T00:00:00Z","relpermalink":"/vislab/publication/mandela-iccv-2021-2/","section":"publication","summary":"The quality of the image representations obtained from self-supervised learning depends strongly on the type of data augmentations used in the learning formulation. Recent papers have ported these methods from still images to videos and found that leveraging both audio and video signals yields strong gains; however, they did not find that spatial augmentations such as cropping, which are very important for still images, work as well for videos. In this paper, we improve these formulations in two ways unique to the spatio-temporal aspect of videos. First, for space, we show that spatial augmentations such as cropping do work well for videos too, but that previous implementations, due to the high processing and memory cost, could not do this at a scale sufficient for it to work well. To address this issue, we first introduce Feature Crop, a method to simulate such augmentations much more efficiently directly in feature space. Second, we show that as opposed to naive average pooling, the use of transformer-based attention improves performance significantly, and is well suited for processing feature crops. Combining both of our discoveries into a new method, Space-Time Crop \u0026 Attend (STiCA) we achieve state-of-the-art performance across multiple video-representation learning benchmarks. In particular, we achieve new state-of-the-art accuracies of 67.0% on HMDB-51 and 93.1% on UCF-101 when pre-training on Kinetics-400.","tags":["Self-supervised learning"],"title":"Space-Time Crop \u0026 Attend: Improving Cross-modal Video Representation Learning","type":"publication"},{"authors":["Andreas Panteli","Jonas Teuwen","Hugo Horlings","Efstratios Gavves"],"categories":[],"content":"","date":1633910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633910400,"objectID":"dcf89d2a838d80a74c484ac7fc2ca36d","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/andreas-iccv-2021/","publishdate":"2021-10-11T00:00:00Z","relpermalink":"/vislab/publication/andreas-iccv-2021/","section":"publication","summary":"Object localisation, in the context of regular images, often depicts objects like people or cars. In these images, there is typically a relatively small number of objects per class, which usually is manageable to annotate. However, outside the setting of regular images, we are often confronted with a different situation. In computational pathology, digitised tissue sections are extremely large images, whose dimensions quickly exceed 250'000x250'000 pixels, where relevant objects, such as tumour cells or lymphocytes can quickly number in the millions. Annotating them all is practically impossible and annotating sparsely a few, out of many more, is the only possibility. Unfortunately, learning from sparse annotations, or sparse-shot learning, clashes with standard supervised learning because what is not annotated is treated as a negative. However, assigning negative labels to what are true positives leads to confusion in the gradients and biased learning. To this end, we present exclusive cross-entropy, which slows down the biased learning by examining the second-order loss derivatives in order to drop the loss terms corresponding to likely biased terms. Experiments on nine datasets and two different localisation tasks, detection with YOLLO and segmentation with Unet, show that we obtain considerable improvements compared to cross-entropy or focal loss, while often reaching the best possible performance for the model with only 10-40% of annotations. ","tags":["Recognition (object detection, categorization)"],"title":"Sparse-Shot Learning With Exclusive Cross-Entropy for Extremely Many Localisations","type":"publication"},{"authors":["Mandela Patrick*","Yuki M. Asano*","Polina Kuznetsova","Ruth Fong","João F. Henriques","Geoffrey Zweig","Andrea Vedaldi"],"categories":[],"content":"","date":1633478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633478400,"objectID":"d63a7202e6839cb533f27158d9813b88","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mandela-iccv-2021/","publishdate":"2021-10-06T00:00:00Z","relpermalink":"/vislab/publication/mandela-iccv-2021/","section":"publication","summary":"In the image domain, excellent representations can be learned by inducing invariance to content-preserving transformations via noise contrastive learning. In this paper, we generalize contrastive learning to a wider set of transformations, and their compositions, for which either invariance or distinctiveness is sought. We show that it is not immediately obvious how existing methods such as SimCLR can be extended to do so. Instead, we introduce a number of formal requirements that all contrastive formulations must satisfy, and propose a practical construction which satisfies these requirements. In order to maximise the reach of this analysis, we express all components of noise contrastive formulations as the choice of certain generalized transformations of the data (GDTs), including data sampling. We then consider videos as an example of data in which a large variety of transformations are applicable, accounting for the extra modalities -- for which we analyze audio and text -- and the dimension of time. We find that being invariant to certain transformations and distinctive to others is critical to learning effective video representations, improving the state-of-the-art for multiple benchmarks by a large margin, and even surpassing supervised pretraining.","tags":["Self-supervised learning"],"title":"On Compositions of Transformations in Contrastive Self-Supervised Learning","type":"publication"},{"authors":["Lei Zhang","Liyun Zuo","Yingjun Du","Xiantong Zhen"],"categories":[],"content":"","date":1631059200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631059200,"objectID":"5a6700c91c5109264fb3e558be679e32","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/lei-itcsvt-2021/","publishdate":"2021-09-08T00:00:00Z","relpermalink":"/vislab/publication/lei-itcsvt-2021/","section":"publication","summary":"Few-shot learning has recently generated increasing popularity in machine learning, which addresses the fundamental yet challenging problem of learning to adapt to new tasks with the limited data. In this paper, we propose a new probabilistic framework that learns to fast adapt with external memory. We model the classifier parameters as distributions that are inferred from the support set and directly applied to the query set for prediction. The model is optimized by formulating as a variational inference problem. The probabilistic modeling enables better handling prediction uncertainty due to the limited data. We impose a discriminative constraint on the feature representations by exploring the class structure, which can improve the classification performance. We further introduce a memory unit to store task-specific information extracted from the support set and used for the query set to achieve explicit adaption to individual tasks. By episodic training, the model learns to acquire the capability of adapting to specific tasks, which guarantees its performance on new related tasks. We conduct extensive experiments on widely-used benchmarks for few-shot recognition. Our method achieves new state-of-the-art performance and largely surpassing previous methods by large margins. The ablation study further demonstrates the effectiveness of the proposed discriminative learning and memory unit.","tags":["Transfer, low-shot, semi- and un- supervised learning"],"title":"Learning to Adapt with Memory for Probabilistic Few-Shot Learning","type":"publication"},{"authors":["Ce Li","Chunyu Xie","Baochang Zhang","Jungong Han","Xiantong Zhen","Jie Chen"],"categories":[],"content":"","date":1630540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630540800,"objectID":"423e2c75dcb7b76a8a4f367435026f28","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/ce-tnnls-2021/","publishdate":"2021-09-02T00:00:00Z","relpermalink":"/vislab/publication/ce-tnnls-2021/","section":"publication","summary":"Skeleton-based action recognition has been extensively studied, but it remains an unsolved problem because of the complex variations of skeleton joints in 3-D spatiotemporal space. To handle this issue, we propose a newly temporal-then-spatial recalibration method named memory attention networks (MANs) and deploy MANs using the temporal attention recalibration module (TARM) and spatiotemporal convolution module (STCM). In the TARM, a novel temporal attention mechanism is built based on residual learning to recalibrate frames of skeleton data temporally. In the STCM, the recalibrated sequence is transformed or encoded as the input of CNNs to further model the spatiotemporal information of skeleton sequence. Based on MANs, a new collaborative memory fusion module (CMFM) is proposed to further improve the efficiency, leading to the collaborative MANs (C-MANs), trained with two streams of base MANs. TARM, STCM, and CMFM form a single network seamlessly and enable the whole network to be trained in an end-to-end fashion. Comparing with the state-of-the-art methods, MANs and C-MANs improve the performance significantly and achieve the best results on six data sets for action recognition. The source code has been made publicly available at https://github.com/memory-attention-networks.","tags":["Recognition (object detection, categorization)"],"title":"Memory Attention Networks for Skeleton-Based Action Recognition","type":"publication"},{"authors":["Douwe Kiela","Hamed Firooz","Aravind Mohan","Vedanuj Goswami","Amanpreet Singh","Casey A. Fitzpatrick","Peter Bull","Greg Lipstein","Tony Nelli","Ron Zhu","Niklas Muennighoff","Rize Velioglu","Jewgeni Rose","Phillip Lippe","Nithin Holla","Shantanu Chandra","Santhosh Rajamanickam","Georgios Antoniou","Ekaterina Shutova","Helen Yannakoudakis","Vlad Sandulescu","Umut Ozertem","Patrick Pantel","Lucia Specia","Devi Parikh"],"categories":[],"content":"","date":1629417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629417600,"objectID":"d453c0a2c530cadc574229850a8e6592","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/phillip-pmlr-2021/","publishdate":"2021-08-20T00:00:00Z","relpermalink":"/vislab/publication/phillip-pmlr-2021/","section":"publication","summary":"Machine learning and artificial intelligence play an ever more crucial role in mitigating important societal problems, such as the prevalence of hate speech. We describe the Hateful Memes Challenge competition, held at NeurIPS 2020, focusing on multimodal hate speech. The aim of the challenge is to facilitate further research into multimodal reasoning and understanding.","tags":["Recognition (object detection, categorization)"],"title":"The Hateful Memes Challenge: Competition Report","type":"publication"},{"authors":["Yuki M. Asano","Jakob J. Kolb","Jobst Heitzig","J. Doyne Farmer"],"categories":[],"content":"","date":1625616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625616000,"objectID":"6509ac059f0acc1d2a73452c8a71cb1c","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yuki-pnas-2021/","publishdate":"2021-07-07T00:00:00Z","relpermalink":"/vislab/publication/yuki-pnas-2021/","section":"publication","summary":"Standard macroeconomic models assume that households are rational in the sense that they are perfect utility maximizers and explain economic dynamics in terms of shocks that drive the economy away from the steady state. Here we build on a standard macroeconomic model in which a single rational representative household makes a savings decision of how much to consume or invest. In our model, households are myopic boundedly rational heterogeneous agents embedded in a social network. From time to time each household updates its savings rate by copying the savings rate of its neighbor with the highest consumption. If the updating time is short, the economy is stuck in a poverty trap, but for longer updating times economic output approaches its optimal value, and we observe a critical transition to an economy with irregular endogenous oscillations in economic output, resembling a business cycle. In this regime households divide into two groups: poor households with low savings rates and rich households with high savings rates. Thus, inequality and economic dynamics both occur spontaneously as a consequence of imperfect household decision-making. Adding a few “rational” agents with a fixed savings rate equal to the long-term optimum allows us to match business cycle timescales. Our work here supports an alternative program of research that substitutes utility maximization for behaviorally grounded decision-making.","tags":["Complexity Economics"],"title":"Emergent inequality and business cycles in a simple behavioral macroeconomic model ","type":"publication"},{"authors":["Zehao Xiao","Jiayi Shen","Xiantong Zhen","Ling Shao","Cees G. M. Snoek"],"categories":null,"content":"","date":1625529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625529600,"objectID":"3ce645a6d77ca522d0ac7c495e05a6d3","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/xiao-icml-2021/","publishdate":"2021-07-06T00:00:00Z","relpermalink":"/vislab/publication/xiao-icml-2021/","section":"publication","summary":"Domain generalization is challenging due to the domain shift and the uncertainty caused by the inaccessibility of target domain data. In this pa- per, we address both challenges with a probabilis- tic framework based on variational Bayesian in- ference, by incorporating uncertainty into neu- ral network weights. We couple domain invari- ance in a probabilistic formula with the varia- tional Bayesian inference. This enables us to ex- plore domain-invariant learning in a principled way. Specifically, we derive domain-invariant rep- resentations and classifiers, which are jointly es- tablished in a two-layer Bayesian neural network. We empirically demonstrate the effectiveness of our proposal on four widely used cross-domain visual recognition benchmarks. Ablation studies validate the synergistic benefits of our Bayesian treatment when jointly learning domain-invariant representations and classifiers for domain general- ization. Further, our method consistently delivers state-of-the-art mean accuracy on all benchmarks.","tags":["Recognition (object detection, categorization)"],"title":"A Bit More Bayesian: Domain-Invariant Learning with Uncertainty","type":"publication"},{"authors":["Mohammad Mahdi Derakhshan","Xiantong Zhen","Ling Shao","Cees G. M. Snoek"],"categories":null,"content":"","date":1625529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625529600,"objectID":"c36ff5d16f1495117531d30c80cc0bee","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mohammad-icml-2021/","publishdate":"2021-07-06T00:00:00Z","relpermalink":"/vislab/publication/mohammad-icml-2021/","section":"publication","summary":"This paper introduces kernel continual learning, a simple but effective variant of continual learning that leverages the non-parametric nature of kernel methods to tackle catastrophic forgetting. We deploy an episodic memory unit that stores a subset of samples for each task to learn task-specific classifiers based on kernel ridge regression. This does not require memory replay and systematically avoids task interference in the classifiers. We further introduce variational random features to learn a data-driven kernel for each task. To do so, we formulate kernel continual learning as a variational inference problem, where a random Fourier basis is incorporated as the latent variable. The variational posterior distribution over the random Fourier basis is inferred from the coreset of each task. In this way, we are able to generate more informative kernels specific to each task, and, more importantly, the coreset size can be reduced to achieve more compact memory, resulting in more efficient continual learning based on episodic memory. Extensive evaluation on four benchmarks demonstrates the effectiveness and promise of kernels for continual learning.","tags":["Continual Learning"],"title":"Kernel Continual Learning","type":"publication"},{"authors":["Yunlu Chen","Basura Fernando","Hakan Bilen","Thomas Mensink","Efstratios Gavves"],"categories":null,"content":"","date":1625529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625529600,"objectID":"f7bbb64a6fb9dc9437b8765549a432d4","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/chen-icml-2021-/","publishdate":"2021-07-06T00:00:00Z","relpermalink":"/vislab/publication/chen-icml-2021-/","section":"publication","summary":"Recently, neural implicit functions have achieved impressive results for encoding 3D shapes. Conditioning on low-dimensional latent codes generalises a single implicit function to learn shared representation space for a variety of shapes, with the advantage of smooth interpolation. While the benefits from the global latent space do not correspond to explicit points at local level, we propose to track the continuous point trajectory by matching implicit features with the latent code interpolating between shapes, from which we corroborate the hierarchical functionality of the deep implicit functions, where early layers map the latent code to fitting the coarse shape structure, and deeper layers further refine the shape details. Furthermore, the structured representation space of implicit functions enables to apply feature matching for shape deformation, with the benefits to handle topology and semantics inconsistency, such as from an armchair to a chair with no arms, without explicit flow functions or manual annotations.","tags":["3D computer vision"],"title":"Neural Feature Matching in Implicit 3D Representations","type":"publication"},{"authors":["Adeel Pervez","Efstratios Gavves"],"categories":null,"content":"","date":1625529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625529600,"objectID":"fbc89b9ca425aa5daf17faadd0ac6c86","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/adeel-icml-2021/","publishdate":"2021-07-06T00:00:00Z","relpermalink":"/vislab/publication/adeel-icml-2021/","section":"publication","summary":"Variational autoencoders with deep stochastic hierarchies are known to suffer from the problem of posterior collapse, where the top layers fall back to the prior and become independent of input. We suggest that the hierarchical VAE objective explicitly includes the variance of the function parameterizing the mean and variance of the latent Gaussian distribution which itself is often a high variance function. Building on this we generalize VAE neural networks by incorporating a smoothing parameter motivated by Gaussian analysis to reduce higher frequency components and consequently the variance in parameterizing functions. We show this helps to solve the problem of posterior collapse. We further show that under such smoothing the VAE loss exhibits a phase transition, where the top layer KL divergence sharply drops to zero at a critical value of the smoothing parameter that is similar for the same model across datasets. We validate the phenomenon across model configurations and datasets.","tags":["generative model"],"title":"Spectral Smoothing Unveils Phase Transitions in Hierarchical Variational Autoencoders","type":"publication"},{"authors":["Mohammadreza Salehi","Atrin Arya","Barbod Pajoum","Mohammad Otoofi","Amirreza Shaeiri","Mohammad Hossein Rohban","Hamid R Rabiee"],"categories":[],"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"471b424297379ca6079e317f8946c6fd","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mohammadreza-mm-2021/","publishdate":"2021-07-01T00:00:00Z","relpermalink":"/vislab/publication/mohammadreza-mm-2021/","section":"publication","summary":"Autoencoders (AE) have recently been widely employed to approach the novelty detection problem. Trained only on the normal data, the AE is expected to reconstruct the normal data effectively while fail to regenerate the anomalous data, which could be utilized for novelty detection. However, in this paper, it is demonstrated that this does not always hold. AE often generalizes so perfectly that it can also reconstruct the anomalous data well. To address this problem, we propose a novel AE that can learn more semantically meaningful features. Specifically, we exploit the fact that adversarial robustness promotes learning of meaningful features. Therefore, we force the AE to learn such features by penalizing networks with a bottleneck layer that is unstable against adversarial perturbations. We show that despite using a much simpler architecture in comparison to the prior methods, the proposed AE outperforms or is competitive to state-of-the-art on three benchmark datasets.","tags":["Transfer, low-shot, semi- and un- supervised learning"],"title":"Arae: Adversarially robust training of autoencoders improves novelty detection","type":"publication"},{"authors":["Ze Wang","Zichen Miao","Xiantong Zhen","Qiang Qiu"],"categories":[],"content":"","date":1624060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624060800,"objectID":"3d59e230fd993674fa4ff26229847f46","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/ze-neurips-2021/","publishdate":"2021-06-19T00:00:00Z","relpermalink":"/vislab/publication/ze-neurips-2021/","section":"publication","summary":"Gaussian processes with deep neural networks demonstrate to be a strong learner for few-shot learning since they combine the strength of deep learning and kernels while being able to well capture uncertainty. However, it remains an open problem to leverage the shared knowledge provided by related tasks. In this paper, we propose to learn Gaussian processes with dense inducing variables by meta-learning for few-shot learning. In contrast to sparse Gaussian processes, we define a set of dense inducing variables to be of a much larger size than the support set in each task, which collects prior knowledge from experienced tasks. The dense inducing variables specify a shared Gaussian process prior over prediction functions of all tasks, which are learned in a variational inference framework and offer a strong inductive bias for learning new tasks. To achieve task-specific prediction functions, we propose to adapt the inducing variables to each task by efficient gradient descent. We conduct extensive experiments on common benchmark datasets for a variety of few-shot learning tasks. Our dense Gaussian processes present significant improvements over vanilla Gaussian processes and comparable or even better performance with state-of-the-art methods.","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"Learning to Learn Dense Gaussian Processes for Few-Shot Learning","type":"publication"},{"authors":["Yunhua Zhang","Lijun Wang","Dong Wang","Jinqing Qi","Huchuan Lu"],"categories":[],"content":"","date":1623974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623974400,"objectID":"fe79a54adf46d0e7fab41b038cfc84c1","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yunhua-ijcv-2021/","publishdate":"2021-06-18T00:00:00Z","relpermalink":"/vislab/publication/yunhua-ijcv-2021/","section":"publication","summary":"This paper proposes a new visual tracking algorithm, which leverages the merits of both template matching approaches and classification models for long-term object detection and tracking. To this end, a regression network is learned offline to detect a set of target candidates through target template matching. To cope with target appearance variations in long-term scenarios, a target-aware feature fusion mechanism is also developed, giving rise to more effective template matching. Meanwhile, a verification network is trained online to better capture target appearance and identify the target from potential candidates. During online update, contaminated training samples can be filtered out through a monitoring module, alleviating model degeneration caused by error accumulation. The regression and verification networks operate in a cascaded manner, which allows tracking to be performed in a coarse-to-fine manner and enforces the discriminative power. To further address the target reappearance issues in long-term tracking, a learning-based switching scheme is proposed, which learns to switch the tracking mode between local and global search based on the tracking results. Extensive evaluations on long-term tracking in the wild have been conducted. We achieve state-of-the-art performance on the OxUvA long-term tracking dataset. Our submission based on the proposed method has also won the 1st place of the long-term tracking challenge in VOT-2018 competition.","tags":["Motion and tracking"],"title":"Learning Regression and Verification Networks for Robust Long-term Tracking","type":"publication"},{"authors":["Ivona Najdenkoska","Xiantong Zhen","Marcel Worring","Ling Shao"],"categories":[],"content":"","date":1623888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623888000,"objectID":"b2b366d22c50e2b0e59635dfe547d7dd","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/ivona-miccai-2021/","publishdate":"2021-06-17T00:00:00Z","relpermalink":"/vislab/publication/ivona-miccai-2021/","section":"publication","summary":"Automating report generation for medical imaging promises to reduce workload and assist diagnosis in clinical practice. Recent work has shown that deep learning models can successfully caption natural images. However, learning from medical data is challenging due to the diversity and uncertainty inherent in the reports written by different radiologists with discrepant expertise and experience. To tackle these challenges, we propose variational topic inference for automatic report generation. Specifically, we introduce a set of topics as latent variables to guide sentence generation by aligning image and language modalities in a latent space. The topics are inferred in a conditional variational inference framework, with each topic governing the generation of a sentence in the report. Further, we adopt a visual attention module that enables the model to attend to different locations in the image and generate more informative descriptions. We conduct extensive experiments on two benchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results demonstrate that our proposed variational topic inference method can generate novel reports rather than mere copies of reports used in training, while still achieving comparable performance to state-of-the-art methods in terms of standard language generation criteria.","tags":["Multi-modal learning"],"title":"Variational Topic Inference for Chest X-Ray Report Generation","type":"publication"},{"authors":["Yingjun Du","Nithin Holla","Xiantong Zhen","Cees GM Snoek","Ekaterina Shutova"],"categories":[],"content":"","date":1622678400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622678400,"objectID":"9bd4a4c943a6e202f0a132f1f1ec495d","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yingjun-acl-2021/","publishdate":"2021-06-03T00:00:00Z","relpermalink":"/vislab/publication/yingjun-acl-2021/","section":"publication","summary":"A critical challenge faced by supervised word sense disambiguation (WSD) is the lack of large annotated datasets with sufficient coverage of words in their diversity of senses. This inspired recent research on few-shot WSD using meta-learning. While such work has successfully applied meta-learning to learn new word senses from very few examples, its performance still lags behind its fully supervised counterpart. Aiming to further close this gap, we propose a model of semantic memory for WSD in a meta-learning setting. Semantic memory encapsulates prior experiences seen throughout the lifetime of the model, which aids better generalization in limited data settings. Our model is based on hierarchical variational inference and incorporates an adaptive memory update rule via a hypernetwork. We show our model advances the state of the art in few-shot WSD, supports effective learning in extremely data scarce (e.g. one-shot) scenarios and produces meaning prototypes that capture similar senses of distinct words.","tags":["Natural Language Processing"],"title":"Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation","type":"publication"},{"authors":["Zenglin Shi","Yunlu Chen","Efstratios Gavves","Pascal Mettes","Cees G.M. Snoek"],"categories":[],"content":"","date":1622592000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622592000,"objectID":"d1fa719ca515d83ec72411d5f5edba68","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/zenglin-tip-2021/","publishdate":"2021-06-02T00:00:00Z","relpermalink":"/vislab/publication/zenglin-tip-2021/","section":"publication","summary":"The goal of this paper is guided image filtering, which emphasizes the importance of structure transfer during filtering by means of an additional guidance image. Where classical guided filters transfer structures using hand-designed functions, recent guided filters have been considerably advanced through parametric learning of deep networks. The state-of-the- art leverages deep networks to estimate the two core coefficients of the guided filter. In this work, we posit that simultaneously estimating both coefficients is suboptimal, resulting in halo arti- facts and structure inconsistencies. Inspired by unsharp masking, a classical technique for edge enhancement that requires only a single coefficient, we propose a new and simplified formulation of the guided filter. Our formulation enjoys a filtering prior from a low-pass filter and enables explicit structure transfer by estimating a single coefficient. Based on our proposed for- mulation, we introduce a successive guided filtering network, which provides multiple filtering results from a single network, allowing for a trade-off between accuracy and efficiency. Exten- sive ablations, comparisons and analysis show the effectiveness and efficiency of our formulation and network, resulting in state-of-the-art results across filtering tasks like upsampling, denoising, and cross-modality filtering. Code is available at https://github.com/shizenglin/Unsharp-Mask-Guided-Filtering.","tags":["Image retrieval"],"title":"Unsharp Mask Guided Filtering","type":"publication"},{"authors":["Jinbao Wang","Shujie Tan","Xiantong Zhen","Shuo Xu","Feng Zheng","Zhenyu He","Ling Shao"],"categories":[],"content":"","date":1621814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621814400,"objectID":"dbc4a37aac7e5147a904a432dde2e66e","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/jinbao-cviu-2021/","publishdate":"2021-05-24T00:00:00Z","relpermalink":"/vislab/publication/jinbao-cviu-2021/","section":"publication","summary":"Three-dimensional (3D) human pose estimation involves estimating the articulated 3D joint locations of a human body from an image or video. Due to its widespread applications in a great variety of areas, such as human motion analysis, human–computer interaction, robots, 3D human pose estimation has recently attracted increasing attention in the computer vision community, however, it is a challenging task due to depth ambiguities and the lack of in-the-wild datasets. A large number of approaches, with many based on deep learning, have been developed over the past decade, largely advancing the performance on existing benchmarks. To guide future development, a comprehensive literature review is highly desired in this area. However, existing surveys on 3D human pose estimation mainly focus on traditional methods and a comprehensive review on deep learning based methods remains lacking in the literature. In this paper, we provide a thorough review of existing deep learning based works for 3D pose estimation, summarize the advantages and disadvantages of these methods and provide an in-depth understanding of this area. Furthermore, we also explore the commonly-used benchmark datasets on which we conduct a comprehensive study for comparison and analysis. Our study sheds light on the state of research development in 3D human pose estimation and provides insights that can facilitate the future design of models and algorithms.","tags":["Action and behavior recognition"],"title":"Deep 3D human pose estimation: A review","type":"publication"},{"authors":["Haochen Wang","Yandan Yang","Xianbin Cao","Xiantong Zhen","Cees Snoek","Ling Shao"],"categories":[],"content":"","date":1621468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621468800,"objectID":"be1586b05d945a63afb4aa8e0636e452","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/haochen-wacv-2021/","publishdate":"2021-05-20T00:00:00Z","relpermalink":"/vislab/publication/haochen-wacv-2021/","section":"publication","summary":"In this paper, we propose variational prototype inference to address few-shot semantic segmentation in a probabilistic framework. A probabilistic latent variable model infers the distribution of the prototype that is treated as the latent variable. We formulate the optimization as a variational inference problem, which is established with an amortized inference network based on an auto-encoder architecture. The probabilistic modeling of the prototype enhances its generalization ability to handle the inherent uncertainty caused by limited data and the huge intra-class variations of objects. Moreover, it offers a principled way to incorporate the prototype extracted from support images into the prediction of the segmentation maps for query images. We conduct extensive experimental evaluation on three benchmark datasets. Ablation studies show the effectiveness of variational prototype inference for few-shot semantic segmentation by probabilistic modeling. On all three benchmarks, our proposal achieves high segmentation accuracy and surpasses previous methods by considerable margins.","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"Variational prototype inference for few-shot semantic segmentation","type":"publication"},{"authors":["Pengwan Yang","Pascal Mettes","Cees G. M. Snoek"],"categories":null,"content":"","date":1619049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619049600,"objectID":"f4ee0d1bac3c48952324cccb8c5454df","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pengwan-cvpr-2021/","publishdate":"2021-04-22T00:00:00Z","relpermalink":"/vislab/publication/pengwan-cvpr-2021/","section":"publication","summary":"This paper introduces the task of few-shot common action localization in time and space. Given a few trimmed support videos containing the same but unknown action, we strive for spatio-temporal localization of that action in a long untrimmed query video. We do not require any class labels, interval bounds, or bounding boxes. To address this challenging task, we introduce a novel few-shot transformer architecture with a dedicated encoder-decoder structure optimized for joint commonality learning and localization prediction, without the need for proposals. Experiments on our reorganizations of the AVA and UCF101-24 datasets show the effectiveness of our approach for few-shot common action localization, even when the support videos are noisy. Although we are not specifically designed for common localization in time only, we also compare favorably against the few-shot and one-shot state-of-the-art in this setting. Lastly, we demonstrate that the few-shot transformer is easily extended to common action localization per pixel.","tags":["video analysis and understanding"],"title":"Few-Shot Transformation of Common Actions into Time and Space","type":"publication"},{"authors":["Mohammadreza Salehi","Niousha Sadjadi","Soroosh Baselizadeh","Mohammad Hossein Rohban","Hamid R. Rabiee"],"categories":[],"content":"","date":1619049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619049600,"objectID":"d23b50b3ba4a53f24250c1685d24059b","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mohammadreza-cvpr-2021/","publishdate":"2021-04-22T00:00:00Z","relpermalink":"/vislab/publication/mohammadreza-cvpr-2021/","section":"publication","summary":"Unsupervised representation learning has proved to be a critical component of anomaly detection/localization in images. The challenges to learn such a representation are two-fold. Firstly, the sample size is not often large enough to learn a rich generalizable representation through conventional techniques. Secondly, while only normal samples are available at training, the learned features should be discriminative of normal and anomalous samples. Here, we propose to use the 'distillation' of features at various layers of an expert network, pre-trained on ImageNet, into a simpler cloner network to tackle both issues. We detect and localize anomalies using the discrepancy between the expert and cloner networks' intermediate activation values given the input data. We show that considering multiple intermediate hints in distillation leads to better exploiting the expert's knowledge and more distinctive discrepancy compared to solely utilizing the last layer activation values. Notably, previous methods either fail in precise anomaly localization or need expensive region-based training. In contrast, with no need for any special or intensive training procedure, we incorporate interpretability algorithms in our novel framework for the localization of anomalous regions. Despite the striking contrast between some test datasets and ImageNet, we achieve competitive or significantly superior results compared to the SOTA methods on MNIST, F-MNIST, CIFAR-10, MVTecAD, Retinal-OCT, and two Medical datasets on both anomaly detection and localization.","tags":["Transfer, low-shot, semi- and un- supervised learning"],"title":"Multiresolution Knowledge Distillation for Anomaly Detection","type":"publication"},{"authors":["Michael Wray","Hazel Doughty","Dima Damen"],"categories":null,"content":"","date":1619049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619049600,"objectID":"e34efe3a6230038dda80bfa5f7c71088","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/hazeldoughty-cvpr-2021/","publishdate":"2021-04-22T00:00:00Z","relpermalink":"/vislab/publication/hazeldoughty-cvpr-2021/","section":"publication","summary":"Current video retrieval efforts all found their evaluation on an instance-based assumption, that only a single caption is relevant to a query video and vice versa. We demonstrate that this assumption results in performance comparisons often not indicative of models retrieval capabilities. We propose a move to semantic similarity video retrieval, where (i) multiple videos/captions can be deemed equally relevant, and their relative ranking does not affect a methods reported performance and (ii) retrieved videos/captions are ranked by their similarity to a query. We propose several proxies to estimate semantic similarities in large-scale retrieval datasets, without additional annotations. Our analysis is performed on three commonly used video retrieval datasets (MSR-VTT, YouCook2 and EPIC-KITCHENS).","tags":["video analysis and understanding"],"title":"On Semantic Similarity in Video Retrieval","type":"publication"},{"authors":["Yunhua Zhang","Ling Shao","Cees G.M. Snoek"],"categories":null,"content":"","date":1619049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619049600,"objectID":"6780647e98a2c01577ab4cc1b1fa42e6","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yunhua-cvpr-2021/","publishdate":"2021-04-22T00:00:00Z","relpermalink":"/vislab/publication/yunhua-cvpr-2021/","section":"publication","summary":"This paper strives for repetitive activity counting in videos. Different from existing works, which all analyze the visual video content only, we incorporate for the first time the corresponding sound into the repetition counting process. This benefits accuracy in challenging vision conditions such as occlusion, dramatic camera view changes, low resolution, etc. We propose a model that starts with analyzing the sight and sound streams separately. Then an audiovisual temporal stride decision module and a reliability estimation module are introduced to exploit cross-modal temporal interaction. For learning and evaluation, an existing dataset is repur- posed and reorganized to allow for repetition counting with sight and sound. We also introduce a variant of this dataset for repetition counting under challenging vision conditions. Experiments demonstrate the benefit of sound, as well as the other introduced modules, for repetition counting. Our sight-only model already outperforms the state-of-the-art by itself, when we add sound, results improve notably, especially under harsh vision conditions.","tags":["video analysis and understanding"],"title":"Repetitive Activity Counting by Sight and Sound","type":"publication"},{"authors":["Deepak Gupta","Devanshu Arya","Efstratios Gavves"],"categories":null,"content":"","date":1619049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619049600,"objectID":"f9c451026f201a29c12723635bda93d2","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/gupta-cvpr-2021/","publishdate":"2021-04-22T00:00:00Z","relpermalink":"/vislab/publication/gupta-cvpr-2021/","section":"publication","summary":"Rotation is among the long prevailing, yet still unresolved, hard challenges encountered in visual object tracking. The existing deep learning-based tracking algorithms use regular CNNs that are inherently translation equivariant, but not designed to tackle rotations. In this paper, we first demonstrate that in the presence of rotation instances in videos, the performance of existing trackers is severely affected. To circumvent the adverse effect of rotations, we present rotation-equivariant Siamese networks (RE-SiamNets), built through the use of group-equivariant convolutional layers comprising steerable filters. SiamNets allow estimating the change in orientation of the object in an unsupervised manner, thereby facilitating its use in relative 2D pose estimation as well. We further show that this change in orientation can be used to impose an additional motion constraint in Siamese tracking through imposing restriction on the change in orientation between two consecutive frames. For benchmarking, we present Rotation Tracking Benchmark (RTB), a dataset comprising a set of videos with rotation instances. Through experiments on two popular Siamese architectures, we show that RE-SiamNets handle the problem of rotation very well and out-perform their regular counterparts. Further, RE-SiamNets can accurately estimate the relative change in pose of the target in an unsupervised fashion, namely the in-plane rotation the target has sustained with respect to the reference frame.","tags":["motion and tracking"],"title":"Rotation Equivariant Siamese Networks for Tracking","type":"publication"},{"authors":["Phillip Lippe","Efstratios Gavves"],"categories":null,"content":"","date":1618876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618876800,"objectID":"7fdaf3440bdd657f0deffa1adfc77f65","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/phillip-iclr-2021/","publishdate":"2021-04-20T00:00:00Z","relpermalink":"/vislab/publication/phillip-iclr-2021/","section":"publication","summary":"Despite their popularity, to date, the application of normalizing flows on categorical data stays limited. The current practice of using dequantization to map discrete data to a continuous space is inapplicable as categorical data has no intrinsic order. Instead, categorical data have complex and latent relations that must be inferred, like the synonymy between words. In this paper, we investigate Categorical Normalizing Flows, that is normalizing flows for categorical data. By casting the encoding of categorical data in continuous space as a variational inference problem, we jointly optimize the continuous representation and the model likelihood. Using a factorized decoder, we introduce an inductive bias to model any interactions in the normalizing flow. As a consequence, we do not only simplify the optimization compared to having a joint decoder, but also make it possible to scale up to a large number of categories that is currently impossible with discrete normalizing flows. Based on Categorical Normalizing Flows, we propose GraphCNF a permutation-invariant generative model on graphs. GraphCNF implements a three step approach modeling the nodes, edges, and adjacency matrix stepwise to increase efficiency. On molecule generation, GraphCNF outperforms both one-shot and autoregressive flow-based state-of-the-art.","tags":["generative model"],"title":"Categorical Normalizing Flows via Continuous Transformations","type":"publication"},{"authors":["Jiaojiao Zhao","Cees G.M. Snoek"],"categories":null,"content":"","date":1618876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618876800,"objectID":"edb3f3acf627d89ecf45617a14118ccb","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/jiaojiaozhao-iclr-2021/","publishdate":"2021-04-20T00:00:00Z","relpermalink":"/vislab/publication/jiaojiaozhao-iclr-2021/","section":"publication","summary":"Pooling is a critical operation in convolutional neural networks for increasing receptive fields and improving robustness to input variations. Most existing pooling operations downsample the feature maps, which is a lossy process. Moreover, they are not invertible: upsampling a downscaled feature map can not recover the lost information in the downsampling. By adopting the philosophy of the classical Lifting Scheme from signal processing, we propose LiftPool for bidirectional pooling layers, including LiftDownPool and LiftUpPool. LiftDownPool decomposes a feature map into various downsized sub-bands, each of which contains information with different frequencies. As the pooling function in LiftDownPool is perfectly invertible, by performing LiftDownPool backwards, a corresponding up-pooling layer LiftUpPool is able to generate a refined upsampled feature map using the detail sub-bands, which is useful for image-to-image translation challenges. Experiments show the proposed methods achieve better results on image classification and semantic segmentation, using various backbones. Moreover, LiftDownPool offers better robustness to input corruptions and perturbations.","tags":["recognition"],"title":"LiftPool: Bidirectional ConvNet Pooling","type":"publication"},{"authors":["Yingjun Du","Xiantong Zhen","Ling Shao","Cees G. M. Snoek"],"categories":null,"content":"","date":1618876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618876800,"objectID":"b364e30a3d5c239508c81402ed11ffb3","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/yingjundu-iclr-2021/","publishdate":"2021-04-20T00:00:00Z","relpermalink":"/vislab/publication/yingjundu-iclr-2021/","section":"publication","summary":"Batch normalization plays a crucial role when training deep neural networks. However, batch statistics become unstable with small batch sizes and are unreliable in the presence of distribution shifts. We propose MetaNorm, a simple yet effective meta-learning normalization. It tackles the aforementioned issues in a unified way by leveraging the meta-learning setting and learns to infer adaptive statistics for batch normalization. MetaNorm is generic, flexible and model-agnostic, making it a simple plug-and-play module that is seamlessly embedded into existing meta-learning approaches. It can be efficiently implemented by lightweight hypernetworks with low computational cost. We verify its effectiveness by extensive evaluation on representative tasks suffering from the small batch and domain shift problems: few-shot learning and domain generalization. We further introduce an even more challenging setting: few-shot domain generalization. Results demonstrate that MetaNorm consistently achieves better, or at least competitive, accuracy compared to existing batch normalization methods.","tags":["transfer learning"],"title":"MetaNorm: Learning to Normalize Few-Shot Batches Across Domains","type":"publication"},{"authors":["Duy-Kien Nguyen","Vedanuj Goswami","Xinlei Chen"],"categories":null,"content":"","date":1618876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618876800,"objectID":"b3f8f7d7f1b3c15cc1fb51024b55895d","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/nguyen-iclr-2021/","publishdate":"2021-04-20T00:00:00Z","relpermalink":"/vislab/publication/nguyen-iclr-2021/","section":"publication","summary":"This paper focuses on visual counting, which aims to predict the number of occurrences given a natural image and a query (e.g. a question or a category). Unlike most prior works that use explicit, symbolic models which can be computationally expensive and limited in generalization, we propose a simple and effective alternative by revisiting modulated convolutions that fuse the query and the image locally. Following the design of residual bottleneck, we call our method MoVie, short for Modulated conVolutional bottlenecks. Notably, MoVie reasons implicitly and holistically and only needs a single forward-pass during inference. Nevertheless, MoVie showcases strong performance for counting: 1) advancing the state-of-the-art on counting-specific VQA tasks while being more efficient; 2) outperforming prior-art on difficult benchmarks like COCO for common object counting; 3) helped us secure the first place of 2020 VQA challenge when integrated as a module for ‘number’ related questions in generic VQA models. Finally, we show evidence that modulated convolutions such as MoVie can serve as a general mechanism for reasoning tasks beyond counting.","tags":["visual reasoning and logical representation"],"title":"MoVie: Revisiting Modulated Convolutions for Visual Counting and Beyond","type":"publication"},{"authors":["David Zhang","Gertjan Burghouts","Cees Snoek"],"categories":null,"content":"","date":1618876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618876800,"objectID":"0e9872d928405af213a29480ccbff9e9","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/davidzhang-iclr-2021/","publishdate":"2021-04-20T00:00:00Z","relpermalink":"/vislab/publication/davidzhang-iclr-2021/","section":"publication","summary":"Set prediction is about learning to predict a collection of unordered variables with unknown interrelations. Training such models with set losses imposes the structure of a metric space over sets. We focus on stochastic and underdefined cases, where an incorrectly chosen loss function leads to implausible predictions. Example tasks include conditional point-cloud reconstruction and predicting future states of molecules. In this paper, we propose an alternative to training via set losses by viewing learning as conditional density estimation. Our learning framework fits deep energy-based models and approximates the intractable likelihood with gradient-guided sampling. Furthermore, we propose a stochastically augmented prediction algorithm that enables multiple predictions, reflecting the possible variations in the target set. We empirically demonstrate on a variety of datasets the capability to learn multi-modal densities and produce different plausible predictions. Our approach is competitive with previous set prediction models on standard benchmarks. More importantly, it extends the family of addressable tasks beyond those that have unambiguous predictions.","tags":["set prediction"],"title":"Set Prediction without Imposing Structure as Conditional Density Estimation","type":"publication"},{"authors":["Mandela Patrick*","Po-Yao Huang*","Yuki M. Asano*","Florian Metze","Alexander Hauptmann","João F. Henriques","Andrea Vedaldi"],"categories":[],"content":"","date":1618876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618876800,"objectID":"5c0e75758d3007611553e1495bb840f3","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mandela-iclr-2021/","publishdate":"2021-04-20T00:00:00Z","relpermalink":"/vislab/publication/mandela-iclr-2021/","section":"publication","summary":"The dominant paradigm for learning video-text representations -- noise contrastive learning -- increases the similarity of the representations of pairs of samples that are known to be related, such as text and video from the same sample, and pushes away the representations of all other pairs. We posit that this last behaviour is too strict, enforcing dissimilar representations even for samples that are semantically-related -- for example, visually similar videos or ones that share the same depicted action. In this paper, we propose a novel method that alleviates this by leveraging a generative model to naturally push these related samples together: each sample's caption must be reconstructed as a weighted combination of other support samples' visual representations. This simple idea ensures that representations are not overly-specialized to individual samples, are reusable across the dataset, and results in representations that explicitly encode semantics shared between samples, unlike noise contrastive learning. Our proposed method outperforms others by a large margin on MSR-VTT, VATEX and ActivityNet, and MSVD for video-to-text and text-to-video retrieval.","tags":["Video analysis and understanding"],"title":"Support-set bottlenecks for video-text representation learning","type":"publication"},{"authors":["Pascal Mettes","William Thong","Cees G.M. Snoek"],"categories":[],"content":"","date":1618790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618790400,"objectID":"8e59066b022788f5d0e86f0fff5f6934","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pascal-ijcv-2021-/","publishdate":"2021-04-19T00:00:00Z","relpermalink":"/vislab/publication/pascal-ijcv-2021-/","section":"publication","summary":"This work strives for the classification and localization of human actions in videos, without the need for any labeled video training examples. Where existing work relies on transferring global attribute or object information from seen to unseen action videos, we seek to classify and spatio-temporally localize unseen actions in videos from image-based object information only. We propose three spatial object priors, which encode local person and object detectors along with their spatial relations. On top we introduce three semantic object priors, which extend semantic matching through word embeddings with three simple functions that tackle semantic ambiguity, object discrimination, and object naming. A video embedding combines the spatial and semantic object priors. It enables us to introduce a new video retrieval task that retrieves action tubes in video collections based on user-specified objects, spatial relations, and object size. Experimental evaluation on five action datasets shows the importance of spatial and semantic object priors for unseen actions. We find that persons and objects have preferred spatial relations that benefit unseen action localization, while using multiple languages and simple object filtering directly improves semantic matching, leading to state-of-the-art results for both unseen action classification and localization. ","tags":["Action and behavior recognition"],"title":"Object Priors for Classifying and Localizing Unseen Actions","type":"publication"},{"authors":["Sadaf Gulshad","Arnold Smeulders"],"categories":[],"content":"","date":1618704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618704000,"objectID":"c7f1a924779739b3b15269e324950209","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/sadaf-ijmir-2021/","publishdate":"2021-04-18T00:00:00Z","relpermalink":"/vislab/publication/sadaf-ijmir-2021/","section":"publication","summary":"In this paper, our aim is to provide human understandable intuitive factual and counterfactual explanations for the decisions of neural networks. Humans tend to reinforce their decisions by providing attributes and counterattributes. Hence, in this work, we utilize attributes as well as examples to provide explanations. In order to provide counterexplanations we make use of directed perturbations to arrive at the counterclass attribute values in doing so, we explain what is present and what is absent in the original image. We evaluate our method when images are misclassified into closer counterclasses as well as when misclassified into completely different counterclasses. We conducted experiments on both finegrained as well as coarsegrained datasets. We verified our attribute-based explanations method both quantitatively and qualitatively and showed that attributes provide discriminating and human understandable explanations for both standard as well as robust networks.","tags":["Explainable AI","Robustness","Image Classification"],"title":"Counterfactual attribute-based visual explanations for classification","type":"publication"},{"authors":["Tom van Sonbeek","Xiantong Zhen","Marcel Worring","Ling Shao"],"categories":[],"content":"","date":1614124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614124800,"objectID":"d30f70f34472160f3d323fda042d7f35","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/tom-ipmi-2021/","publishdate":"2021-02-24T00:00:00Z","relpermalink":"/vislab/publication/tom-ipmi-2021/","section":"publication","summary":"Disease classification relying solely on imaging data attracts great interest in medical image analysis. Current models could be further improved, however, by also employing Electronic Health Records (EHRs), which contain rich information on patients and findings from clinicians. It is challenging to incorporate this information into disease classification due to the high reliance on clinician input in EHRs, limiting the possibility for automated diagnosis. In this paper, we propose \textit{variational knowledge distillation} (VKD), which is a new probabilistic inference framework for disease classification based on X-rays that leverages knowledge from EHRs. Specifically, we introduce a conditional latent variable model, where we infer the latent representation of the X-ray image with the variational posterior conditioning on the associated EHR text. By doing so, the model acquires the ability to extract the visual features relevant to the disease during learning and can therefore perform more accurate classification for unseen patients at inference based solely on their X-ray scans. We demonstrate the effectiveness of our method on three public benchmark datasets with paired X-ray images and EHRs. The results show that the proposed variational knowledge distillation can consistently improve the performance of medical image classification and significantly surpasses current methods.","tags":["Transfer, low-shot, semi- and un-supervised learning"],"title":"Variational Knowledge Distillation for Disease Classification in Chest X-Rays","type":"publication"},{"authors":["Riaan Zoetmulder","Praneeta R Konduri","Iris V Obdeijn","Efstratios Gavves","Ivana Isgum","Charles BLM Majoie","Diederik WJ Dippel","Yvo BWEM Roos","Mayank Goyal","Peter J Mitchell","Bruce CV Campbell","Demetrius K Lopes","Gernot Reimann","Tudor G Jovin","Jeffrey L Saver","Keith W Muir","Phil White","Serge Bracard","Bailiang Chen","Scott Brown","Wouter J Schonewille","Erik van der Hoeven","Volker Puetz","Henk A Marquering"],"categories":[],"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"d2360a66adc0853d9928f4239f34caa3","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/riaan-d-2021-/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/vislab/publication/riaan-d-2021-/","section":"publication","summary":"Final lesion volume (FLV) is a surrogate outcome measure in anterior circulation stroke (ACS). In posterior circulation stroke (PCS), this relation is plausibly understudied due to a lack of methods that automatically quantify FLV. The applicability of deep learning approaches to PCS is limited due to its lower incidence compared to ACS. We evaluated strategies to develop a convolutional neural network (CNN) for PCS lesion segmentation by using image data from both ACS and PCS patients. We included follow-up non-contrast computed tomography scans of 1018 patients with ACS and 107 patients with PCS. To assess whether an ACS lesion segmentation generalizes to PCS, a CNN was trained on ACS data (ACS-CNN). Second, to evaluate the performance of only including PCS patients, a CNN was trained on PCS data. Third, to evaluate the performance when combining the datasets, a CNN was trained on both datasets. Finally, to evaluate the performance of transfer learning, the ACS-CNN was fine-tuned using PCS patients. The transfer learning strategy outperformed the other strategies in volume agreement with an intra-class correlation of 0.88 (95% CI: 0.83-0.92) vs. 0.55 to 0.83 and a lesion detection rate of 87% vs. 41-77 for the other strategies. Hence, transfer learning improved the FLV quantification and detection rate of PCS lesions compared to the other strategies.","tags":["Transfer, low-shot, semi- and un- supervised learning"],"title":"Automated Final Lesion Segmentation in Posterior Circulation Acute Ischemic Stroke Using Deep Learning","type":"publication"},{"authors":["Riaan Zoetmulder","Efstratios Gavves","Matthan Caan","Henk Marquering"],"categories":[],"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"16304b6595f5d652e0ded4f5993d0ab9","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/riaan-cmpb-2021/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/vislab/publication/riaan-cmpb-2021/","section":"publication","summary":"Background and objectives: Transfer learning is a valuable approach to perform medical image segmentation in settings with limited cases available for training convolutional neural networks (CNN). Both the source task and the source domain influence transfer learning performance on a given target medical image segmentation task. This study aims to assess transfer learning-based medical segmentation task performance for various source task and domain combinations. Methods: CNNs were pre-trained on classification, segmentation, and self-supervised tasks on two domains: natural images and T1 brain MRI. Next, these CNNs were fine-tuned on three target T1 brain MRI segmentation tasks: stroke lesion, MS lesions, and brain anatomy segmentation. In all experiments, the CNN architecture and transfer learning strategy were the same. The segmentation accuracy on all target tasks was evaluated using the mIOU or Dice coefficients. The detection accuracy was evaluated for the stroke and MS lesion target tasks only. Results: CNNs pre-trained on a segmentation task on the same domain as the target tasks resulted in higher or similar segmentation accuracy compared to other source task and domain combinations. Pre-training a CNN on ImageNet resulted in a comparable, but not consistently higher lesion detection rate, despite the amount of training data used being 10 times larger. Conclusions: This study suggests that optimal transfer learning for medical segmentation is achieved with a similar task and domain for pre-training. As a result, CNNs can be effectively pre-trained on smaller datasets by selecting a source domain and task similar to the target domain and task.","tags":["Transfer, low-shot, semi- and un- supervised learning"],"title":"Domain- and task-specific transfer learning for medical segmentation tasks","type":"publication"},{"authors":["Ivan Sosnovik","Artem Moskalev","Arnold Smeulders"],"categories":[],"content":"","date":1610496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610565879,"objectID":"80b6e91b0eadbb82fe474ee2a5c88961","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/moskalev-wacv-2021/","publishdate":"2021-01-13T19:24:39.516215Z","relpermalink":"/vislab/publication/moskalev-wacv-2021/","section":"publication","summary":"Siamese trackers turn tracking into similarity estimation between a template and the candidate regions in the frame. Mathematically, one of the key ingredients of success of the similarity function is translation equivariance. Non-translation-equivariant architectures induce a positional bias during training, so the location of the target will be hard to recover from the feature space. In real life scenarios, objects undergoe various transformations other than translation, such as rotation or scaling. Unless the model has an internal mechanism to handle them, the similarity may degrade. In this paper, we focus on scaling and we aim to equip the Siamese network with additional built-in scale equivariance to capture the natural variations of the target a priori. We develop the theory for scale-equivariant Siamese trackers, and provide a simple recipe for how to make a wide range of existing trackers scale-equivariant. We present SE-SiamFC, a scale-equivariant variant of SiamFC built according to the recipe. We conduct experiments on OTB and VOT benchmarks and on the synthetically generated T-MNIST and S-MNIST datasets. We demonstrate that a built-in additional scale equivariance is useful for visual object tracking.","tags":["motion and tracking"],"title":"Scale Equivariance Improves Siamese Tracking","type":"publication"},{"authors":["Fida Mohamamd Thoker","Cees Snoek"],"categories":[],"content":"","date":1609200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609200000,"objectID":"8c010720fa6e12fad40d818cc2cc8f34","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/thoker-icpr-2020/","publishdate":"2020-12-29T00:00:00Z","relpermalink":"/vislab/publication/thoker-icpr-2020/","section":"publication","summary":"This paper strives for action recognition and detection in video modalities like RGB, depth maps or 3D-skeleton sequences when only limited modality-specific labeled examples are available. For the RGB, and derived optical-flow, modality many large-scale labeled datasets have been made available. They have become the de facto pre-training choice when recognizing or detecting new actions from RGB datasets that have limited amounts of labeled examples available. Unfortunately, large-scale labeled action datasets for other modalities are unavailable for pre-training. In this paper, our goal is to recognize actions from limited examples in non-RGB video modalities, by learning from large-scale labeled RGB data. To this end, we propose a two-step training process: (i) we extract action representation knowledge from an RGB-trained teacher network and adapt it to a non-RGB student network. (ii) we then fine-tune the transfer model with available labeled examples of the target modality. For the knowledge transfer we introduce feature-supervision strategies, which rely on unlabeled pairs of two modalities (the RGB and the target modality) to transfer feature-level representations from the teacher to the student network. Ablations and generalizations with two RGB source datasets and two non-RGB target datasets demonstrate that an optical-flow teacher provides better action transfer features than RGB for both depth maps and 3D-skeletons, even when evaluated on a different target domain, or for a different task. Compared to alternative cross-modal action transfer methods we show a good improvement in performance especially when labeled non-RGB examples to learn from are scarce.","tags":["action recognition"],"title":"Feature-Supervised Action Modality Transfer","type":"publication"},{"authors":["Efstratios Gavves","Ran Tao","Deepak Gupta","Arnold Smeulders"],"categories":[],"content":"","date":1609200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609200000,"objectID":"ff954a05bd01c2df28fdff76dadc3063","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/gavves-icpr-2020/","publishdate":"2020-12-29T00:00:00Z","relpermalink":"/vislab/publication/gavves-icpr-2020/","section":"publication","summary":"Updating the tracker model with adverse bounding box predictions adds an unavoidable bias term to the learning. This bias term, which we refer to as model decay, offsets the learning and causes tracking drift. While its adverse affect might not be visible in short-term tracking, accumulation of this bias over a long-term can eventually lead to a permanent loss of the target. In this paper, we look at the problem of model bias from a mathematical perspective. Further, we briefly examine the effect of various sources of tracking error on model decay, using a correlation filter (ECO) and a Siamese (SINT) tracker. Based on observations and insights, we propose simple additions that help to reduce model decay in long-term tracking. The proposed tracker is evaluated on four long-term and one short term tracking benchmarks, demonstrating superior accuracy and robustness, even in 30 minute long videos.","tags":["motion and tracking"],"title":"Model Decay in Long-Term Tracking","type":"publication"},{"authors":["Shuai Liao","Efstratios Gavves","ChangYong Oh","Cees Snoek"],"categories":[],"content":"","date":1609200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609200000,"objectID":"720d98d8f7161adc2ff65e23a89e16fe","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/liao-icpr-2020/","publishdate":"2020-12-29T00:00:00Z","relpermalink":"/vislab/publication/liao-icpr-2020/","section":"publication","summary":"The softmax and binary classifier are commonly preferred for image classification applications. However, as softmax is specifically designed for categorical classification, it assumes each image has just one class label. This limits its applicability for problems where the number of labels does not equal one, most notably zero-and multi-label problems. In these challenging settings, binary classifiers are, in theory, better suited. However, as they ignore the correlation between classes, they are not as accurate and scalable in practice. In this paper, we start from the observation that the only difference between binary and softmax classifiers is their normalization function. Specifically, while the binary classifier self-normalizes its score, the softmax classifier combines the scores from all classes before normalisation. On the basis of this observation we introduce a normalization function that is learnable, constant, and shared between classes and data points. By doing so, we arrive at a new type of binary classifier that we coin quasibinary classifier. We show in a variety of image classification settings, and on several datasets, that quasibinary classifiers are considerably better in classification settings where regular binary and softmax classifiers suffer, including zerolabel and multi-label classification. What is more, we show that quasibinary classifiers yield well-calibrated probabilities allowing for direct and reliable comparisons, not only between classes but also between data points.","tags":["recognition"],"title":"Quasibinary Classifier for Images with Zero and Multiple Labels","type":"publication"},{"authors":["Mert Kilickaya","Noureldien Hussein","Efstratios Gavves","Arnold Smeulders"],"categories":[],"content":"","date":1609200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609200000,"objectID":"6d7aec9c1e3b951a907680b23e905e74","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mert-icpr-2020-/","publishdate":"2020-12-29T00:00:00Z","relpermalink":"/vislab/publication/mert-icpr-2020-/","section":"publication","summary":"Human-object interaction recognition aims for identifying the relationship between a human subject and an object. Researchers incorporate global scene context into the early layers of deep Convolutional Neural Networks as a solution. They report a significant increase in the performance since generally interactions are correlated with the scene (\\ie riding bicycle on the city street). However, this approach leads to the following problems. It increases the network size in the early layers, therefore not efficient. It leads to noisy filter responses when the scene is irrelevant, therefore not accurate. It only leverages scene context whereas human-object interactions offer a multitude of contexts, therefore incomplete. To circumvent these issues, in this work, we propose Self-Selective Context (SSC). SSC operates on the joint appearance of human-objects and context to bring the most discriminative context(s) into play for recognition. We devise novel contextual features that model the locality of human-object interactions and show that SSC can seamlessly integrate with the State-of-the-art interaction recognition models. Our experiments show that SSC leads to an important increase in interaction recognition performance, while using much fewer parameters.","tags":["vision and language"],"title":"Self-Selective Context for Interaction Recognition","type":"publication"},{"authors":["D. Gupta","E. Gavves","A. Smeulders"],"categories":[],"content":"","date":1609200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609200000,"objectID":"95a577f3aafcad190ec446a55284d880","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/gupta-icpr-2020/","publishdate":"2020-12-29T00:00:00Z","relpermalink":"/vislab/publication/gupta-icpr-2020/","section":"publication","summary":"Occlusion is one of the most difficult challenges in object tracking to model. This is because unlike other challenges, where data augmentation can be of help, occlusion is hard to simulate as the occluding object can be anything in any shape. In this paper, we propose a simple solution to simulate the effects of occlusion in the latent space. Specifically, we present structured dropout to mimick the change in latent codes under occlusion. We present three forms of dropout (channel dropout, segment dropout and slice dropout) with the various forms of occlusion in mind. To demonstrate its effectiveness, the dropouts are incorporated into two modern Siamese trackers (SiamFC and SiamRPN++). The outputs from multiple dropouts are combined using an encoder network to obtain the final prediction. Experiments on several tracking benchmarks show the benefits of structured dropouts, while due to their simplicity requiring only small changes to the existing tracker models.","tags":["motion and tracking"],"title":"Tackling Occlusion in Siamese Tracking with Structured Dropouts","type":"publication"},{"authors":["Rutger R. van de Leur","Lennart J. Blom","Efstratios Gavves","Irene E. Hof","Jeroen F. van der Heijden","Nick C. Clappers","Pieter A. Doevendans","Rutger J. Hassink","René van Es"],"categories":[],"content":"","date":1608681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608681600,"objectID":"4860a1c4f70e8fa14ea3a1ad13716f4a","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/leur-jaha-2020-/","publishdate":"2020-12-23T00:00:00Z","relpermalink":"/vislab/publication/leur-jaha-2020-/","section":"publication","summary":"The correct interpretation of the ECG is pivotal for the accurate diagnosis of many cardiac abnormalities, and conventional computerized interpretation has not been able to reach physician‐level accuracy in detecting (acute) cardiac abnormalities. This study aims to develop and validate a deep neural network for comprehensive automated ECG triage in daily practice.\nWe developed a 37‐layer convolutional residual deep neural network on a data set of free‐text physician‐annotated 12‐lead ECGs. The deep neural network was trained on a data set with 336.835 recordings from 142.040 patients and validated on an independent validation data set (n=984), annotated by a panel of 5 cardiologists electrophysiologists. The 12‐lead ECGs were acquired in all noncardiology departments of the University Medical Center Utrecht. The algorithm learned to classify these ECGs into the following 4 triage categories: normal, abnormal not acute, subacute, and acute. Discriminative performance is presented with overall and category‐specific concordance statistics, polytomous discrimination indexes, sensitivities, specificities, and positive and negative predictive values. The patients in the validation data set had a mean age of 60.4 years and 54.3% were men. The deep neural network showed excellent overall discrimination with an overall concordance statistic of 0.93 (95% CI, 0.92–0.95) and a polytomous discriminatory index of 0.83 (95% CI, 0.79–0.87).\nThis study demonstrates that an end‐to‐end deep neural network can be accurately trained on unstructured free‐text physician annotations and used to consistently triage 12‐lead ECGs. When further fine‐tuned with other clinical outcomes and externally validated in clinical practice, the demonstrated deep learning–based ECG interpretation can potentially improve time to treatment and decrease healthcare burden.","tags":["medical machine learning"],"title":"Automatic Triage of 12‐Lead ECGs Using Deep Convolutional Neural Networks","type":"publication"},{"authors":["Xiantong Zhen","Yingjun Du","Huan Xiong","Qiang Qiu","Cees G. M. Snoek","Ling Shao"],"categories":[],"content":"","date":1607472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607472000,"objectID":"109a94c9482fec135e7e202ca07a3b85","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/zhen-neurips-2020-/","publishdate":"2020-12-09T00:00:00Z","relpermalink":"/vislab/publication/zhen-neurips-2020-/","section":"publication","summary":"In this paper, we introduce variational semantic memory into meta-learning to acquire long-term knowledge for few-shot learning. The variational semantic memory accrues and stores semantic information for the probabilistic inference of class prototypes in a hierarchical Bayesian framework. The semantic memory is grown from scratch and gradually consolidated by absorbing information from tasks it experiences. By doing so, it is able to accumulate long-term, general knowledge that enables it to learn new concepts of objects. We formulate memory recall as the variational inference of a latent memory variable from addressed contents, which offers a principled way to adapt the knowledge to individual tasks. Our variational semantic memory, as a new long-term memory module, confers principled recall and update mechanisms that enable semantic information to be efficiently accrued and adapted for few-shot learning. Experiments demonstrate that the probabilistic modelling of prototypes achieves a more informative representation of object classes compared to deterministic vectors. The consistent new state-of-the-art performance on four benchmarks shows the benefit of variational semantic memory in boosting few-shot recognition.","tags":["few-shot learning"],"title":"Learning to Learn Variational Semantic Memory","type":"publication"},{"authors":["Victor Zuanazzi","Joris van Vugt","Olaf Booij","Pascal Mettes"],"categories":[],"content":"","date":1606262400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606262400,"objectID":"51a3cf6b1d67744c35375befca76041d","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mettes-3dv-2020/","publishdate":"2020-11-25T00:00:00Z","relpermalink":"/vislab/publication/mettes-3dv-2020/","section":"publication","summary":"This work proposes a metric learning approach for self-supervised scene flow estimation. Scene flow estimation is the task of estimating 3D flow vectors for consecutive 3D point clouds. Such flow vectors are fruitful, \\eg for recognizing actions, or avoiding collisions. Training a neural network via supervised learning for scene flow is impractical, as this requires manual annotations for each 3D point at each new timestamp for each scene. To that end, we seek for a self-supervised approach, where a network learns a latent metric to distinguish between points translated by flow estimations and the target point cloud. Our adversarial metric learning includes a multi-scale triplet loss on sequences of two-point clouds as well as a cycle consistency loss. Furthermore, we outline a benchmark for self-supervised scene flow estimation: the Scene Flow Sandbox. The benchmark consists of five datasets designed to study individual aspects of flow estimation in progressive order of complexity, from a moving object to real-world scenes. Experimental evaluation on the benchmark shows that our approach obtains state-of-the-art self-supervised scene flow results, outperforming recent neighbor-based approaches. We use our proposed benchmark to expose shortcomings and draw insights on various training setups. We find that our setup captures motion coherence and preserves local geometries. Dealing with occlusions, on the other hand, is still an open challenge.","tags":["3D computer vision"],"title":"Adversarial Self-Supervised Scene Flow Estimation","type":"publication"},{"authors":["Mehmet Altinkaya","Arnold Smeulders"],"categories":[],"content":"","date":1602806400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602806400,"objectID":"ad383038f8ef22244b8f2a67f7db793d","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mehmet-mucai-2020/","publishdate":"2020-10-16T00:00:00Z","relpermalink":"/vislab/publication/mehmet-mucai-2020/","section":"publication","summary":"Stuttering affects at least 1% of the world population. It is caused by irregular disruptions in speech production. These interruptions occur in various forms and frequencies. Repetition of words or parts of words, prolongations, or blocks in getting the words out are the most common ones.\nAccurate detection and classification of stuttering would be important in the assessment of severity for speech therapy. Furthermore, real time detection might create many new possibilities to facilitate reconstruction into fluent speech. Such an interface could help people to utilize voice-based interfaces like Apple Siri and Google Assistant, or to make (video) phone calls more fluent by delayed delivery.\nIn this paper we present the first expandable audio-visual database of stuttered speech. We explore an end-to-end, real-time, multi-modal model for detection and classification of stuttered blocks in unbound speech. We also make use of video signals since acoustic signals cannot be produced immediately. We use multiple modalities as acoustic signals together with secondary characteristics exhibited in visual signals will permit an increased accuracy of detection.","tags":["video analysis and understanding"],"title":"A Dynamic, Self Supervised, Large Scale AudioVisual Dataset for Stuttered Speech","type":"publication"},{"authors":["Tessa van der Heiden","Florian Mirus","Herke van Hoof"],"categories":[],"content":"","date":1602720000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602720000,"objectID":"7a1d0045e61f60624f10513956c34bfb","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/tessa-icann-2020-/","publishdate":"2020-10-15T00:00:00Z","relpermalink":"/vislab/publication/tessa-icann-2020-/","section":"publication","summary":"Mobile robot navigation has seen extensive research in the last decades. The aspect of collaboration with robots and humans sharing workspaces will become increasingly important in the future. Therefore, the next generation of mobile robots needs to be socially-compliant to be accepted by their human collaborators. However, a formal definition of compliance is not straightforward. On the other hand, empowerment has been used by artificial agents to learn complicated and generalized actions and also has been shown to be a good model for biological behaviors. In this paper, we go beyond the approach of classical Reinforcement Learning (RL) and provide our agent with intrinsic motivation using empowerment. In contrast to self-empowerment, a robot employing our approach strives for the empowerment of people in its environment, so they are not disturbed by the robot’s presence and motion. In our experiments, we show that our approach has a positive influence on humans, as it minimizes its distance to humans and thus decreases human travel time while moving efficiently towards its own goal. An interactive user-study shows that our method is considered more social than other state-of-the-art approaches by the participants.","tags":["Multi-agent reinforcement learning"],"title":"Social Navigation with Human Empowerment Driven Deep Reinforcement Learning","type":"publication"},{"authors":["Jun Xu","Zhi-Ang Liu","Yingkun Hou","Xiantong Zhen","Ling Shao","Ming-Ming Cheng"],"categories":[],"content":"","date":1602460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602460800,"objectID":"9ab47c6f4bd64260c14c6c97e30bccef","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/xu-tmm-2020/","publishdate":"2020-10-12T00:00:00Z","relpermalink":"/vislab/publication/xu-tmm-2020/","section":"publication","summary":"Recently, image smoothing has gained increasing attention due to its prerequisite role in other image processing tasks, e.g., image enhancement and editing. However, the evaluation of image smoothing algorithms is usually performed by subjective observation on images without corresponding ground truths. To promote the development of image smoothing algorithms, in this paper, we construct a novel Nankai Smoothing (NKS) dataset containing 200 images blended by versatile structure images and natural textures. The structure images are inherently smooth and naturally taken as ground truths. On our NKS dataset, we comprehensively evaluate 14 popular image smoothing algorithms. Moreover, we propose a Pixel-level Non-Local Smoothing (PNLS) method to well preserve the structure of the smoothed images, by exploiting the pixel-level non-local self-similarity prior of natural images. Extensive experiments on several benchmark datasets demonstrate that our PNLS outperforms previous algorithms on the image smoothing task. Ablation studies also reveal the work mechanism of our PNLS on image smoothing. To further show its effectiveness, we apply our PNLS on several applications such as semantic region smoothing, detail/edge enhancement, and image abstraction. The dataset and code are available at https://github.com/zal0302/PNLS.","tags":["image restoration"],"title":"Pixel-level non-local image smoothing with objective evaluation","type":"publication"},{"authors":["Iris Groen","Giovanni Piantoni","Adeen Flinker","Sasha Devore","Orrin Devinsky","Werner Doyle","Nick Ramsey","Natalia Petridou","Jonathan Winawer"],"categories":[],"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"9994a72d6c3961a6892298569ca8e392","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/groen-jov-2020/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/vislab/publication/groen-jov-2020/","section":"publication","summary":"Cortical responses to visual stimuli exhibit complex temporal dynamics, including sub-additive temporal summation, response reduction with repeated or sustained stimuli (adaptation), and slower dynamics at low contrast. Multiple computational models have been proposed to account for these dynamics in several measurement domains, including single-cell recordings, psychophysics, and fMRI. It is challenging to compare these models because there are differences in model form, test stimuli, and instrument. Here we present a new dataset that is well-suited to compare models of neural temporal dynamics. The dataset is from electrocorticographic (ECoG) recordings of human visual cortex, which measures cortical neural population responses with high spatial and temporal precision. The stimuli were large, static contrast patterns and varied systematically in contrast, duration, and inter-stimulus interval (ISI). Time-varying broadband responses were computed using the power envelope of the band-pass filtered voltage time course (50-170 Hz) recorded from a total of 126 electrodes in ten epilepsy patients, covering earlier (V1-V4) and higher-order (LO, TO, IPS) retinotopic maps. In all visual regions, the ECoG broadband responses show several non-linear features: peak response amplitude saturates with high contrast and long stimulus durations; response latency decreases with increasing contrast; and the response to a second stimulus is suppressed for short ISIs and recovers for longer ISIs. These features were well predicted by a computational model (Zhou, Benson, Kay and Winawer, 2019) comprised of a small set of canonical neuronal operations: linear filtering, rectification, exponentiation, and a delayed divisive gain control. These results demonstrate that a simple computational model comprised of canonical neuronal computations captures a wide range of temporal and contrast-dependent neuronal dynamics at millisecond resolution. Finally, we present a software repository that implements models of temporal dynamics in a modular fashion, enabling the comparison of many models fit to the same data and analyzed with the same methods.","tags":["human vision"],"title":"Modeling the temporal dynamics of neural responses in human visual cortex","type":"publication"},{"authors":["Ivan Sosnovik","Michał Szmaja","Arnold Smeulders"],"categories":[],"content":"","date":1600961939,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600961939,"objectID":"5e0fa4a3d95cc0190c237842aca2ab37","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/sosnovik-iclr-2020/","publishdate":"2020-09-24T17:38:59+02:00","relpermalink":"/vislab/publication/sosnovik-iclr-2020/","section":"publication","summary":"The effectiveness of Convolutional Neural Networks (CNNs) has been substantially attributed to their built-in property of translation equivariance. However, CNNs do not have embedded mechanisms to handle other types of transformations. In this work, we pay attention to scale changes, which regularly appear in various tasks due to the changing distances between the objects and the camera. First, we introduce the general theory for building scale-equivariant convolutional networks with steerable filters. We develop scale-convolution and generalize other common blocks to be scale-equivariant. We demonstrate the computational efficiency and numerical stability of the proposed method. We compare the proposed models to the previously developed methods for scale equivariance and local scale invariance. We demonstrate state-of-the-art results on the MNIST-scale dataset and on the STL-10 dataset in the supervised learning setting.","tags":["Symmetry in Computer Vision"],"title":"Scale-Equivariant Steerable Networks","type":"publication"},{"authors":["Anna Gaglianese","Mariana P. Branco","Iris I. A. Groen","Noah C. Benson","Mariska J. Vansteensel","Micah M. Murray","Natalia Petridou","Nick F. Ramsey"],"categories":[],"content":"","date":1600905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600975479,"objectID":"b9fed072f6d6248a183b35eff1086f06","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/gaglianese-brain-2020/","publishdate":"2020-09-23T19:24:39.771118Z","relpermalink":"/vislab/publication/gaglianese-brain-2020/","section":"publication","summary":"There is ongoing debate regarding the extent to which human cortices are specialized for processing a given sensory input versus a given type of information, independently of the sensory source. Many neuroimaging and electrophysiological studies have reported that primary and extrastriate visual cortices respond to tactile and auditory stimulation, in addition to visual inputs, suggesting these cortices are intrinsically multisensory. In particular for tactile responses, few studies have proven neuronal processes in visual cortex in humans. Here, we assessed tactile responses in both low-level and extrastriate visual cortices using electrocorticography recordings in a human participant. Specifically, we observed significant spectral power increases in the high frequency band (30–100 Hz) in response to tactile stimuli, reportedly associated with spiking neuronal activity, in both low-level visual cortex (i.e. V2) and in the anterior part of the lateral occipital–temporal cortex. These sites were both involved in processing tactile information and responsive to visual stimulation. More generally, the present results add to a mounting literature in support of task-sensitive and sensory-independent mechanisms underlying functions like spatial, motion, and self-processing in the brain and extending from higher-level as well as to low-level cortices.","tags":["human sensory perception"],"title":"Electrocorticography Evidence of Tactile Responses in Visual Cortices","type":"publication"},{"authors":["Noor Seijdel","Sara Jahfari","Iris I.A. Groen","H.S. Scholte"],"categories":[],"content":"","date":1600905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600975479,"objectID":"3039cb49f0ed865f16056bd39d959d4a","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/seijdel-scientific_report-2020/","publishdate":"2020-08-23T19:24:39.771118Z","relpermalink":"/vislab/publication/seijdel-scientific_report-2020/","section":"publication","summary":"A fundamental component of interacting with our environment is gathering and interpretation of sensory information. When investigating how perceptual information influences decision-making, most researchers have relied on manipulated or unnatural information as perceptual input, resulting in findings that may not generalize to real-world scenes. Unlike simplified, artificial stimuli, real-world scenes contain low-level regularities that are informative about the structural complexity, which the brain could exploit. In this study, participants performed an animal detection task on low, medium or high complexity scenes as determined by two biologically plausible natural scene statistics, contrast energy (CE) or spatial coherence (SC). In experiment 1, stimuli were sampled such that CE and SC both influenced scene complexity. Diffusion modelling showed that the speed of information processing was affected by low-level scene complexity. Experiment 2a/b refined these observations by showing how isolated manipulation of SC resulted in weaker but comparable effects, with an additional change in response boundary, whereas manipulation of only CE had no effect. Overall, performance was best for scenes with intermediate complexity. Our systematic definition quantifies how natural scene complexity interacts with decision-making.We speculate that CE and SC serve as an indication to adjust perceptual decision-making based on the complexity of the input.","tags":["human vision"],"title":"Low‐level image statistics in natural scenes infuence perceptual decision‐making","type":"publication"},{"authors":["Sadaf Gulshad","Arnold Smeulders"],"categories":[],"content":"","date":1600616339,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600616339,"objectID":"235fc49128ebecb2538c0e6c292133c7","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/gulshad-icmr-2020/","publishdate":"2020-09-21T17:38:59+02:00","relpermalink":"/vislab/publication/gulshad-icmr-2020/","section":"publication","summary":"In this paper, we aim to explain the decisions of neural networks by utilizing multimodal information. That is counter-intuitive attributes and counter visual examples which appear when perturbed samples are introduced. Different from previous work on interpreting decisions using saliency maps, text, or visual patches we propose to use attributes and counter-attributes, and examples and counter-examples as part of the visual explanations. When humans explain visual decisions they tend to do so by providing attributes and examples. Hence, inspired by the way of human explanations in this paper we provide attribute-based and example-based explanations. Moreover, humans also tend to explain their visual decisions by adding counter-attributes and counter-examples to explain what is not seen. We introduce directed perturbations in the examples to observe which attribute values change when classifying the examples into the counter classes. This delivers intuitive counter-attributes and counter-examples. Our experiments with both coarse and fine-grained datasets show that attributes provide discriminating and human-understandable intuitive and counter-intuitive explanations.","tags":["explainable AI","classification","attributes","adversarial examples"],"title":"Explaining with Counter Visual Attributes and Examples","type":"publication"},{"authors":["Shuo Chen","Pascal Mettes","Tao Hu","Cees G.M. Snoek"],"categories":[],"content":"","date":1600616339,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600616339,"objectID":"3548cc37a0533a28aad17f02aa8674a9","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/chen-icmr-2020/","publishdate":"2020-08-22T17:38:59+02:00","relpermalink":"/vislab/publication/chen-icmr-2020/","section":"publication","summary":"This paper introduces spatio-temporal interactivity proposals for video surveillance. Rather than focusing solely on actions performed by subjects, we explicitly include the objects that the subjects interact with. To enable interactivity proposals, we introduce the notion of interactivityness, a score that reflects the likelihood that a subject and object have an interplay. For its estimation, we propose a network containing an interactivity block and geometric encoding between subjects and objects. The network computes local interactivity likelihoods from subject and object trajectories, which we use to link intervals of high scores into spatio-temporal proposals. Experiments on an interactivity dataset with new evaluation metrics show the general benefit of interactivity proposals as well as its favorable performance compared to traditional temporal and spatio-temporal action proposals.","tags":["action recognition"],"title":"Interactivity Proposals for Surveillance Videos","type":"publication"},{"authors":["Yingjun Du","Jun Xu","Huan Xiong","Qiang Qiu","Xiantong Zhen","Cees G. M. Snoek","Ling Shao"],"categories":[],"content":"","date":1600616339,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600616339,"objectID":"7207b3928478fef4225df5b637d88454","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/du-eccv-2020/","publishdate":"2020-08-22T17:38:59+02:00","relpermalink":"/vislab/publication/du-eccv-2020/","section":"publication","summary":"Domain generalization models learn to generalize to previously unseen domains, but suffer from prediction uncertainty and domain shift. In this paper, we address both problems. We introduce a probabilistic meta-learning model for domain generalization, in which classifier parameters shared across domains are modeled as distributions. This enables better handling of prediction uncertainty on unseen domains. To deal with domain shift, we learn domain-invariant representations by the proposed principle of meta variational information bottleneck, we call MetaVIB. MetaVIB is derived from novel variational bounds of mutual information, by leveraging the meta-learning setting of domain generalization. Through episodic training, MetaVIB learns to gradually narrow domain gaps to establish domain-invariant representations, while simultaneously maximizing prediction accuracy. We conduct experiments on three benchmarks for cross-domain visual recognition. Comprehensive ablation studies validate the benefits of MetaVIB for domain generalization. The comparison results demonstrate our method outperforms previous approaches consistently.","tags":["transfer learning"],"title":"Learning to Learn with Variational Information Bottleneck for Domain Generalization","type":"publication"},{"authors":["Sanath Narayan","Akshita Gupta","Fahad Khan","Cees G. M. Snoek","Ling Shao"],"categories":[],"content":"","date":1600560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600646400,"objectID":"3827a577161301502e8dfe03ffaad2c9","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/narayan-eccv-2020/","publishdate":"2020-09-20T00:00:00Z","relpermalink":"/vislab/publication/narayan-eccv-2020/","section":"publication","summary":"Zero-shot learning strives to classify unseen categories for which no data is available during training. In the generalized variant, the test samples can further belong to seen or unseen categories. The state-of-the-art relies on Generative Adversarial Networks that synthesize unseen class features by leveraging class-specific semantic embeddings. During training, they generate semantically consistent features, but discard this constraint during feature synthesis and classification. We propose to enforce semantic consistency at all stages of (generalized) zero-shot learning: training, feature synthesis and classification. We further introduce a feedback loop, from a semantic embedding decoder, that iteratively refines the generated features during both the training and feature synthesis stages. The synthesized features together with their corresponding latent embeddings from the decoder are transformed into discriminative features and utilized during classification to reduce ambiguities among categories. Experiments on (generalized) zero-shot learning for object and action classification reveal the benefit of semantic consistency and iterative feedback, outperforming existing methods on six zero-shot learning benchmarks.","tags":["zero-shot learning"],"title":"Latent Embedding Feedback and Discriminative Features for Zero-Shot Classification","type":"publication"},{"authors":["Xiantong Zhen","Haoliang Sun","Yingjun Du","Jun Xu","Yilong Yin","Ling Shao","Cees Snoek"],"categories":[],"content":"","date":1600560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600560000,"objectID":"b49c126ce56674265f69d7d8029f6dca","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/zhen-icml-2020-/","publishdate":"2020-09-20T00:00:00Z","relpermalink":"/vislab/publication/zhen-icml-2020-/","section":"publication","summary":"In this work, we introduce kernels with random Fourier features in the meta-learning framework to leverage their strong few-shot learning ability. We propose meta variational random features (MetaVRF) to learn adaptive kernels for the base-learner, which is developed in a latent variable model by treating the random feature basis as the latent variable. We formulate the optimization of MetaVRF as a variational inference problem by deriving an evidence lower bound under the meta-learning framework. To incorporate shared knowledge from related tasks, we propose a context inference of the posterior, which is established by an LSTM architecture. The LSTM-based inference network can effectively integrate the context information of previous tasks with task-specific information, generating informative and adaptive features. The learned MetaVRF can produce kernels of high representational power with a relatively low spectral sampling rate and also enables fast adaptation to new tasks. Experimental results on a variety of few-shot regression and classification tasks demonstrate that MetaVRF delivers much better, or at least competitive, performance compared to existing meta-learning alternatives.","tags":["few-shot learning"],"title":"Learning to Learn Kernels with Variational Random Features","type":"publication"},{"authors":["Jiaojiao Zhao","Jungong Han","Ling Shao","Cees G. M. Snoek"],"categories":[],"content":"","date":1600560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600560000,"objectID":"aa6bd9c71170c9ac0e7e9bdbb01bea12","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/zhao-ijcv-2020/","publishdate":"2020-08-23T19:24:42.673924Z","relpermalink":"/vislab/publication/zhao-ijcv-2020/","section":"publication","summary":"While many image colorization algorithms have recently shown the capability of producing plausible color versions from gray-scale photographs, they still suffer from limited semantic understanding. To address this shortcoming, we propose to exploit pixelated object semantics to guide image colorization. The rationale is that human beings perceive and distinguish colors based on the semantic categories of objects. Starting from an autoregressive model, we generate image color distributions, from which diverse colored results are sampled. We propose two ways to incorporate object semantics into the colorization model: through a pixelated semantic embedding and a pixelated semantic generator. Specifically, the proposed convolutional neural network includes two branches. One branch learns what the object is, while the other branch learns the object colors. The network jointly optimizes a color embedding loss, a semantic segmentation loss and a color generation loss, in an end-to-end fashion. Experiments on PASCAL VOC2012 and COCO-stuff reveal that our network, when trained with semantic segmentation labels, produces more realistic and finer results compared to the colorization state-of-the-art.","tags":["image colorization"],"title":"Pixelated Semantic Colorization","type":"publication"},{"authors":["Pim Dijt","Pascal Mettes"],"categories":[],"content":"","date":1599868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599868800,"objectID":"2d7f71d4c5fdb66a48129db9e784d928","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/dijt-icmr-2020/","publishdate":"2020-08-22T17:44:27+02:00","relpermalink":"/vislab/publication/dijt-icmr-2020/","section":"publication","summary":"This work investigates the anticipation of future ship locations based on multimodal sensors. Predicting future trajectories of ships is an important component for the development of safe autonomous sailing ships on water. A core challenge towards future trajectory prediction is making sense of multiple modalities from vastly dif- ferent sensors, including GPS coordinates, radar images, and charts specifying water and land regions. To that end, we propose a Tra- jectory Prediction Network, an end-to-end approach for trajectory anticipation based on multimodal sensors. Our approach is framed as a multi-task sequence-to-sequence network, with network com- ponents for coordinate sequences and radar images. In the network, water/land segmentations from charts are integrated as an auxil- iary training objective. Since future anticipation of ships has not previously been studied from such a multimodal perspective, we introduce the Inland Shipping Dataset (ISD), a novel dataset for future anticipation of ships. Experimental evaluation on ISD shows the potential of our approach, outperforming single-modal variants and baselines from related anticipation tasks.","tags":["motion and tracking"],"title":"Trajectory Prediction Network for Future Anticipation of Ships","type":"publication"},{"authors":["Petr Byvshev","Pascal Mettes","Yu Xiao"],"categories":[],"content":"","date":1599782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599782400,"objectID":"28d6180368c8e2796d9a1139fdc02140","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/byvshev-icmr-2020/","publishdate":"2020-08-22T17:44:27+02:00","relpermalink":"/vislab/publication/byvshev-icmr-2020/","section":"publication","summary":"In this work, we investigate activity recognition using mul- timodal inputs from heterogeneous sensors. Activity recog- nition is commonly tackled from a single-modal perspective using videos. In case multiple signals are used, they come from the same homogeneous modality, e.g. in the case of color and optical flow. Here, we propose an activity network that fuses multimodal inputs coming from completely different and het- erogeneous sensors. We frame such a heterogeneous fusion as a non-local operation. The observation is that in a non-local operation, only the channel dimensions need to match. In the network, heterogeneous inputs are fused, while maintaining the shapes and dimensionalities that fit each input. We outline both asymmetric fusion, where one modality serves to enforce the other, and symmetric fusion variants. To further promote research into multimodal activity recognition, we introduce GloVid, a first-person activity dataset captured with video recordings and smart glove sensor readings. Experiments on GloVid show the potential of heterogeneous non-local fusion for activity recognition, outperforming individual modalities and standard fusion techniques.","tags":["action recognition"],"title":"Heterogeneous Non-Local Fusion for Multimodal Activity Recognition","type":"publication"},{"authors":["William Thong","Pascal Mettes","C. G. M. Snoek"],"categories":[],"content":"","date":1599782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210679,"objectID":"d0369eeb3dfdac22b7359f830ff7eeae","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/thong-cviu-2020/","publishdate":"2020-08-23T19:24:39.771118Z","relpermalink":"/vislab/publication/thong-cviu-2020/","section":"publication","summary":"This paper addresses cross-domain visual search, where visual queries retrieve category samples from a different domain. For example, we may want to sketch an airplane and retrieve photographs of airplanes. Despite considerable progress, the search occurs in a closed setting between two pre-defined domains. In this paper, we make the step towards an open setting where multiple visual domains are available. This notably translates into a search between any pair of domains, from a combination of domains or within multiple domains. We introduce a simple -- yet effective -- approach. We formulate the search as a mapping from every visual domain to a common semantic space, where categories are represented by hyperspherical prototypes. Open cross-domain visual search is then performed by searching in the common semantic space, regardless of which domains are used as source or target. Domains are combined in the common space to search from or within multiple domains simultaneously. A separate training of every domain-specific mapping function enables an efficient scaling to any number of domains without affecting the search performance. We empirically illustrate our capability to perform open cross-domain visual search in three different scenarios. Our approach is competitive with respect to existing closed settings, where we obtain state-of-the-art results on several benchmarks for three sketch-based search tasks.","tags":["image retrieval"],"title":"Open Cross-Domain Visual Search","type":"publication"},{"authors":["William Thong","C. G. M. Snoek"],"categories":[],"content":"","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210679,"objectID":"783617514794f200d18f90a547fd0663","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/thong-bmvc-2020/","publishdate":"2020-08-23T19:24:39.771118Z","relpermalink":"/vislab/publication/thong-bmvc-2020/","section":"publication","summary":"Generalized zero-shot learning recognizes inputs from both seen and unseen classes. Yet, existing methods tend to be biased towards the classes seen during training. In this paper, we strive to mitigate this bias. We propose a bias-aware learner to map inputs to a semantic embedding space for generalized zero-shot learning. During training, the model learns to regress to real-valued class prototypes in the embedding space with temperature scaling, while a margin-based bidirectional entropy term regularizes seen and unseen probabilities. Relying on a real-valued semantic embedding space provides a versatile approach, as the model can operate on different types of semantic information for both seen and unseen classes. Experiments are carried out on four benchmarks for generalized zero-shot learning and demonstrate the benefits of the proposed bias-aware classifier, both as a stand-alone method or in combination with generated features.","tags":["zero-shot learning"],"title":"Bias-Awareness for Zero-Shot Learning the Seen and Unseen","type":"publication"},{"authors":["P. S. M. Mettes","Dennis Koelma","C. G. M. Snoek"],"categories":[],"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210679,"objectID":"cd61133852a2f52d6ee1bb9369e2bfa3","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mettes-tomm-2020/","publishdate":"2020-08-23T19:24:39.082799Z","relpermalink":"/vislab/publication/mettes-tomm-2020/","section":"publication","summary":"This article aims for the detection and search of events in videos, where video examples are either scarce or even absent during training. To enable such event detection and search, ImageNet concept banks have shown to be effective. Rather than employing the standard concept bank of 1,000 ImageNet classes, we leverage the full 21,841-class dataset. We identify two problems with using the full dataset: (i) there is an imbalance between the number of examples per concept, and (ii) not all concepts are equally relevant for events. In this article, we propose to balance large-scale image hierarchies for pre-training. We shuffle concepts based on bottom-up and top-down operations to overcome the problems of example imbalance and concept relevance. Using this strategy, we arrive at the shuffled ImageNet bank, a concept bank with an order of magnitude more concepts compared to standard ImageNet banks. Compared to standard ImageNet pre-training, our shuffles result in more discriminative representations to train event models from the limited video event examples. For event search, the broad range of concepts enable a closer match between textual queries of events and concept detections in videos. Experimentally, we show the benefit of the proposed bank for event detection and event search, with state-of-the-art performance for both tasks on the challenging TRECVID Multimedia Event Detection and Ad-Hoc Video Search benchmarks.","tags":["action recognition"],"title":"Shuffled ImageNet-Banks for Video Event Detection and Search","type":"publication"},{"authors":[],"categories":[],"content":"","date":1598112967,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598112967,"objectID":"72d2283e206481befe589d104be63ba6","permalink":"https://ivi.fnwi.uva.nl/vislab/post/eccv20/","publishdate":"2020-08-22T18:16:07+02:00","relpermalink":"/vislab/post/eccv20/","section":"post","summary":"","tags":[],"title":"Four papers were accepted by ECCV 2020!","type":"post"},{"authors":["Pengwan Yang","Vincent Tao Hu","Pascal Mettes","Cees G. M. Snoek"],"categories":[],"content":"","date":1598111067,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598111067,"objectID":"1109f823badbe7fb3ba154eb5374994c","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/focal/","publishdate":"2020-08-22T17:44:27+02:00","relpermalink":"/vislab/publication/focal/","section":"publication","summary":"This paper strives to localize the temporal extent of an action in a long untrimmed video. Where existing work leverages many examples with their start, their ending, and/or the class of the action during training time, we propose few-shot common action localization. The start and end of an action in a long untrimmed video is determined based on just a hand-full of trimmed video examples containing the same action, without knowing their common class label. To address this task, we introduce a new 3D convolutional network architecture able to align representations from the support videos with the relevant query video segments. The network contains: (\textit{i}) a mutual enhancement module to simultaneously complement the representation of the few trimmed support videos and the untrimmed query video; (\textit{ii}) a progressive alignment module that iteratively fuses the support videos into the query branch; and (\textit{iii}) a pairwise matching module to weigh the importance of different support videos. Evaluation of few-shot common action localization in untrimmed videos containing a single or multiple action instances demonstrates the effectiveness and general applicability of our proposal.","tags":["few-shot learning","action localization"],"title":"Localizing the Common Action Among a Few Videos","type":"publication"},{"authors":["Yunlu Chen","Vincent Tao Hu","Efstratios Gavves","Thomas Mensink","Pascal Mettes","Pengwan Yang","Cees G. M. Snoek"],"categories":[],"content":"","date":1598110739,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598110739,"objectID":"c1d7663732c68ad5330b89e08886421c","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/pointmixup/","publishdate":"2020-08-22T17:38:59+02:00","relpermalink":"/vislab/publication/pointmixup/","section":"publication","summary":"This paper introduces data augmentation for point clouds by interpolation between examples. Data augmentation by interpolation has shown to be a simple and effective approach in the image domain. Such a mixup is however not directly transferable to point clouds, as we do not have a one-to-one correspondence between the points of two different objects. In this paper, we define data augmentation between point clouds as a shortest path linear interpolation. To that end, we introduce PointMixup, an interpolation method that generates new examples through an optimal assignment of the path function between two point clouds. We prove that our PointMixup finds the shortest path between two point clouds and that the interpolation is assignment invariant and linear. With the definition of interpolation, PointMixup allows to introduce strong interpolation-based regularizers such as mixup and manifold mixup to the point cloud domain. Experimentally, we show the potential of PointMixup for point cloud classification, especially when examples are scarce, as well as increased robustness to noise and geometric transformations to points. The code for PointMixup and the experimental details are publicly available.","tags":["few-shot learning","cloud point"],"title":"PointMixup: Augmentation for Point Clouds","type":"publication"},{"authors":["Adeel Pervez","Taco Cohen","Efstratios Gavves"],"categories":[],"content":"","date":1595030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595030400,"objectID":"cf2e6c81fd99eb1fa93eba91706b0c62","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/adeel-icml-2020-/","publishdate":"2020-07-18T00:00:00Z","relpermalink":"/vislab/publication/adeel-icml-2020-/","section":"publication","summary":"Stochastic neural networks with discrete random variables are an important class of models for their expressiveness and interpretability. Since direct differentiation and backpropagation is not possible, Monte Carlo gradient estimation techniques are a popular alternative. Efficient stochastic gradient estimators, such Straight-Through and Gumbel-Softmax, work well for shallow stochastic models. Their performance, however, suffers with hierarchical, more complex models. We focus on stochastic networks with Boolean latent variables. To analyze such networks, we introduce the framework of harmonic analysis for Boolean functions to derive an analytic formulation for the bias and variance in the Straight-Through estimator. Exploiting these formulations, we propose\\emph {FouST}, a low-bias and low-variance gradient estimation algorithm that is just as efficient. Extensive experiments show that FouST performs favorably compared to state-of-the-art biased estimators and is much faster than unbiased ones.","tags":["Gradient Estimation"],"title":"Low Bias Low Variance Gradient Estimates for Boolean Stochastic Networks","type":"publication"},{"authors":["Andreas Panteli","Deepak K. Gupta","Nathan Bruijn","Efstratios Gavves"],"categories":[],"content":"","date":1593993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593993600,"objectID":"8c199939d3a12f147086fd9ca60ca32e","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/andreas-midl-2020-/","publishdate":"2020-07-06T00:00:00Z","relpermalink":"/vislab/publication/andreas-midl-2020-/","section":"publication","summary":"Tracking and segmentation of biological cells in video sequences is a challenging problem, especially due to the similarity of the cells and high levels of inherent noise. Most machine learning-based approaches lack robustness and suffer from sensitivity to less prominent events such as mitosis, apoptosis and cell collisions. Due to the large variance in medical image characteristics, most approaches are dataset-specific and do not generalise well on other datasets. In this paper, we propose a simple end-to-end cascade neural architecture that can effectively model the movement behaviour of biological cells and predict collision and mitosis events. Our approach uses U-Net for an initial segmentation which is then improved through processing by a siamese tracker capable of matching each cell along the temporal axis. By facilitating the re-segmentation of collided and mitotic cells, our method demonstrates its capability to handle volatile trajectories and unpredictable cell locations while being invariant to cell morphology. We demonstrate that our tracking approach achieves state-of-the-art results on PhC-C2DL-PSC and Fluo-N2DH-SIM+ datasets and ranks second on the DIC-C2DH-HeLa dataset of the cell tracking challenge benchmarks.","tags":["motion and tracking"],"title":"Siamese Tracking of Cell Behaviour Patterns","type":"publication"},{"authors":["Mengshi Qi","Jie Qin","Xiantong Zhen","Di Huang","Yi Yang","Jiebo Luo"],"categories":[],"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"8827806ce98ced8f64a9b20136d3017c","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/qi-acmmm-2020/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/vislab/publication/qi-acmmm-2020/","section":"publication","summary":"In the era of big data, few-shot learning has recently received much attention in multimedia analysis and computer vision due to its appealing ability of learning from scarce labeled data. However, it has been largely underdeveloped in the video domain, which is even more challenging due to the huge spatial-temporal variability of video data. In this paper, we address few-shot video classification by learning an ensemble of SlowFast networks augmented with memory units. Specifically, we introduce a family of few-shot learners based on SlowFast networks which are used to extract informative features at multiple rates, and we incorporate a memory unit into each network to enable encoding and retrieving crucial information instantly. Furthermore, we propose a choice controller network to leverage the diversity of few-shot learners by learning to adaptively assign a confidence score to each SlowFast memory network, leading to a strong classifier for enhanced prediction. Experimental results on two widely-adopted video datasets demonstrate the effectiveness of the proposed method, as well as its superior performance over the state-of-the-art approaches.","tags":["action recognition"],"title":"Few-Shot Ensemble Learning for Video Classification with SlowFast Memory Networks","type":"publication"},{"authors":["Haochen Wang","Xudong Zhang","Yutao Hu","Yandan Yang","Xianbin Cao","Xiantong Zhen"],"categories":[],"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"7dbe5127ff07662940da64e6d24cb4d0","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/wang-eccv-2020/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/vislab/publication/wang-eccv-2020/","section":"publication","summary":"Few-shot segmentation has recently generated great popularity, addressing the challenging yet important problem of segmenting objects from unseen categories with scarce annotated support images. The crux of few-shot segmentation is to extract object information from the support image and then propagate it to guide the segmentation of query images. In this paper, we propose the Democratic Attention Network (DAN) for few-shot semantic segmentation. We introduce the democratized graph attention mechanism, which can activate more pixels on the object to establish a robust correspondence between support and query images. Thus, the network is able to propagate more guiding information of foreground objects from support to query images, enhancing its robustness and generalizability to new objects. Furthermore, we propose multi-scale guidance by designing a refinement fusion unit to fuse features from intermediate layers for the segmentation of the query image. This offers an efficient way of leveraging multi-level semantic information to achieve more accurate segmentation. Extensive experiments on three benchmarks demonstrate that the proposed DAN achieves the new state-of-the-art performance, surpassing the previous methods by large margins. The thorough ablation studies further reveal its great effectiveness for few-shot semantic segmentation.","tags":["few-shot learning"],"title":"Few-Shot Semantic Segmentation with Democratic Attention Networks","type":"publication"},{"authors":["M. Jain","A. Ghodrati","C. G. M. Snoek"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210677,"objectID":"bb61ec06785b50f958cde3d9833668fe","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/jain-cvpr-2020/","publishdate":"2020-08-23T19:24:36.936261Z","relpermalink":"/vislab/publication/jain-cvpr-2020/","section":"publication","summary":"This paper tackles the problem of localizing actions in long untrimmed videos. Different from existing works, which all use annotated untrimmed videos during training, we learn only from short trimmed videos. This enables learning from large-scale datasets originally designed for action classification. We propose a method to train an action localization network that segments a video into interpretable fragments, we call ActionBytes. Our method jointly learns to cluster ActionBytes and trains the localization network using the cluster assignments as pseudolabels. By doing so, we train on short trimmed videos that become untrimmed for ActionBytes. In isolation, or when merged, the ActionBytes also serve as effective action proposals. Experiments demonstrate that our boundary-guided training generalizes to unknown action classes and localizes actions in long videos of Thumos14, MultiThumos, and ActivityNet1.2. Furthermore, we show the advantage of ActionBytes for zero-shot localization as well as traditional weakly supervised localization, that train on long videos, to achieve state-of-the-art results.","tags":["action localization"],"title":"ActionBytes: Learning from Trimmed Videos to Localize Actions","type":"publication"},{"authors":["K. Gavrilyuk","R. Sanford","M. Javan","C. G. M. Snoek"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210676,"objectID":"045fc2e0e8523acd9b40b2f6da65edf4","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/gavrilyuk-cvpr-2020/","publishdate":"2020-08-23T19:24:36.799943Z","relpermalink":"/vislab/publication/gavrilyuk-cvpr-2020/","section":"publication","summary":"This paper strives to recognize individual actions and group activities from videos. While existing solutions for this challenging problem explicitly model spatial and temporal relationships based on location of individual actors, we propose an actor-transformer model able to learn and selectively extract information relevant for group activity recognition. We feed the transformer with rich actorspecific static and dynamic representations expressed by features from a 2D pose network and 3D CNN, respectively. We empirically study different ways to combine these representations and show their complementary benefits. Experiments show what is important to transform and how it should be transformed. What is more, actor-transformers achieve state-of-the-art results on two publicly available benchmarks for group activity recognition, outperforming the previous best published results by a considerable margin.","tags":["action recognition"],"title":"Actor-Transformers for Group Activity Recognition","type":"publication"},{"authors":["T. F. H. Runia","K. Gavrilyuk","C. G. M. Snoek","A. W. M. Smeulders"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210677,"objectID":"42f62771777dc6b62e4c2f9fec174277","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/runia-cvpr-2020/","publishdate":"2020-08-23T19:24:37.316395Z","relpermalink":"/vislab/publication/runia-cvpr-2020/","section":"publication","summary":"For many of the physical phenomena around us, we have developed sophisticated models explaining their behavior. Nevertheless, measuring physical properties from visual observations is challenging due to the high number of causally underlying physical parameters – including material properties and external forces. In this paper, we propose to measure latent physical properties for cloth in the wind without ever having seen a real example before. Our solution is an iterative refinement procedure with simulation at its core. The algorithm gradually updates the physical model parameters by running a simulation of the observed phenomenon and comparing the current simulation to a real-world observation. The correspondence is measured using an embedding function that maps physically similar examples to nearby points. We consider a case study of cloth in the wind, with curling flags as our leading example – a seemingly simple phenomena but physically highly involved. Based on the physics of cloth and its visual manifestation, we propose an instantiation of the embedding function. For this mapping, modeled as a deep network, we introduce a spectral layer that decomposes a video volume into its temporal spectral power and corresponding frequencies. Our experiments demonstrate that the proposed method compares favorably to prior work on the task of measuring cloth material properties and external wind force from a real-world video.","tags":[],"title":"Cloth in the Wind: A Case Study of Physical Measurement through Simulation","type":"publication"},{"authors":["V. Escorcia","D. Cuong","M. Jain","B. Ghanem","C. G. M. Snoek"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210676,"objectID":"c1c1d10ba5f11646317772072c40507e","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/escorcia-cviu-2020/","publishdate":"2020-08-23T19:24:36.544862Z","relpermalink":"/vislab/publication/escorcia-cviu-2020/","section":"publication","summary":"This paper addresses the problem of spatiotemporal localization of actions in videos. Compared to leading approaches, which all learn to localize based on carefully annotated boxes on training video frames, we adhere to a solution only requiring video class labels. We introduce an actor-supervised architecture that exploits the inherent compositionality of actions in terms of actor transformations, to localize actions. We make two contributions. First, we propose actor proposals derived from a detector for human and non-human actors intended for images, which are linked over time by Siamese similarity matching to account for actor deformations. Second, we propose an actor-based attention mechanism enabling localization from action class labels and actor proposals. It exploits a new actor pooling operation and is end-to-end trainable. Experiments on four action datasets show actor supervision is state-of-the-art for action localization from video class labels and is even competitive to some box-supervised alternatives.","tags":["action localization"],"title":"Guess Where? Actor-Supervision for Spatiotemporal Video Action Localization","type":"publication"},{"authors":["T. Long","P. S. M. Mettes","H. T. Shen","C. G. M. Snoek"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210677,"objectID":"dbc2fd05bd0f709f32baade75c43a37a","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/long-cvpr-2020/","publishdate":"2020-08-23T19:24:37.187167Z","relpermalink":"/vislab/publication/long-cvpr-2020/","section":"publication","summary":"In this paper, we introduce hierarchical action search. Starting from the observation that hierarchies are mostly ignored in the action literature, we retrieve not only individual actions but also relevant and related actions, given an action name or video example as input. We propose a hyperbolic action network, which is centered around a hyperbolic space shared by action hierarchies and videos. Our discriminative hyperbolic embedding projects actions on the shared space while jointly optimizing hypernymhyponym relations between action pairs and a large margin separation between all actions. The projected actions serve as hyperbolic prototypes that we match with projected video representations. The result is a learned space where videos are positioned in entailment cones formed by different subtrees. To perform search in this space, we start from a query and increasingly enlarge its entailment cone to retrieve hierarchically relevant action videos. Experiments on three action datasets with new hierarchy annotations show the effectiveness of our approach for hierarchical action search by name and by video example, regardless of whether queried actions have been seen or not during training.","tags":["video retrieval","hyperbolic learning","zero-shot learning","action recognition"],"title":"Searching for Actions on the Hyperbole","type":"publication"},{"authors":["P. O'Connor","E. Gavves","M. Welling"],"categories":[],"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"0c071df4d301d173b78bef9c9d625a4c","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/o-connor-icais-2019/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/vislab/publication/o-connor-icais-2019/","section":"publication","summary":"Backpropagation is almost universally used to train artificial neural networks. However, there are several reasons that backpropagation could not be plausibly implemented by biological neurons. Among these are the facts that (1) biological neurons appear to lack any mechanism for sending gradients backwards across synapses, and (2) biological “spiking” neurons emit binary signals, whereas back-propagation requires that neurons communicate continuous values between one another. Recently, Scellier and Bengio [2017], demonstrated an alternative to backpropagation, called Equilibrium Propagation, wherein gradients are implicitly computed by the dynamics of the neural network, so that neurons do not need an internal mechanism for backpropagation of gradients. This provides an interesting solution to problem (1). In this paper, we address problem (2) by proposing a way in which Equilibrium Propagation can be implemented with neurons which are constrained to just communicate binary values at each time step. We show that with appropriate step-size annealing, we can converge to the same fixed-point as a real-valued neural network, and that with predictive coding, we can make this convergence much faster. We demonstrate that the resulting model can be used to train a spiking neural network using the update scheme from Equilibrium propagation.","tags":[],"title":"Training a Spiking Neural Network with Equilibrium Propagation","type":"publication"},{"authors":["Y. Chen","T. E. J. Mensink","E. Gavves"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210677,"objectID":"a3abfd4c482855a0ffc0cd36dcc263a6","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/chen-ic-3-dv-2019/","publishdate":"2020-08-23T19:24:37.719632Z","relpermalink":"/vislab/publication/chen-ic-3-dv-2019/","section":"publication","summary":"A key challenge for RGB-D segmentation is how to effectively incorporate 3D geometric information from the depth channel into 2D appearance features. We propose to model the effective receptive field of 2D convolution based on the scale and locality from the 3D neighborhood. Standard convolutions are local in the image space (u,v), often with a fixed receptive field of 3x3 pixels. We propose to define convolutions local with respect to the corresponding point in the 3D real-world space (x,y,z), where the depth channel is used to adapt the receptive field of the convolution, which yields the resulting filters invariant to scale and focusing on the certain range of depth. We introduce 3D Neighborhood Convolution (3DN-Conv), a convolutional operator around 3D neighborhoods. Further, we can use estimated depth to use our RGB-D based semantic segmentation model from RGB input. Experimental results validate that our proposed 3DN-Conv operator improves semantic segmentation, using either ground-truth depth (RGB-D) or estimated depth (RGB).","tags":[],"title":"3D Neighborhood Convolution: Learning Depth-Aware Features for RGB-D and RGB Semantic Segmentation","type":"publication"},{"authors":["M. O. Turkoglu","W. Thong","L. J. Spreeuwers","B. Kicanaoglu"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210680,"objectID":"1e0aa6c093866509ee816f28015610f1","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/turkoglu-aaai-2019/","publishdate":"2020-08-23T19:24:39.970173Z","relpermalink":"/vislab/publication/turkoglu-aaai-2019/","section":"publication","summary":"The visual world we sense, interpret and interact everyday is a complex composition of interleaved physical entities. Therefore, it is a very challenging task to generate vivid scenes of similar complexity using computers. In this work, we present a scene generation framework based on Generative Adversarial Networks (GANs) to sequentially compose a scene, breaking down the underlying problem into smaller ones. Different than the existing approaches, our framework offers an explicit control over the elements of a scene through separate background and foreground generators. Starting with an initially generated background, foreground objects then populate the scene one-by-one in a sequential manner. Via quantitative and qualitative experiments on a subset of the MS-COCO dataset, we show that our proposed framework produces not only more diverse images but also copes better with affine transformations and occlusion artifacts of foreground objects than its counterparts.","tags":["scene generation"],"title":"A Layer-Based Sequential Framework for Scene Generation with GANs","type":"publication"},{"authors":["C. Zhang","T. Hu","Y. Guan","Z. Ye"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210680,"objectID":"7700f75951005e59072d450b8b30aae5","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/zhang-dcc-2019/","publishdate":"2020-08-23T19:24:40.157396Z","relpermalink":"/vislab/publication/zhang-dcc-2019/","section":"publication","summary":"Network acceleration has become a hot topic, for the substantial challenge in deploying such networks in real-time applications or on resource-limited devices. A wide variety of pruning-based acceleration methods were proposed to expend the sparsity of parameters, thus omit computations involving those pruned parameters. However, these element-wise pruning methods can hardly be efficiently used for accelerating without special-customized speed-up algorithms. Due to this difficulty, recent work has turned to prune filters or channels instead, which directly reduce the number of matrix multiplications. While Channel Pruning method reforms the original CNNs to a kernel-wisely or channel-wisely pruned one, Runtime Neural Pruning (RNP) argues that models pruned with static pruning methods will lose the ability for some hard tasks since some potentially significant weights are lost during the pruning process. Dynamically pruning the channels is found to be a good solution. In this paper, we propose to use Channel Threshold-Weighting (T-Weighting) modules to choose and prune unimportant feature channels at inference phase. As the pruning is done dynamically, it is called Dynamic Channel Pruning (DCP). DCP consists of the original convolutional network and a number of \"Channel T-Weighting\" modules at certain layers. The \"Channel T-Weighting\" module assigns weights to corresponding channels, pruning those channels whose weights are zero. Those pruned channels make the CNN accelerated, and those remained channels multiplying with weights help feature expression enhanced. The reason for not considering fully-connected layers are two-fold: 1. convolution operations occupying the vast majority of all computation cost. 2. DCP is not designed only for classification, but for many tasks taking CNN as their backbone networks. In this work, we propose as a specific choice for h(·) the thresholded sigmoid function to offer sparsity to w_l, called thresholded sigmoid (T-sigmoid), h(x) = σ(x)· 1{x  T}, where σ(·) refers to sigmoid function. 1{x} is boolean indicator function, where output being 1 when input x is True, and vice versa. The T-sigmoid function is inspired by spike-and-slab models, which formulates distributions over hidden variables as the product of a binary spike variable and a real-valued code. The DCP is trained in a layer-by-layer manner. We first train the \"Channel T-Weighting\" module, and then set the threshold based on the given pruned ratio, and adjust the threshold in an iterative way at the end. The proposed DCP could reach 5× speed-up with only 4.77% drops on ILSVRC2012 dataset. Comparing the increasing error with baseline methods (Filter Pruning, Channel Pruning and RNP), DCP outperforms other methods consistently as the speed-up ratio increasing. The experiment show that DCP also consistently outperforms the baseline model whenever for Cifar10 and Cifar100. By comparing the full model and accelerated model (3×), we can see that DCP generalized well on scenes classification task (on the Places365-Challenge dataset) with VGG-16, with the top-1 accuracy top-5 accuracy dropping 2.07% and 1.96% respectively. DCP (3×) trained with ResNet-50 also suffered slight drops, with the top-1 accuracy top-5 accuracy dropping 2.78% and 2.55% respectively, outperforming Channel Pruning (our impl.) by a large margin. For the detection task on the PASCAL VOC2007 dataset using Faster R-CNN, we observe 0.5% mAP drops and 1.7% mAP drops of our 2× acceleration model and 4× acceleration model respectively, showing little accuracy degradation, showing a competitive result for proving DCP generalized well on detection task. ","tags":[],"title":"Accelerating Convolutional Neural Networks with Dynamic Channel Pruning","type":"publication"},{"authors":["T. Hu","P. Yang","C. Zhang","G. Yu","Y. Mu","C. G. M. Snoek"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210678,"objectID":"71ceaa974080b9f33d513ec0b9044d4f","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/hu-aaai-2019/","publishdate":"2020-08-23T19:24:37.968737Z","relpermalink":"/vislab/publication/hu-aaai-2019/","section":"publication","summary":"Few-shot learning is a nascent research topic, motivated by the fact that traditional deep learning methods require tremendous amounts of data. The scarcity of annotated data becomes even more challenging in semantic segmentation since pixellevel annotation in segmentation task is more labor-intensive to acquire. To tackle this issue, we propose an Attentionbased Multi-Context Guiding (A-MCG) network, which consists of three branches: the support branch, the query branch, the feature fusion branch. A key differentiator of A-MCG is the integration of multi-scale context features between support and query branches, enforcing a better guidance from the support set. In addition, we also adopt a spatial attention along the fusion branch to highlight context information from several scales, enhancing self-supervision in one-shot learning. To address the fusion problem in multi-shot learning, Conv-LSTM is adopted to collaboratively integrate the sequential support features to elevate the final accuracy. Our architecture obtains state-of-the-art on unseen classes in a variant of PASCAL VOC12 dataset and performs favorably against previous work with large gains of 1.1%, 1.4% measured in mIoU in the 1-shot and 5-shot setting.","tags":["few-shot learning","semantic segmentation"],"title":"Attention-based Multi-Context Guiding for Few-Shot Semantic Segmentation","type":"publication"},{"authors":["C. Oh","J. Tomczak","E. Gavves","M. Welling"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210679,"objectID":"eec719a49e5ae057974e6dd250d19b98","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/oh-nips-2019/","publishdate":"2020-08-23T19:24:39.516215Z","relpermalink":"/vislab/publication/oh-nips-2019/","section":"publication","summary":"This paper focuses on Bayesian Optimization (BO) for objectives on combinatorial search spaces, including ordinal and categorical variables. Despite the abundance of potential applications of Combinatorial BO, including chipset configuration search and neural architecture search, only a handful of methods have been proposed. We introduce COMBO, a new Gaussian Process (GP) BO. COMBO quantifies \"smoothness\" of functions on combinatorial search spaces by utilizing a combinatorial graph. The vertex set of the combinatorial graph consists of all possible joint assignments of the variables, while edges are constructed using the graph Cartesian product of the sub-graphs that represent the individual variables. On this combinatorial graph, we propose an ARD diffusion kernel with which the GP is able to model high-order interactions between variables leading to better performance. Moreover, using the Horseshoe prior for the scale parameter in the ARD diffusion kernel results in an effective variable selection procedure, making COMBO suitable for high dimensional problems. Computationally, in COMBO the graph Cartesian product allows the Graph Fourier Transform calculation to scale linearly instead of exponentially. We validate COMBO in a wide array of realistic benchmarks, including weighted maximum satisfiability problems and neural architecture search. COMBO outperforms consistently the latest state-of-the-art while maintaining computational and statistical efficiency.","tags":[],"title":"Combinatorial Bayesian Optimization using the Graph Cartesian Product","type":"publication"},{"authors":["Z. Shi","P. S. M. Mettes","C. G. M. Snoek"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210679,"objectID":"dbbadd192feb72cd19a8a19c7a404d08","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/shi-iccv-2019/","publishdate":"2020-08-23T19:24:39.771118Z","relpermalink":"/vislab/publication/shi-iccv-2019/","section":"publication","summary":"This paper aims to count arbitrary objects in images. The leading counting approaches start from point annotations per object from which they construct density maps. Then, their training objective transforms input images to density maps through deep convolutional networks. We posit that the point annotations serve more supervision purposes than just constructing density maps. We introduce ways to repurpose the points for free. First, we propose supervised focus from segmentation, where points are converted into binary maps. The binary maps are combined with a network branch and accompanying loss function to focus on areas of interest. Second, we propose supervised focus from global density, where the ratio of point annotations to image pixels is used in another branch to regularize the overall density estimation. To assist both the density estimation and the focus from segmentation, we also introduce an improved kernel size estimator for the point annotations. Experiments on six datasets show that all our contributions reduce the counting error, regardless of the base network, resulting in state-of-the-art accuracy using only a single network. Finally, we are the first to count on WIDER FACE, allowing us to show the benefits of our approach in handling varying object scales and crowding levels.","tags":["counting"],"title":"Counting with Focus for Free","type":"publication"},{"authors":["J. Zhao","C. G. M. Snoek"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210680,"objectID":"9f23f7d792f0264a518c3bcdfd5ce6e0","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/zhao-cvpr-2019/","publishdate":"2020-08-23T19:24:40.333902Z","relpermalink":"/vislab/publication/zhao-cvpr-2019/","section":"publication","summary":"The goal of this paper is to detect the spatio-temporal extent of an action. The two-stream detection network based on RGB and flow provides state-of-the-art accuracy at the expense of a large model-size and heavy computation. We propose to embed RGB and optical-flow into a single two-in-one stream network with new layers. A motion condition layer extracts motion information from flow images, which is leveraged by the motion modulation layer to generate transformation parameters for modulating the low-level RGB features. The method is easily embedded in existing appearance- or two-stream action detection networks, and trained end-to-end. Experiments demonstrate that leveraging the motion condition to modulate RGB features improves detection accuracy. With only half the computation and parameters of the state-of-the-art two-stream methods, our two-in-one stream still achieves impressive results on UCF101-24, UCFSports and J-HMDB.","tags":["action recognition"],"title":"Dance with Flow: Two-in-One Stream Action Detection","type":"publication"},{"authors":["T. Cohen","M. Weiler","B. Kicanaoglu","M. Welling"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210677,"objectID":"63bcb3d7afb7770acc0fb4e4a1ac2b7e","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/cohen-icml-2019/","publishdate":"2020-08-23T19:24:37.849363Z","relpermalink":"/vislab/publication/cohen-icml-2019/","section":"publication","summary":"The principle of equivariance to symmetry transformations enables a theoretically grounded approach to neural network architecture design. Equivariant networks have shown excellent performance and data efficiency on vision and medical imaging problems that exhibit symmetries. Here we show how this principle can be extended beyond global symmetries to local gauge transformations. This enables the development of a very general class of convolutional neural networks on manifolds that depend only on the intrinsic geometry, and which includes many popular methods from equivariant and geometric deep learning. We implement gauge equivariant CNNs for signals defined on the surface of the icosahedron, which provides a reasonable approximation of the sphere. By choosing to work with this very regular manifold, we are able to implement the gauge equivariant convolution using a single conv2d call, making it a highly scalable and practical alternative to Spherical CNNs. Using this method, we demonstrate substantial improvements over previous methods on the task of segmenting omnidirectional images and global climate patterns.","tags":[],"title":"Gauge Equivariant Convolutional Networks and the Icosahedral CNN","type":"publication"},{"authors":["P. S. M. Mettes","E. van der Pol","C. G. M. Snoek"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210679,"objectID":"d74e05d878a8f7488ffb83b28cb06526","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mettes-nips-2019/","publishdate":"2020-08-23T19:24:39.082799Z","relpermalink":"/vislab/publication/mettes-nips-2019/","section":"publication","summary":"This paper introduces hyperspherical prototype networks, which unify classification and regression with prototypes on hyperspherical output spaces. For classification, a common approach is to define prototypes as the mean output vector over training examples per class. Here, we propose to use hyperspheres as output spaces, with class prototypes defined a priori with large margin separation. We position prototypes through data-independent optimization, with an extension to incorporate priors from class semantics. By doing so, we do not require any prototype updating, we can handle any training size, and the output dimensionality is no longer constrained to the number of classes. Furthermore, we generalize to regression, by optimizing outputs as an interpolation between two prototypes on the hypersphere. Since both tasks are now defined by the same loss function, they can be jointly trained for multi-task problems. Experimentally, we show the benefit of hyperspherical prototype networks for classification, regression, and their combination over other prototype methods, softmax cross-entropy, and mean squared error approaches.","tags":[],"title":"Hyperspherical Prototype Networks","type":"publication"},{"authors":["P. O'Connor","E. Gavves","M. Welling"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210679,"objectID":"3a3d4455a3d116a6fb10293b4d91a211","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/o-connor-iclr-2019/","publishdate":"2020-08-23T19:24:39.385731Z","relpermalink":"/vislab/publication/o-connor-iclr-2019/","section":"publication","summary":"Deep neural networks are almost universally trained with reverse-mode automatic differentiation (a.k.a. backpropagation). Biological networks, on the other hand, appear to lack any mechanism for sending gradients back to their input neurons, and thus cannot be learning in this way. In response to this, Scellier \u0026 Bengio (2017) proposed Equilibrium Propagation - a method for gradient-based train- ing of neural networks which uses only local learning rules and, crucially, does not rely on neurons having a mechanism for back-propagating an error gradient. Equilibrium propagation, however, has a major practical limitation: inference involves doing an iterative optimization of neural activations to find a fixed-point, and the number of steps required to closely approximate this fixed point scales poorly with the depth of the network. In response to this problem, we propose Initialized Equilibrium Propagation, which trains a feedforward network to initialize the iterative inference procedure for Equilibrium propagation. This feed-forward network learns to approximate the state of the fixed-point using a local learning rule. After training, we can simply use this initializing network for inference, resulting in a learned feedforward network. Our experiments show that this network appears to work as well or better than the original version of Equilibrium propagation. This shows how we might go about training deep networks without using backpropagation.","tags":[],"title":"Initialized Equilibrium Propagation for Backprop-Free Training","type":"publication"},{"authors":["S. Ibrahimi","S. Chen","D. Arya","A. Câmara","Y. Chen","T. Crijns","M. van der Goes","T. E. J. Mensink","E. van Miltenburg","D. Odijk","W. Thong","J. Zhao","P. S. M. Mettes"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210678,"objectID":"552104834943e8846638b96bdbae84d4","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/ibrahimi-icm-2019/","publishdate":"2020-08-23T19:24:38.495477Z","relpermalink":"/vislab/publication/ibrahimi-icm-2019/","section":"publication","summary":"This demo presents a system for journalists to explore video footage for broadcasts. Daily news broadcasts contain multiple news items that consist of many video shots and searching for relevant footage is a labor intensive task. Without the need for annotated video shots, our system extracts semantics from footage and automatically matches these semantics to query terms from the journalist. The journalist can then indicate which aspects of the query term need to be emphasized, e.g. the title or its thematic meaning. The goal of this system is to support the journalists in their search process by encouraging interaction and exploration with the system.","tags":[],"title":"Interactive Exploration of Journalistic Video Footage through Multimodal Semantic Matching","type":"publication"},{"authors":["X. Cao","B. Qiu","X. Li","Z. Shi","G. Xu","J. Xu"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"d27368ae1abb4d26b9ca535f9b01ec73","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/cao-nnls-2019/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/vislab/publication/cao-nnls-2019/","section":"publication","summary":"The balance of neighborhood space around a central point is an important concept in cluster analysis. It can be used to effectively detect cluster boundary objects. The existing neighborhood analysis methods focus on the distribution of data, i.e., analyzing the characteristic of the neighborhood space from a single perspective, and could not obtain rich data characteristics. In this paper, we analyze the high-dimensional neighborhood space from multiple perspectives. By simulating each dimension of a data points k nearest neighbors space (kNNs) as a lever, we apply the lever principle to compute the balance fulcrum of each dimension after proving its inevitability and uniqueness. Then, we model the distance between the projected coordinate of the data point and the balance fulcrum on each dimension and construct the DHBlan coefficient to measure the balance of the neighborhood space. Based on this theoretical model, we propose a simple yet effective cluster boundary detection algorithm called Lever. Experiments on both low- and high-dimensional data sets validate the effectiveness and efficiency of our proposed algorithm. ","tags":[],"title":"Multidimensional Balance-Based Cluster Boundary Detection for High-Dimensional Data","type":"publication"},{"authors":["S. H. Cappallo","S. Svetlichnaya","P. Garrigues","T. E. J. Mensink","C. G. M. Snoek"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210677,"objectID":"2d6d2b580e85865cc6a3d5836866d3f6","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/cappallo-tmm-2019/","publishdate":"2020-08-23T19:24:37.604417Z","relpermalink":"/vislab/publication/cappallo-tmm-2019/","section":"publication","summary":"Over the past decade, emoji have emerged as a new and widespread form of digital communication, spanning diverse social networks and spoken languages. We propose to treat these ideograms as a new modality in their own right, distinct in their semantic structure from both the text in which they are often embedded as well as the images which they resemble. As a new modality, emoji present rich novel possibilities for representation and interaction. In this paper, we explore the challenges that arise naturally from considering the emoji modality through the lens of multimedia research. Specifically, the ways in which emoji can be related to other common modalities such as text and images. To do so, we first present a large scale dataset of real-world emoji usage collected from Twitter. This dataset contains examples of both text-emoji and image-emoji relationships. We present baseline results on the challenge of predicting emoji from both text and images, using state-of-the-art neural networks. Further, we offer a first consideration into the problem of how to account for new, unseen emoji - a relevant issue as the emoji vocabulary continues to expand on a yearly basis. Finally, we present results for multimedia retrieval using emoji as queries.","tags":[],"title":"New Modality: Emoji Challenges in Prediction, Anticipation, and Retrieval","type":"publication"},{"authors":["P. S. M. Mettes","C. G. M. Snoek"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210679,"objectID":"8c30286483feb3d596ebc7a2e974119e","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/mettes-ijcv-2019/","publishdate":"2020-08-23T19:24:38.949399Z","relpermalink":"/vislab/publication/mettes-ijcv-2019/","section":"publication","summary":"This paper strives for spatio-temporal localization of human actions in videos. In the literature, the consensus is to achieve localization by training on bounding box annotations provided for each frame of each training video. As annotating boxes in video is expensive, cumbersome and error-prone, we propose to bypass box-supervision. Instead, we introduce action localization based on point-supervision. We start from unsupervised spatio-temporal proposals, which provide a set of candidate regions in videos. While normally used exclusively for inference, we show spatio-temporal proposals can also be leveraged during training when guided by a sparse set of point annotations. We introduce an overlap measure between points and spatio-temporal proposals and incorporate them all into a new objective of a multiple instance learning optimization. During inference, we introduce pseudo-points, visual cues from videos, that automatically guide the selection of spatio-temporal proposals. We outline five spatial and one temporal pseudo-point, as well as a measure to best leverage pseudo-points at test time. Experimental evaluation on three action localization datasets shows our pointly-supervised approach (1) is as effective as traditional box-supervision at a fraction of the annotation cost, (2) is robust to sparse and noisy point annotations, (3) benefits from pseudo-points during inference, and (4) outperforms recent weakly-supervised alternatives. This leads us to conclude that points provide a viable alternative to boxes for action localization.","tags":["action localization","weakly-supervised learning"],"title":"Pointly-Supervised Action Localization","type":"publication"},{"authors":["C. Louizos","M. Reisser","T. Blankevoort","E. Gavves","M. Welling"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210678,"objectID":"2ba97389ade04d065cd9da3888059dfc","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/louizos-iclr-2019/","publishdate":"2020-08-23T19:24:38.800243Z","relpermalink":"/vislab/publication/louizos-iclr-2019/","section":"publication","summary":"Neural network quantization has become an important research area due to its great impact on deployment of large models on resource constrained devices. In order to train networks that can be effectively discretized without loss of performance, we introduce a differentiable quantization procedure. Differentiability can be achieved by transforming continuous distributions over the weights and activations of the network to categorical distributions over the quantization grid. These are subsequently relaxed to continuous surrogates that can allow for efficient gradient-based optimization. We further show that stochastic rounding can be seen as a special case of the proposed approach and that under this formulation the quantization grid itself can also be optimized with gradient descent. We experimentally validate the performance of our method on MNIST, CIFAR 10 and Imagenet classification.","tags":[],"title":"Relaxed quantization for discretized neural networks","type":"publication"},{"authors":["T. F. H. Runia","C. G. M. Snoek","A. W. M. Smeulders"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210679,"objectID":"f9ded429bf8ac1f26bf60f986e47618d","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/runia-ijcv-2019/","publishdate":"2020-08-23T19:24:39.654874Z","relpermalink":"/vislab/publication/runia-ijcv-2019/","section":"publication","summary":"Visual repetition is ubiquitous in our world. It appears in human activity (sports, cooking), animal behavior (a bee’s waggle dance), natural phenomena (leaves in the wind) and in urban environments (flashing lights). Estimating visual repetition from realistic video is challenging as periodic motion is rarely perfectly static and stationary. To better deal with realistic video, we elevate the static and stationary assumptions often made by existing work. Our spatiotemporal filtering approach, established on the theory of periodic motion, effectively handles a wide variety of appearances and requires no learning. Starting from motion in 3D we derive three periodic motion types by decomposition of the motion field into its fundamental components. In addition, three temporal motion continuities emerge from the field’s temporal dynamics. For the 2D perception of 3D motion we consider the viewpoint relative to the motion; what follows are 18 cases of recurrent motion perception. To estimate repetition under all circumstances, our theory implies constructing a mixture of differential motion maps: F, ∇F, ∇·F and ∇×F. We temporally convolve the motion maps with wavelet filters to estimate repetitive dynamics. Our method is able to spatially segment repetitive motion directly from the temporal filter responses densely computed over the motion maps. For experimental verification of our claims, we use our novel dataset for repetition estimation, better-reflecting reality with non-static and non-stationary repetitive motion. On the task of repetition counting, we obtain favorable results compared to a deep learning alternative.","tags":["repetition estimation"],"title":"Repetition Estimation","type":"publication"},{"authors":["T. Hu","P. S. M. Mettes","J-H. Huang","C. G. M. Snoek"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210678,"objectID":"4e9ecf05255e358074b55c827ea0f6fd","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/hu-iccv-2019/","publishdate":"2020-08-23T19:24:38.101105Z","relpermalink":"/vislab/publication/hu-iccv-2019/","section":"publication","summary":"Few-shot learning is a nascent research topic, motivated by the fact that traditional deep learning requires tremendous amounts of data. In this work, we propose a new task along this research direction, we call few-shot commonlocalization. Given a few weakly-supervised support images, we aim to localize the common object in the query image without any box annotation. This task differs from standard few-shot settings, since we aim to address the localization problem, rather than the global classification problem. To tackle this new problem, we propose a network that aims to get the most out of the support and query images. To that end, we introduce a spatial similarity module that searches the spatial commonality among the given images. We furthermore introduce a feature reweighting module to balance the influence of different support images through graph convolutional networks. To evaluate fewshot common-localization, we repurpose and reorganize the well-known Pascal VOC and MS-COCO datasets, as well as a video dataset from ImageNet VID. Experiments on the new settings for few-shot common-localization shows the importance of searching for spatial similarity and feature reweighting, outperforming baselines from related tasks","tags":["few-shot learning","object detection"],"title":"SILCO: Show a Few Images, Localize the Common Object","type":"publication"},{"authors":["S. Liao","Gavves E.","C. G. M. Snoek"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210678,"objectID":"d9b36b83a9155b192199c83e0c3a1a67","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/liao-cvpr-2019/","publishdate":"2020-08-23T19:24:38.634035Z","relpermalink":"/vislab/publication/liao-cvpr-2019/","section":"publication","summary":"Many computer vision challenges require continuous outputs, but tend to be solved by discrete classification. The reason is classification’s natural containment within a probability n-simplex, as defined by the popular softmax activation function. Regular regression lacks such a closed geometry, leading to unstable training and convergence to suboptimal local minima. Starting from this insight we revisit regression in convolutional neural networks. We observe many continuous output problems in computer vision are naturally contained in closed geometrical manifolds, like the Euler angles in viewpoint estimation or the normals in surface normal estimation. A natural framework for posing such continuous output problems are n-spheres, which are naturally closed geometric manifolds defined in the R (n+1) space. By introducing a spherical exponential mapping on n-spheres at the regression output, we obtain well-behaved gradients, leading to stable training. We show how our spherical regression can be utilized for several computer vision challenges, specifically viewpoint estimation, surface normal estimation and 3D rotation estimation. For all these problems our experiments demonstrate the benefit of spherical regression.","tags":[],"title":"Spherical Regression: Learning Viewpoints, Surface Normals and 3D Rotations on n-Spheres","type":"publication"},{"authors":["N. Hussein","E. Gavves","A. W. M. Smeulders"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210678,"objectID":"157fedf593a47d66ca43ea5bbd00d655","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/hussein-cvpr-2019/","publishdate":"2020-08-23T19:24:38.248906Z","relpermalink":"/vislab/publication/hussein-cvpr-2019/","section":"publication","summary":"This paper focuses on the temporal aspect for recognizing human activities in videos; an important visual cue that has long been undervalued. We revisit the conventional definition of activity and restrict it to “Complex Action”: a set of one-actions with a weak temporal pattern that serves a specific purpose. Related works use spatiotemporal 3D convolutions with fixed kernel size, too rigid to capture the varieties in temporal extents of complex actions, and too short for long-range temporal modeling. In contrast, we use multi-scale temporal convolutions, and we reduce the complexity of 3D convolutions. The outcome is Timeception convolution layers, which reasons about minute-long temporal patterns, a factor of 8 longer than best related works. As a result, Timeception achieves impressive accuracy in recognizing the human activities of Charades, Breakfast Actions, and MultiTHUMOS. Further, we demonstrate that Timeception learns long-range temporal dependencies and tolerate temporal extents of complex actions.","tags":["action recognition"],"title":"Timeception for Complex Action Recognition","type":"publication"},{"authors":["J. Zhao","L. Liu","and C. G. M. Snoek","J. Han","L. Shao"],"categories":[],"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"fa15471244c79785ca7345844d0eb5cc","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/zhao-bmvc-2018/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/vislab/publication/zhao-bmvc-2018/","section":"publication","summary":"While many image colorization algorithms have recently shown the capability of producing plausible color versions from gray-scale photographs, they still suffer from the problems of context confusion and edge color bleeding. To address context confusion, we propose to incorporate the pixel-level object semantics to guide the image colorization. The rationale is that human beings perceive and distinguish colors based on the object’s semantic categories. We propose a hierarchical neural network with two branches. One branch learns what the object is while the other branch learns the object’s colors. The network jointly optimizes a semantic segmentation loss and a colorization loss. To attack edge color bleeding we generate more continuous color maps with sharp edges by adopting a joint bilateral upsamping layer at inference. Our network is trained on PASCAL VOC2012 and COCO-stuff with semantic segmentation labels and it produces more realistic and finer results compared to the colorization state-of-the-art.","tags":["image colorization"],"title":"Pixel-level Semantics Guided Image Colorization","type":"publication"},{"authors":["Jianfeng Dong","Xirong Li","Cees G. M. Snoek"],"categories":[],"content":"","date":1537457939,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537457939,"objectID":"52fd452af6156c3dfdf24822436655fa","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/dong-tmm-2018/","publishdate":"2018-08-22T17:38:59+02:00","relpermalink":"/vislab/publication/dong-tmm-2018/","section":"publication","summary":"This paper strives to find amidst a set of sentences the one best describing the content of a given image or video. Different from existing works, which rely on a joint subspace for their image and video caption retrieval, we propose to do so in a visual space exclusively. Apart from this conceptual novelty, we contribute Word2VisualVec, a deep neural network architecture that learns to predict a visual feature representation from textual input. Example captions are encoded into a textual embedding based on multi-scale sentence vectorization and further transferred into a deep visual feature of choice via a simple multi-layer perceptron. We further generalize Word2VisualVec for video caption retrieval, by predicting from text both 3-D convolutional neural network features as well as a visual-audio representation. Experiments on Flickr8k, Flickr30k, the Microsoft Video Description dataset and the very recent NIST TrecVid challenge for video caption retrieval detail Word2VisualVec's properties, its benefit over textual embeddings, the potential for multimodal query composition and its state-of-the-art results.","tags":["video retrieval"],"title":"Predicting Visual Features from Text for Image and Video Caption Retrieval","type":"publication"},{"authors":["H. Bilen","B. Fernando","E. Gavves","A. Vedaldi"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210680,"objectID":"b6a4e7621977696a0edacc1506ade5fa","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/bilen-tpami-2018/","publishdate":"2020-08-23T19:24:40.636163Z","relpermalink":"/vislab/publication/bilen-tpami-2018/","section":"publication","summary":"We introduce the concept of \"dynamic image\", a novel compact representation of videos useful for video analysis, particularly in combination with convolutional neural networks (CNNs). A dynamic image encodes temporal data such as RGB or optical flow videos by using the concept of rank pooling. The idea is to learn a ranking machine that captures the temporal evolution of the data and to use the parameters of the latter as a representation. When a linear ranking machine is used, the resulting representation is in the form of an image, which we call dynamic because it summarizes the video dynamics in addition of appearance. This is a powerful idea because it allows to convert any video to an image so that existing CNN models pre-trained for the analysis of still images can be immediately extended to videos. We also present an efficient and effective approximate rank pooling operator, accelerating standard rank pooling algorithms by orders of magnitude, and formulate that as a CNN layer. This new layer allows generalizing dynamic images to dynamic feature maps. We demonstrate the power of the new representations on standard benchmarks in action recognition achieving state-of-the-art performance.","tags":["action recognition"],"title":"Action recognition with dynamic image networks","type":"publication"},{"authors":["K. Gavrilyuk","A. Ghodrati","Z. Li","C. G. M. Snoek"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210680,"objectID":"dc073312bf762a83e7b2e9c585abbc46","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/gavrilyuk-cvpr-2018/","publishdate":"2020-08-23T19:24:40.842717Z","relpermalink":"/vislab/publication/gavrilyuk-cvpr-2018/","section":"publication","summary":"This paper strives for pixel-level segmentation of actors and their actions in video content. Different from existing works, which all learn to segment from a fixed vocabulary of actor and action pairs, we infer the segmentation from a natural language input sentence. This allows to distinguish between fine-grained actors in the same super-category, identify actor and action instances, and segment pairs that are outside of the actor and action vocabulary. We propose a fully-convolutional model for pixel-level actor and action segmentation using an encoder-decoder architecture optimized for video. To show the potential of actor and action video segmentation from a sentence, we extend two popular actor and action datasets with more than 7,500 natural language descriptions. Experiments demonstrate the quality of the sentence-guided segmentations, the generalization ability of our model, and its advantage for traditional actor and action segmentation compared to the state-of-the-art.","tags":["video segmentation"],"title":"Actor and Action Video Segmentation From a Sentence","type":"publication"},{"authors":["C. Oh","E. Gavves","M. Welling"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210682,"objectID":"e4a983ceba3d29cf3e409a608415fccc","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/oh-icml-2018/","publishdate":"2020-08-23T19:24:42.040559Z","relpermalink":"/vislab/publication/oh-icml-2018/","section":"publication","summary":"A major challenge in Bayesian Optimization is the boundary issue (Swersky, 2017) where an algorithm spends too many evaluations near the boundary of its search space. In this paper, we propose BOCK, Bayesian Optimization with Cylindrical Kernels, whose basic idea is to transform the ball geometry of the search space using a cylindrical transformation. Because of the transformed geometry, the Gaussian Process-based surrogate model spends less budget searching near the boundary, while concentrating its efforts relatively more near the center of the search region, where we expect the solution to be located. We evaluate BOCK extensively, showing that it is not only more accurate and efficient, but it also scales successfully to problems with a dimensionality as high as 500. We show that the better accuracy and scalability of BOCK even allows optimizing modestly sized neural network layers, as well as neural network hyperparameters.","tags":["bayesian optimization"],"title":"BOCK: Bayesian Optimization with Cylindrical Kernels","type":"publication"},{"authors":["G. Migut","D. C. Koelma","C. G. M. Snoek","N. Brouwer-Zupancic"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210681,"objectID":"09b5f5c8dc2f4c025ce733f9741d9c01","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/migut-itcse-2018/","publishdate":"2020-08-23T19:24:41.784229Z","relpermalink":"/vislab/publication/migut-itcse-2018/","section":"publication","summary":"Detecting fraud in digital assessment is currently done by human proctor, that observes recordings of the exam. This is costly, tedious and time consuming process. In this paper we present preliminary results on automated video proctoring, which has the potential to significantly reduce manual effort and scale-up digital assessment, while retaining good fraud detection.","tags":[],"title":"Cheat me not: automated proctoring of digital exams on Bring-Your-Own-Device","type":"publication"},{"authors":["Z. Shi","L. Zhang","Y. Liu","X. Cao","Y. Ye","M-M. Cheng","G. Zheng"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210682,"objectID":"d5bfda3464e4677b31a8cca6a50b2b92","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/shi-cvpr-2018/","publishdate":"2020-08-23T19:24:42.420371Z","relpermalink":"/vislab/publication/shi-cvpr-2018/","section":"publication","summary":"Deep convolutional networks (ConvNets) have achieved unprecedented performances on many computer vision tasks. However, their adaptations to crowd counting on single images are still in their infancy and suffer from severe over-fitting. Here we propose a new learning strategy to produce generalizable features by way of deep negative correlation learning (NCL). More specifically, we deeply learn a pool of decorrelated regressors with sound generalization capabilities through managing their intrinsic diversities. Our proposed method, named decorrelated ConvNet (D-ConvNet), is end-to-end-trainable and independent of the backbone fully-convolutional network architectures. Extensive experiments on very deep VGGNet as well as our customized network structure indicate the superiority of D-ConvNet when compared with several state-ofthe-art methods. ","tags":["counting"],"title":"Crowd Counting With Deep Negative Correlation Learning","type":"publication"},{"authors":["B. Kicanaoglu","R. Tao","A. W. M. Smeulders"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210681,"objectID":"235fd2c3820488a3cb5b709e69501b5a","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/kicanaoglu-bmvc-2018/","publishdate":"2020-08-23T19:24:41.347672Z","relpermalink":"/vislab/publication/kicanaoglu-bmvc-2018/","section":"publication","summary":"Distinction among nearby poses and among symmetries of an object is challenging. In this paper, we propose a unified, group-theoretic approach to tackle both. Different from existing works which directly predict absolute pose, our method measures the pose of an object relative to another pose, i.e., the pose difference. The proposed method generates the complete orbit of an object from a single view of the object with respect to the subgroup of SO(3) of rotations around the z-axis, and compares the orbit of the object with another orbit using a novel orbit metric to estimate the pose difference. The generated orbit in the latent space records all the differences in pose in the original observational space, and as a result, the method is capable of finding subtle differences in pose. We demonstrate the effectiveness of the proposed method on cars, where identifying the subtle pose differences is vital.","tags":[],"title":"Estimating small differences in car-pose from orbits","type":"publication"},{"authors":["J-H. Jacobsen","A. W. M. Smeulders","E. Oyallon"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210681,"objectID":"5d118f3832d441ee011537aa31e86c61","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/jacobsen-iclr-2018/","publishdate":"2020-08-23T19:24:41.238935Z","relpermalink":"/vislab/publication/jacobsen-iclr-2018/","section":"publication","summary":"It is widely believed that the success of deep convolutional networks is based on progressively discarding uninformative variability about the input with respect to the problem at hand. This is supported empirically by the difficulty of recovering images from their hidden representations, in most commonly used network architectures. In this paper we show via a one-to-one mapping that this loss of information is not a necessary condition to learn representations that generalize well on complicated problems, such as ImageNet. Via a cascade of homeomorphic layers, we build the i-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. Building an invertible architecture is difficult, for one, because the local inversion is ill-conditioned, we overcome this by providing an explicit inverse. An analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the i-RevNet we reconstruct linear interpolations between natural image representations.","tags":["invertible network"],"title":"i-RevNet: Deep Invertible Networks","type":"publication"},{"authors":["T. Scheepers","E. Kanoulas","E. Gavves"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210682,"objectID":"2cfaab56ffcb0d2c38a75464b262ef31","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/scheepers-www-2018/","publishdate":"2020-08-23T19:24:42.30374Z","relpermalink":"/vislab/publication/scheepers-www-2018/","section":"publication","summary":"We present an in-depth analysis of four popular word embeddings (Word2Vec, GloVe, fastText and Paragram) in terms of their semantic compositionality. In addition, we propose a method to tune these embeddings towards better compositionality. We find that training the existing embeddings to compose lexicographic definitions improves their performance in this task significantly, while also getting similar or better performance in both word similarity and sentence embedding evaluations. Our method tunes word embeddings using a simple neural network architecture with definitions and lemmas from WordNet. Since dictionary definitions are semantically similar to their associated lemmas, they are the ideal candidate for our tuning method, as well as evaluating for compositionality. Our architecture allows for the embeddings to be composed using simple arithmetic operations, which makes these embeddings specifically suitable for production applications such as web search and data mining. We also explore more elaborate and involved compositional models. In our analysis, we evaluate original embeddings, as well as tuned embeddings, using existing word similarity and sentence embedding evaluation methods. Aside from these evaluation methods used in related work, we also evaluate embeddings using a ranking method which tests composed vectors using the lexicographic definitions already mentioned. In contrast to other evaluation methods, ours is not invariant to the magnitude of the embedding vector, which we show is important for composition. We consider this new evaluation method, called CompVecEval, to be a key contribution.","tags":[],"title":"Improving Word Embedding Compositionality using Lexicographic Definitions","type":"publication"},{"authors":["J. Valmadre","L. Bertinetto","J. F. Henriques","R. Tao","A. Vedaldi","A. W. M. Smeulders","P. H. S. Torr","E. Gavves"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210682,"objectID":"098aaf4d30554d4175daf7619d22c0d4","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/valmadre-eccv-2018/","publishdate":"2020-08-23T19:24:42.560648Z","relpermalink":"/vislab/publication/valmadre-eccv-2018/","section":"publication","summary":"We introduce the OxUvA dataset and benchmark for evaluating single-object tracking algorithms. Benchmarks have enabled great strides in the field of object tracking by defining standardized evaluations on large sets of diverse videos. However, these works have focused exclusively on sequences that are just tens of seconds in length and in which the target is always visible. Consequently, most researchers have designed methods tailored to this \"short-term\" scenario, which is poorly representative of practitioners  needs. Aiming to address this disparity, we compile a long-term, large-scale tracking dataset of sequences with average length greater than two minutes and with frequent target object disappearance. The OxUvA dataset is much larger than the object tracking datasets of recent years: it comprises 366 sequences spanning 14 hours of video. We assess the performance of several algorithms, considering both the ability to locate the target and to determine whether it is present or absent. Our goal is to offer the community a large and diverse benchmark to enable the design and evaluation of tracking methods ready to be used \"in the wild\".","tags":["tracking"],"title":"Long-Term Tracking in the Wild: A Benchmark","type":"publication"},{"authors":["T. F. H. Runia","C. G. M. Snoek","A. W. M. Smeulders"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210682,"objectID":"81b5fd8be7e4d8184928958e45068410","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/runia-cvpr-2018/","publishdate":"2020-08-23T19:24:42.164536Z","relpermalink":"/vislab/publication/runia-cvpr-2018/","section":"publication","summary":"We consider the problem of estimating repetition in video, such as performing push-ups, cutting a melon or playing violin. Existing work shows good results under the assumption of static and stationary periodicity. As realistic video is rarely perfectly static and stationary, the often preferred Fourier-based measurements is inapt. Instead, we adopt the wavelet transform to better handle non-static and non-stationary video dynamics. From the flow field and its differentials, we derive three fundamental motion types and three motion continuities of intrinsic periodicity in 3D. On top of this, the 2D perception of 3D periodicity considers two extreme viewpoints. What follows are 18 fundamental cases of recurrent perception in 2D. In practice, to deal with the variety of repetitive appearance, our theory implies measuring time-varying flow Ft and its differentials ∇Ft , ∇ · Ft and ∇ × Ft over segmented foreground motion. For experiments, we introduce the new QUVA Repetition dataset, reflecting reality by including non-static and non-stationary videos. On the task of counting repetitions in video, we obtain favorable results compared to a deep learning alternative.","tags":["repetition estimation"],"title":"Real-World Repetition Estimation by Div, Grad and Curl","type":"publication"},{"authors":["S. Georgoulis","K. Rematas","T. Ritschel","E. Gavves","M. Fritz","L. Van Gool","T. Tuytelaars"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210681,"objectID":"d20de135f54927f09caedd492a20ecb7","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/georgoulis-tpami-2018/","publishdate":"2020-08-23T19:24:40.986023Z","relpermalink":"/vislab/publication/georgoulis-tpami-2018/","section":"publication","summary":"","tags":[],"title":"Reflectance and natural illumination from single-material specular objects using deep learning","type":"publication"},{"authors":["S. Liao","Gavves E.","C. G. M. Snoek"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210681,"objectID":"b53cc24dce91191d7b05bdffe3a3f389","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/liao-icmr-2018/","publishdate":"2020-08-23T19:24:41.619828Z","relpermalink":"/vislab/publication/liao-icmr-2018/","section":"publication","summary":"The goal of this paper is to search and match the best rendered view of a texture-free 3D shape to an object of interest in a 2D query image. Matching rendered views of 3D shapes to RGB images is challenging because, 1) 3D shapes are not always a perfect match for the image queries, 2) there is great domain difference between rendered and RGB images, and 3) estimating the object scale versus distance is inherently ambiguous in images from uncalibrated cameras. In this work we propose a deeply learned matching function that attacks these challenges and can be used for a search engine that finds the appropriate 3D shape and matches it to objects in 2D query images. We evaluate the proposed matching function and search engine with a series of controlled experiments on the 24 most populated vehicle categories in PASCAL3D+. We test the capability of the learned matching function in transferring to unseen 3D shapes and study overall search engine sensitivity w.r.t. available 3D shapes and object localization accuracy, showing promising results in retrieving 3D shapes given 2D image queries.","tags":[],"title":"Searching and Matching Texture-free 3D Shapes in Images","type":"publication"},{"authors":["P. O'Connor","E. Gavves","M. Welling"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210682,"objectID":"2104b61ae48ffca9f98340d24efa2bfa","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/o-connor-iclr-2018/","publishdate":"2020-08-23T19:24:41.913191Z","relpermalink":"/vislab/publication/o-connor-iclr-2018/","section":"publication","summary":"The vast majority of natural sensory data is temporally redundant. Video frames or audio samples which are sampled at nearby points in time tend to have similar values. Typically, deep learning algorithms take no advantage of this redundancy to reduce computation. This can be an obscene waste of energy. We present a variant on backpropagation for neural networks in which computation scales with the rate of change of the data - not the rate at which we process the data. We do this by having neurons communicate a combination of their state, and their temporal change in state. Intriguingly, this simple communication rule give rise to units that resemble biologically-inspired leaky integrate-and-fire neurons, and to a weight-update rule that is equivalent to a form of Spike-Timing Dependent Plasticity (STDP), a synaptic learning rule observed in the brain. We demonstrate that on MNIST and a temporal variant of MNIST, our algorithm performs about as well as a Multilayer Perceptron trained with backpropagation, despite only communicating discrete values between layers.","tags":[],"title":"Temporally Efficient Deep Learning with Spikes","type":"publication"},{"authors":["A. Ghodrati","E. Gavves","C. G. M. Snoek"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210681,"objectID":"a87394a614c4857b6b0e8930841d9db4","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/ghodrati-bmvc-2018/","publishdate":"2020-08-23T19:24:41.104175Z","relpermalink":"/vislab/publication/ghodrati-bmvc-2018/","section":"publication","summary":"Time-aware encoding of frame sequences in a video is a fundamental problem in video understanding. While many attempted to model time in videos, an explicit study on quantifying video time is missing. To fill this lacuna, we aim to evaluate video time explicitly. We describe three properties of video time, namely a) temporal asymmetry, b) temporal continuity and c) temporal causality. Based on each we formulate a task able to quantify the associated property. This allows assessing the effectiveness of modern video encoders, like C3D and LSTM, in their ability to model time. Our analysis provides insights about existing encoders while also leading us to propose a new video time encoder, which is better suited for the video time recognition tasks than C3D and LSTM. We believe the proposed meta-analysis can provide a reasonable baseline to assess video time encoders on equal grounds on a set of temporal-aware tasks.","tags":["action recognition"],"title":"Video Time: Properties, Encoders and Evaluation","type":"publication"},{"authors":["Z. Li","K. Gavrilyuk","E. Gavves","M. Jain","C. G. M. Snoek"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598210681,"objectID":"49e2e6f0b225a28850dddf14a9637792","permalink":"https://ivi.fnwi.uva.nl/vislab/publication/li-cviu-2018/","publishdate":"2020-08-23T19:24:41.502975Z","relpermalink":"/vislab/publication/li-cviu-2018/","section":"publication","summary":"We present VideoLSTM for end-to-end sequence learning of actions in video. Rather than adapting the video to the peculiarities of established recurrent or convolutional architectures, we adapt the architecture to fit the requirements of the video medium. Starting from the soft-Attention LSTM, VideoLSTM makes three novel contributions. First, video has a spatial layout. To exploit the spatial correlation we hardwire convolutions in the soft-Attention LSTM architecture. Second, motion not only informs us about the action content, but also guides better the attention towards the relevant spatio-temporal locations. We introduce motion-based attention. And finally, we demonstrate how the attention from VideoLSTM can be exploited for action localization by relying on the action class label and temporal attention smoothing. Experiments on UCF101, HMDB51 and THUMOS13 reveal the benefit of the video-specific adaptations of VideoLSTM in isolation as well as when integrated in a combined architecture. It compares favorably against other LSTM architectures for action classification and especially action localization.","tags":["action recognition"],"title":"VideoLSTM Convolves, Attends and Flows for Action Recognition","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://ivi.fnwi.uva.nl/vislab/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/vislab/people/","section":"","summary":"","tags":null,"title":"People","type":"widget_page"}]